[{"path":"https://jlmelville.github.io/uwot/articles/abparams.html","id":"theory","dir":"Articles","previous_headings":"","what":"Theory","title":"Fine-tuning UMAP Visualizations","text":"output weight two points ii jj given : wij=1/(1+adij2b) w_{ij} = 1 / \\left(1 + ad_{ij}^{2b}\\right) dijd_{ij} Euclidean distance embedding two points. drop ijij subscript clarity’s sake. aa bb two hyper-parameters. Usually determined non-linear least squares fit based exponential decay curve parameterized min_dist spread: w=exp[−max(0,d−ρ)/σ] w = \\exp\\left[-\\max \\left(0, d - \\rho \\right) / \\sigma \\right] min_dist ρ\\rho spread σ\\sigma. used symbols make obvious equation form UMAP’s weighting function edge weights input space. reminder, presence max operation shifting distances ρ\\rho enforce local connectivity constraint: always edge weight 1 point nearest neighbor. spread determines x-value range y-value decays zero, set spread multiplied 3. ’s R code works plots results using Python UMAP defaults spread = 1, min_dist = 0.1: title indicates, curve leads default parameters =1.577a = 1.577, b=0.895b = 0.895. uwot uses slightly different default min_dist = 0.001, leads =1.93,b=0.79a = 1.93, b = 0.79. don’t know used different default min_dist – probably made mistake. likely change later version uwot, doesn’t make much difference results, fortunately. Setting =1,b=1a = 1, b = 1 gives Cauchy distribution used t-SNE (tumap function uwot), corresponds roughly spread = 1.12 min_dist = 0.23: putting values back curve-fitting routine give back =0.99a = 0.99, b=1.01b = 1.01. Close enough. uwot default results orange Cauchy results blue overlaid UMAP defaults (green, previous plot): still leaves open question changing spread min_dist, b affects output UMAP. current version UMAP docs doesn’t mention spread treats min_dist adjustable parameter, can varied 0 close 1. follows ’ll take look min_dist spread, use trusty MNIST digits dataset investigate effect.","code":"spread <- 1 min_dist <- 0.1  # define the exponential xv <- seq(   from = 0,   to = spread * 3,   length.out = 300 ) yv <- exp(-(pmax(0, xv - min_dist)) / spread)  # Fit the a,b curve to the exponential params <- stats::nls(yv ~ 1 / (1 + a * xv^(2.0 * b)),   start = list(a = 1, b = 1) )$m$getPars() a <- params[\"a\"] b <- params[\"b\"]  # Plot the results title <-   paste0(     \"exp curve spread = \",     spread,     \", min_dist = \",     min_dist   ) sub <-   paste0(\"UMAP fit (green) a = \", formatC(a), \" b = \", formatC(b)) plot(   xv,   yv,   xlab = \"d\",   ylab = \"w\",   type = \"l\",   main = title,   lwd = 2 )  graphics::mtext(sub) lines(xv, 1 / (1 + a * xv^(2.0 * b)), col = \"#1B9E77FF\", lwd = 2)"},{"path":"https://jlmelville.github.io/uwot/articles/abparams.html","id":"min_dist-and-spread","dir":"Articles","previous_headings":"","what":"min_dist and spread","title":"Fine-tuning UMAP Visualizations","text":"Let’s look min_dist spread first. First, ’ll show results changing spread, keeping min_dist = 0.1 ’ll look changing min_dist, fixing spread = 1. explanation exponential curve suggests allowing min_dist exceed value spread give increasingly odd results.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/abparams.html","id":"spread","dir":"Articles","previous_headings":"min_dist and spread","what":"spread","title":"Fine-tuning UMAP Visualizations","text":"plots , value spread min_dist given title, along values b give rise . value spread increases 0.1 10 go left right top bottom. top-left result, spread = min_dist = 0.1 gives clear indication want spread larger min_dist. , values spread 0.5-3 much happens. spread gets 5 , clusters start overlap . ’s table summarizing b change spread varied. low values spread, starts getting large. min_dist? , min_dist increases 0.0001 2, spread = 1, suspect highest values also show eccentric results.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/abparams.html","id":"min_dist","dir":"Articles","previous_headings":"min_dist and spread","what":"min_dist","title":"Fine-tuning UMAP Visualizations","text":", range min_dist values, 0.0001 0.1 much happens plot. value, clusters begin expand. final shapes diffuse. table shows b change min_dist increases. value min_dist larger 0.5, values drop quickly b increases rapidly. terms differences spread min_dist, min_dist seems obviously increase size clusters, whereas increasing spread keeps shape boundary clusters bit better. spread can therefore used control inter-cluster distances extent, min_dist controls size clusters.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/abparams.html","id":"a-and-b","dir":"Articles","previous_headings":"","what":"a and b","title":"Fine-tuning UMAP Visualizations","text":"Rather change spread min_dist, can supply values b directly. Perhaps can also interpreted. , ’ll look changing value separately, leaving value UMAP defaults (= 1.58 b = 0.90). used range values b resulted changing min_dist spread previous section’s results set range values look . Wang co-workers recommend b > 0.5 good loss function dimensionality reduction (see Proposition 1 2 PaCMAP paper).","code":""},{"path":"https://jlmelville.github.io/uwot/articles/abparams.html","id":"a","dir":"Articles","previous_headings":"a and b","what":"a","title":"Fine-tuning UMAP Visualizations","text":"seemed wider range values b, looked values = 0.0001 = 100. seems control spread clusters range. Low values certainly result diffuse round cloud. = 10, suspect running numerical issues taking large power small positive value. Values 0.1 10 seem reasonable, higher values leading smaller clusters.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/abparams.html","id":"b","dir":"Articles","previous_headings":"a and b","what":"b","title":"Fine-tuning UMAP Visualizations","text":"b definitely seems smaller range useful values compared , looked values b = 0.1 b = 2.5. b seems work like heavy-tail parameter sometimes used t-SNE: low values increase distances clusters, relative size, also reveal sub-clusters appear one structure plots. high values, space clusters reduced, can still see borders clusters, unlike happens low . don’t see big difference using b directly, sticking min_dist spread prefer b , reminds approach used ABSNE (PDF), although don’t claim equivalence b parameters UMAP α\\alpha β\\beta parameters work.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/abparams.html","id":"examples","dir":"Articles","previous_headings":"","what":"Examples","title":"Fine-tuning UMAP Visualizations","text":"MNIST useful seeing effect changing b, can visualized well default parameters. two examples show can improve default visualizations, using knowledge b roughly represent terms cluster size separation.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/abparams.html","id":"example-1-tasic2018","dir":"Articles","previous_headings":"Examples","what":"Example 1: tasic2018","title":"Fine-tuning UMAP Visualizations","text":"transcriptomics dataset tasic2018 good example default UMAP parameters sub-optimal. default UMAP result top left image, series results based fiddling b response . eventually fumble way setting lower higher b provides better visualization (opinion), can see lower right.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/abparams.html","id":"example-2-coil-20","dir":"Articles","previous_headings":"Examples","what":"Example 2: COIL-20","title":"Fine-tuning UMAP Visualizations","text":"’s another example arguably default UMAP results spread clusters bit much. three alternatives based controlling b.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/abparams.html","id":"my-recommendation","dir":"Articles","previous_headings":"","what":"My recommendation","title":"Fine-tuning UMAP Visualizations","text":"Hopefully enough convince embedding parameters can profitably twiddled random way give visualizations improve default settings. min_dist spread, modifying min_dist 0 1, suggested UMAP docs seems fruitful parameters meddle . personally prefer use b directly. find good values b, can start = 1 b = 1, gives t-SNE-like output function, can use tumap function generate initial plot much faster. also recommend PCA dimensionality reduction outside uwot using ret_nn = TRUE first plot, can re-use nearest neighbors data subsequent runs umap. substantial speed makes repeating runs different values b lot tolerable. ’s example workflow:","code":"# PCA to 50 dimensions first mnist_pca <- irlba::prcomp_irlba(mnist, n = 50, retx = TRUE, center = center,                              scale = FALSE)$x  # t-UMAP is equivalent to a = 1, b = 1 # remember to get the nearest neighbor data back too mnist_a1b1 <- tumap(mnist_pca, ret_nn = TRUE)  # find a mnist_a1.5b1 <- umap(mnist_pca, nn_method = mnist_a1b1$nn, a = 1.5, b = 1) mnist_a0.5b1 <- umap(mnist_pca, nn_method = mnist_a1b1$nn, a = 0.5, b = 1)  # find b based on whichever value of a you prefer mnist_a0.5b1.2 <- umap(mnist_pca, nn_method = mnist_a1b1$nn, a = 0.5, b = 1.2)"},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/fast-sgd.html","id":"norb","dir":"Articles","previous_headings":"","what":"norb","title":"Fast SGD settings","text":"distribution clusters look little bit different, think ’s within variation one expect stochastic nature optimization. bolster point, two runs UMAP NORB dataset fast_sgd = FALSE different seeds: think didn’t know one four images generated fast settiings, ’d hard pressed pick line . variation images likely due default n_neighbors low capture global structure. ’s plots n_neighbors = 150, n_epochs = 500 account increased number edges need sampling: Apart one blue loops open fast_sgd = TRUE result, probably fixable longer optimization, global arrangement two plots pretty similar.","code":""},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/fast-sgd.html","id":"recommendations","dir":"Articles","previous_headings":"","what":"Recommendations","title":"Fast SGD settings","text":"results, ’d say fast_sgd = TRUE settings give results effectively indistinguishable slower settings, want save bit time, seems harm using . actual time savings ’ll see depend long nearest neighbor search takes, initial PCA carry input (examples) can take fair amount run time. example NORB, PCA takes 5 minutes six---half minute total run time, ’s lot time saved. MNIST Fashion, can effectively halve run time (six threads, anyway). reproducibility important , using multiple threads optimization question, although ’s gives biggest speed increase. However, still consider setting approx_pow = TRUE, pcg_rand = FALSE. MNIST-sized datasets (mnist, fashion, kuzushiji) saw reasonable speed around 25%. datasets PCA nearest neighbor search dominates, gains smaller: 10-15% speedup tasic2018 macosko2015, 5% norb. need set n_epochs higher, time savings increase.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/fast-sgd.html","id":"update-december-22-2024","dir":"Articles","previous_headings":"Recommendations","what":"Update December 22 2024","title":"Fast SGD settings","text":"Since wrote document, extra options available. uwot 0.2.3, rng_type parameter preferred pcg_rand. equivalent pcg_rand = FALSE rng_type = \"tausworthe\", can also set rng_type = \"deterministic\", deterministically sample vertices. can give bigger speed rng_type = \"tausworthe\". , using umap2 function rnndescent installed, following parameters can used speed nearest neighbor search: nn_method = \"nndescent\", nn_args = list(n_trees = 8, max_candidates = 20). Finally, can also consider setting negative_sample_rate = 4, give slight speed versus default sample rate 5. suggestions based comment Leland McInnes Reddit.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"hnsw-for-nearest-neighbors","dir":"Articles","previous_headings":"","what":"HNSW for nearest neighbors","title":"Using HNSW for nearest neighbors","text":"Hierachical Navigable Small World (HNSW) method finding approximate nearest neighbors popular due speed good performance. header-C++ implementation HNSW authors can found hnswlib, R-bindings available package RcppHNSW. like use HNSW nearest neighbor search, uwot now integrates RcppHNSW optional dependency. just need install RcppHNSW package: uwot now able use set nn_method = \"hnsw\", e.g.","code":"install.packages(\"RcppHNSW\") library(uwot) # doesn't use HNSW iris_umap <- umap(iris)  # does use HNSW iris_umap_hnsw <- umap(iris, nn_method = \"hnsw\")"},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"mnist-example","dir":"Articles","previous_headings":"HNSW for nearest neighbors","what":"MNIST example","title":"Using HNSW for nearest neighbors","text":"involved example ’ll use MNIST digits data set article, need install snedata package GitHub: , download MNIST dataset (requires access internet), split data traditional training/test split used dataset. mnist dataframe first 768 columns contain pixel data image, last column factor Label, contains digit image represents.","code":"# install.packages(\"pak\") pak::pkg_install(\"jlmelville/snedata\")  # or # install.packages(\"devtools\") # devtools::install_github(\"jlmelville/snedata\") mnist <- snedata::download_mnist() mnist_train <- head(mnist, 60000) mnist_test <- tail(mnist, 10000)"},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"umap-on-training-data-with-hnsw","dir":"Articles","previous_headings":"HNSW for nearest neighbors > MNIST example","what":"UMAP on training data with HNSW","title":"Using HNSW for nearest neighbors","text":"noted , need use nn_method = \"hnsw\" use HNSW. going also set non-default parameters: verbose = TRUE – just like know ’s going . n_sgd_threads = 6 number threads use optimization step. batch = TRUE alternative optimization method gives bit repeatable results setting n_sgd_threads greater one. n_epochs = 500 number epochs use optimization step. find batch = TRUE, need use larger number epochs usual default (MNIST-sized datasets n_epochs = 200). ret_model = TRUE parameter tells uwot return UMAP model can transform new data later.","code":"mnist_train_umap <-   umap(     mnist_train,     nn_method = \"hnsw\",     batch = TRUE,     n_epochs = 500,     n_sgd_threads = 6,     ret_model = TRUE,     verbose = TRUE   ) UMAP embedding parameters a = 1.896 b = 0.8006 Converting dataframe to numerical matrix Read 60000 rows and found 784 numeric columns Building HNSW index with metric 'l2' ef = 200 M = 16 using 6 threads Finished building index Searching HNSW index with ef = 15 and 6 threads Finished searching Commencing smooth kNN distance calibration using 6 threads with target n_neighbors = 15 Initializing from normalized Laplacian + noise (using RSpectra) Commencing optimization for 500 epochs, with 1239066 positive edges using 6 threads Using method 'umap' Optimizing with Adam alpha = 1 beta1 = 0.5 beta2 = 0.9 eps = 1e-07 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----| **************************************************| Optimization finished"},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"umap-on-test-data","dir":"Articles","previous_headings":"HNSW for nearest neighbors > MNIST example","what":"UMAP on test data","title":"Using HNSW for nearest neighbors","text":"don’t need specify use HNSW transforming new data, model contains necessary information. can just use umap_transform normal:","code":"mnist_test_umap <-   umap_transform(     X = mnist_test,     model = mnist_train_umap,     n_sgd_threads = 6,     verbose = TRUE   ) Read 10000 rows and found 784 numeric columns Processing block 1 of 1 Finished searching Commencing smooth kNN distance calibration using 6 threads with target n_neighbors = 15 Initializing by weighted average of neighbor coordinates using 6 threads Commencing optimization for 167 epochs, with 150000 positive edges using 6 threads Using method 'umap' Optimizing with Adam alpha = 1 beta1 = 0.5 beta2 = 0.9 eps = 1e-07 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----| **************************************************| Finished"},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"plotting-the-results","dir":"Articles","previous_headings":"HNSW for nearest neighbors > MNIST example","what":"Plotting the results","title":"Using HNSW for nearest neighbors","text":"Let’s plot embeddings make sure HNSW giving us useful results. use ggplot2, Polychrome package create set distinct colors digit. gives palette 10 colors following recipe bit like Python glasbey package: Now can plot training test data, two separate plots. training set embedding looks like typical UMAP--MNIST result, test clusters right place relative training set. can celebrate victory successful use HNSW.","code":"install.packages(c(\"ggplot2\", \"Polychrome\")) library(ggplot2) library(Polychrome) palette <- as.vector(Polychrome::createPalette(   length(levels(mnist$Label)) + 2,   seedcolors = c(\"#ffffff\", \"#000000\"),   range = c(10, 90) )[-(1:2)]) ggplot(   data.frame(mnist_train_umap$embedding, Digit = mnist_train$Label),   aes(x = X1, y = X2, color = Digit) ) +   geom_point(alpha = 0.5, size = 1.0) +   scale_color_manual(values = palette) +   theme_minimal() +   labs(     title = \"MNIST training set UMAP\",     x = \"\",     y = \"\",     color = \"Digit\"   ) +   theme(legend.position = \"right\") +   guides(color = guide_legend(override.aes = list(size = 5, alpha = 1))) ggplot(   data.frame(mnist_test_umap, Digit = mnist_test$Label),   aes(x = X1, y = X2, color = Digit) ) +   geom_point(alpha = 0.5, size = 1.0) +   scale_color_manual(values = palette) +   theme_minimal() +   labs(     title = \"MNIST test set UMAP\",     x = \"\",     y = \"\",     color = \"Digit\"   ) +   theme(legend.position = \"right\") +   guides(color = guide_legend(override.aes = list(size = 5, alpha = 1)))"},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"hnsw-parameters","dir":"Articles","previous_headings":"HNSW for nearest neighbors","what":"HNSW parameters","title":"Using HNSW for nearest neighbors","text":"umap number parameters can set control behavior nearest neighbor search default method, Annoy, namely n_trees search_k. used nn_method = \"hnsw\" set. Instead, can control behavior HNSW index passing list arguments nn_args. HNSW, can set: M: number bi-directional links created every new element construction. Higher values mean accurate search, slower construction larger index size. Default 16. ef_construction: size dynamic list nearest neighbors construction. Higher values mean accurate search, slower construction larger index size. Default 200. ef: size dynamic list nearest neighbors search. Higher values mean accurate search, slower search. Default 10, set n_neighbors latter larger. example something like:","code":"iris_umap <- umap(iris, nn_method = \"hnsw\",                    nn_args = list(M = 12, ef_construction = 64, ef = 20))"},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"using-rcpphnsw-directly","dir":"Articles","previous_headings":"HNSW for nearest neighbors","what":"Using RcppHNSW Directly","title":"Using HNSW for nearest neighbors","text":"nn_method = \"hnsw\" new feature uwot. previous versions, still make use HNSW neighbors need work RcppHNSW directly. rest article shows , can used way understand RcppHNSW works use external nearest neighbor data uwot.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"preparing-the-data","dir":"Articles","previous_headings":"HNSW for nearest neighbors > Using RcppHNSW Directly","what":"Preparing the data","title":"Using HNSW for nearest neighbors","text":"starting point still mnist_train mnist_test. reminder, dataframes numerical pixel intensities images well Label factor describing number digit represents. won’t use labels calculating nearest neighbors, fact RcppHNSW bit fussy uwot. wants numerical matrix data input, define function convert dataframe matrix containing numerical data generate matrices need.","code":"df2m <- function(X) {   as.matrix(X[, which(vapply(X, is.numeric, logical(1)))]) }  mnist_train_data <- df2m(mnist_train) mnist_test_data <- df2m(mnist_test)"},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"using-rcpphnsw","dir":"Articles","previous_headings":"HNSW for nearest neighbors","what":"Using RcppHNSW","title":"Using HNSW for nearest neighbors","text":"Now load RcppHNSW: Time build index using training data. default settings hnsw_build perfectly good MNIST, ’ll go . recommend using many threads can stage. use 6 threads:","code":"library(RcppHNSW) mnist_index <- hnsw_build(mnist_train_data, n_threads = 6)"},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"mnist-training-data-k-nearest-neighbors","dir":"Articles","previous_headings":"HNSW for nearest neighbors > Using RcppHNSW","what":"MNIST training data k-nearest neighbors","title":"Using HNSW for nearest neighbors","text":"Now query index training data find k-nearest neighbors training data. Note case built index data querying . index building, can get good performance default settings, ’s recommended use many threads can. searching substantially faster index building, though. extra parameter need specify number neighbors want. uwot, default number neighbors 15, shall use k parameter: separate article uwot’s nearest neighbor format good news output format RcppHNSW already right format uwot (coincidence: created maintain RcppHNSW package). Nonetheless, let’s take look output, list two matrices: first matrix, idx, contains indices nearest neighbors point training data. second matrix, dist, contains distances nearest neighbors. Let’s take look dimensions matrices: 60,000 rows, one point training data, 15 columns, one nearest neighbor. Let’s take look first rows columns matrix: k-nearest neighbors dataset, ’s often convention nearest neighbor item item , nearest neighbor first item index 1, distance zero. nearest neighbor packages follow convention, uwot , ’re good.","code":"mnist_train_knn <-   hnsw_search(     mnist_train_data,     mnist_index,     k = 15,     n_threads = 6   ) names(mnist_train_knn) [1] \"idx\"  \"dist\" lapply(mnist_train_knn, `dim`) $idx [1] 60000    15  $dist [1] 60000    15 mnist_train_knn$idx[1:3, 1:3] [,1]  [,2]  [,3] [1,]    1 32249  8729 [2,]    2   640 51122 [3,]    3 54198 46129 mnist_train_knn$dist[1:3, 1:3] [,1]     [,2]     [,3] [1,]    0 1561.472 1591.601 [2,]    0 1020.647 1100.529 [3,]    0 1377.631 1541.127"},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"mnist-test-set-query-neighbors","dir":"Articles","previous_headings":"HNSW for nearest neighbors > Using RcppHNSW","what":"MNIST test set query neighbors","title":"Using HNSW for nearest neighbors","text":"also need test set neighbors, let’s now. going build index test set, instead query test set item training set index: output format training set neighbors, surprises : first indices distances test set: Remember queried test set training set, test set indices don’t get chance neighbors . Consequently, also nearest neighbor distances zero.","code":"mnist_test_query_neighbors <-   hnsw_search(     mnist_test_data,     mnist_index,     k = 15,     n_threads = 6,     verbose = TRUE   ) lapply(mnist_test_query_neighbors, `dim`) $idx [1] 10000    15  $dist [1] 10000    15 mnist_test_query_neighbors$idx[1:3, 1:3] [,1]  [,2]  [,3] [1,] 53844 38621 16187 [2,] 28883 49161 24613 [3,] 58742 46513 15225 mnist_test_query_neighbors$dist[1:3, 1:3] [,1]      [,2]      [,3] [1,]  676.5841  793.9868  862.6766 [2,] 1162.9316 1211.8445 1285.9285 [3,]  321.6629  332.4635  341.0484"},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"using-hnsw-k-nearest-neighbors-with-umap","dir":"Articles","previous_headings":"HNSW for nearest neighbors","what":"Using HNSW k-nearest neighbors with UMAP","title":"Using HNSW for nearest neighbors","text":"now ready use HNSW k-nearest neighbors uwot.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"umap-on-training-data-with-hnsw-knn","dir":"Articles","previous_headings":"HNSW for nearest neighbors > Using HNSW k-nearest neighbors with UMAP","what":"UMAP on training data with HNSW knn","title":"Using HNSW for nearest neighbors","text":"UMAP works nearest neighbor graph, pass pre-computed nearest neighbor data, don’t actually need give data. : set X = NULL. pass nearest neighbor data nn_method parameter. parameters used nn_method = \"hnsw\", see description. Nothing exciting output, except notice last line output reminding us want transform new data better generated neighbors data . , ’re ok.","code":"mnist_train_umap <-   umap(     X = NULL,     nn_method = mnist_train_knn,     batch = TRUE,     n_epochs = 500,     n_sgd_threads = 6,     ret_model = TRUE,     verbose = TRUE   ) UMAP embedding parameters a = 1.896 b = 0.8006 Commencing smooth kNN distance calibration using 6 threads with target n_neighbors = 15 Initializing from normalized Laplacian + noise (using irlba) Commencing optimization for 500 epochs, with 1239236 positive edges using 6 threads Using method 'umap' Optimizing with Adam alpha = 1 beta1 = 0.5 beta2 = 0.9 eps = 1e-07 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----| **************************************************| Optimization finished Note: model requested with precomputed neighbors. For transforming new data, distance data must be provided separately"},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"transforming-the-test-data","dir":"Articles","previous_headings":"HNSW for nearest neighbors > Using HNSW k-nearest neighbors with UMAP","what":"Transforming the test data","title":"Using HNSW for nearest neighbors","text":"Let us now transform test data using UMAP model just created. far fewer nobs twiddle transforming new data mainly baked UMAP model. , pass X = NULL don’t need original test set data, pass test set nearest neighbor data nn_method parameter. also need pass UMAP model just created. point visualize data , can take word looks just like nn_method = \"hnsw\" approach.","code":"mnist_test_umap <-   umap_transform(     X = NULL,     model = mnist_train_umap,     nn_method = mnist_test_query_neighbors,     n_sgd_threads = 6,     verbose = TRUE   ) Read 10000 rows Processing block 1 of 1 Commencing smooth kNN distance calibration using 6 threads with target n_neighbors = 15 Initializing by weighted average of neighbor coordinates using 6 threads Commencing optimization for 167 epochs, with 150000 positive edges using 6 threads Using method 'umap' Optimizing with Adam alpha = 1 beta1 = 0.5 beta2 = 0.9 eps = 1e-07 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----| **************************************************| Finished"},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"conclusions","dir":"Articles","previous_headings":"HNSW for nearest neighbors","what":"Conclusions","title":"Using HNSW for nearest neighbors","text":"summarize, use HNSW uwot following: Make sure RcppHNSW installed. Set nn_method = \"hsnw\". Pass nn_args list containing combination M, ef_construction ef control speed/accuracy trade-. transform new data, just use umap_transform normal. use RcppHNSW directly UMAP following: build index data hnsw_build. query index hnsw_search obtain k-nearest neighbors. run UMAP umap setting passing neighbor data nn_method. transform new data, thing following additions modifications: use hnsw_search new data index created first step get neighbors new data (respect training data). run UMAP, set ret_model = TRUE get UMAP model back. use umap_transform UMAP model pass new data’s neighbors nn_method. also don’t need keep original data around neighbors, can set X = NULL running umap umap_transform. data can still useful initialization. example want use PCA-based initialization need keep original data around. ’s necessary default settings umap.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Initialization","text":"default initialization UMAP uses eigenvectors normalized graph Laplacian sparse affinity matrix. normally works well except : graph disconnected components, case really multiple separate graphs. uwot, risk RSpectra can trouble converging circumstances. Even connected graph, can sometimes convergence issues, tried mitigate. However, still risk long initialization, possibly hang. Unfortunately, interrupting RSpectra can result requiring entire R session lost.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"pca-initialization","dir":"Articles","previous_headings":"Introduction","what":"PCA Initialization","title":"Initialization","text":"alternative using spectral initialization use PCA, although depending scaling input variables, lead quite spread-initial set coordinates, can hard optimize. related technique t-SNE, Kobak Berens recommend scaling PCA results standard deviation dimension 1e-4. can get behavior uwot using umap function init = \"spca\" umap2 init = \"spca\", init_sdev = 1e-4.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"changes-with-umap2","dir":"Articles","previous_headings":"Introduction","what":"Changes with umap2","title":"Initialization","text":"Based years experience, umap2 function slightly different defaults recommend umap. umap2 function, see umap2 article. recommend using init = \"pca\". point wrote uwot (just never noticed), Python UMAP package added extra scaling coordinates contained bounding box side length 10. now default umap2 seems work pretty well. results shown page now use umap2.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"if-you-are-still-using-umap","dir":"Articles","previous_headings":"Introduction","what":"If you are still using umap","title":"Initialization","text":"still using umap, init = \"spca\" option works ok, standard deviation 1e-4 may small datasets. see results suspicious clusters broken , recommend trying value init_sdev 1e-2, 1 2.5. want use Python UMAP-style scaling umap, can get setting init_sdev = \"range\", probably best choice.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"the-approximate-global-spectral-initialization","dir":"Articles","previous_headings":"Introduction","what":"The Approximate Global Spectral Initialization","title":"Initialization","text":"PCA solid choice initialization spectral embedding fails. using PCA ignores information affinity matrix might useful initialization: example, using supervised UMAP mixing different types distance metrics. conceptually, using affinity matrix lot appeal. Dzwinel co-workers describe fast multi-dimensional scaling-like method called ivhd generates connected graph, knowing neighbor non-neighbor points pretending target distances 0 1, respectively. Similar theoretical support given Linderman co-workers. basis work, implemented “approximate global” spectral method initialization: minor modification standard spectral initialization: non-zero elements affinity matrix set 1. random number zero elements set 0.1, rate n_neg_samples positive edge graph. consistent ratio neighbor non-neighbor edge sampling optimization , using default value n_neg_samples, similar order magnitude ratio neighbor non-neighbor entries graph used ivhd. Theoretically, enough guarantee graph single component. spectral initialization carried . initialization, original affinity matrix used optimization. can get behavior uwot init = \"agspectral\".","code":""},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"laplacian-eigenmap","dir":"Articles","previous_headings":"Introduction","what":"Laplacian Eigenmap","title":"Initialization","text":"Another option connected graph use variation normalization used graph Laplacian get something close Laplacian Eigenmap init = \"laplacian\". Luxburg’s spectral clustering tutorial (PDF) form normalization recommended used init = \"spectral\" normalization. ’ll try see makes difference.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"random-initialization","dir":"Articles","previous_headings":"Introduction","what":"Random Initialization","title":"Initialization","text":"else fails, can use random initialization. spirit generosity, uwot offers two types random initialization. first used UMAP, uses random uniform distribution -10 +10 along axis. can get init = \"rand\". Alternatively, method favoured LargeVis (used default lvish), use Gaussian distribution standard deviation 1e-4. also method used t-SNE. can get setting init = \"lvrand\". summarize options: , ’ll explore effect settings. details datasets, see examples page. Apart changing init, mainly default settings used. save bit time, used pre-calculated exact nearest neighbors, don’t expect major effect results. typical commands using iris dataset :","code":"# Default initialization: use spectral initialization if possible # falling back to scaled PCA if multiple components are found in the graph embedding <- umap2(data)  # same as the above embedding <- umap2(data, init = \"spectral\")  # use (scaled) PCA: defaults are different between umap and umap2 embedding <- umap2(data, init = \"pca\")  # If you want to use `umap` then you can use the following: # umap with default scaled PCA embedding <- umap(data, init = \"spca\") # equivalent to: embedding <- umap(data, init = \"pca\", init_sdev = 1e-4) # but this might be a better choice of settings embedding <- umap(data, init = \"pca\", init_sdev = \"range\")  # use an \"approximate global\" spectral initialization that should be applicable # even under conditions where \"spectral\" fails and has to use spca embedding <- umap2(data, init = \"agspectral\")  # use a Laplacian Eigenmap style initialization, which will also fall back to # spca if it has to embedding <- umap2(data, init = \"laplacian\")  # use random initialization, UMAP style embedding <- umap2(data, init = \"rand\") # use random initialization, t-SNE/LargeVis style embedding <- umap2(data, init = \"lvrand\") set.seed(1337) # Using the `umap` function with spca initialization iris_umap <- umap(iris, init = \"spca\") # Using the `umap2` with agspectral with slightly different (and hopefully # better) defaults iris_umap <- umap2(iris, init = \"agspectral\")"},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"datasets","dir":"Articles","previous_headings":"","what":"Datasets","title":"Initialization","text":"details see examples article.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"spectral-vs-pca","dir":"Articles","previous_headings":"","what":"Spectral vs PCA","title":"Initialization","text":"examples using either PCA initialization spectral approach. first row, left-hand image result using init = \"pca\", right-hand image init = \"agspectral\". second row, left-hand image init = \"spectral\". right-hand image init = \"laplacian\". datasets don’t second row images, settings used , generate one component graph, therefore init = \"spectral\" init = \"laplacian\" fall back similar results seen init = \"pca\".","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"isoswiss","dir":"Articles","previous_headings":"Spectral vs PCA","what":"isoswiss","title":"Initialization","text":"really minor differences PCA results spectral results cases. big difference occurs isoswiss. shows Achilles heel PCA initialization: respect manifolds. spectral initialization able “unfold” structure, PCA leaves folded projection onto 2D leaves broken three pieces. () cases, agspectral results similar spectral results. isoswiss shows limitations method. Like PCA, manifold initialized properly unpleasant breakage occurs. certainly worse ways initialize dataset though, shall see . don’t think results fundamental limitation agspectral approach, balance random vs near neighbor affinities may ideal dataset. laplacian results also strongly resemble spectral initialization, differ ’s obvious one strictly better , terms initialization UMAP, doesn’t seem strong advantage using one type graph Laplacian .","code":""},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"random-initialization-rand","dir":"Articles","previous_headings":"","what":"Random Initialization: rand","title":"Initialization","text":"two results using random initialization, can get sense much variation can expect getting lucky initialization.","code":"# left-hand plots used this seed set.seed(1337) # right-hand plots used this seed # set.seed(42) iris_umap <- umap(iris, init = \"rand\")"},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"isoswiss-1","dir":"Articles","previous_headings":"Random Initialization: rand","what":"isoswiss","title":"Initialization","text":"Results terrible smaller datasets, anything mnist-sized, start see broken-clusters. oof, isoswiss disaster.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"random-initialization-lvrand","dir":"Articles","previous_headings":"","what":"Random Initialization: lvrand","title":"Initialization","text":"result using random initialization t-SNE/LargeVis style, use Gaussian distribution much smaller initial range coordinate values. make much difference?","code":"# left-hand plots used this seed set.seed(1337) # right-hand plots used this seed # set.seed(42) iris_umap <- umap(iris, init = \"lvrand\")"},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"isoswiss-2","dir":"Articles","previous_headings":"Random Initialization: lvrand","what":"isoswiss","title":"Initialization","text":", doesn’t make huge difference.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"recommendations","dir":"Articles","previous_headings":"","what":"Recommendations","title":"Initialization","text":"default spectral initialization good job. reason can’t use , example due disconnections, init = \"pca\" good choice. one exception data think manifold structure. case, hope connected graph, case won’t need use PCA. love say agspectral can help difficulty slow spectral initialization, general seems useful, may work well manifold data. laplacian initialization provide meaningful difference spectral initialization based results shown . Random initialization used last resort. Increasing n_neighbors can sometimes help bit, may need set high value (e.g. 150) may introduce long-distance interactions doesn’t represent data.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"densmap","dir":"Articles","previous_headings":"","what":"densMAP","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"densMAP variation UMAP attempts preserve differences density original data adding new component cost function. available Python UMAP implementation setting densmap=True. See also excellent tutorial. request add densMAP uwot since start 2021. document describes approximate version densMAP available uwot may good enough. first, bit densMAP. densMAP can seen adding regularization UMAP cost function “local radius” observation input output space calculated Pearson correlation two optimized. radius observation ii, RiR_i calculated : Ri=1∑iPij∑idij2Pij R_i = \\frac{1}{\\sum_i P_{ij}} \\sum_i d_{ij}^2 P_{ij} dijd_{ij} distance point ii jj PijP_{ij} symmetrized edge weight ii jj, .e. radius edge-weighted average squared distances ii neighbors. matrix symmetrized, ii may different number neighbors n_neighbors parameter user specifies. get feel densMAP , results Python UMAP (.e. uwot output ), without densmap=True. recommended current UMAP README, also set n_neighbors=30, except subset_clusters, ’ll explain get . Everything else left defaults. images : Top left: UMAP results colored typical label dataset. cryptic numbers subtitle feel free ignore. Top right: UMAP results point colored log input local radius, using ColorBrewer Spectral palette. Red means small radius, blue means large radius. show areas densMAP likely contract expand, respectively. Middle left: densMAP results colored typical label dataset. Middle right: densMAP results colored local radius. Bottom: plot log input local radii vs log output radii Pearson correlation . radii returned fit_transform method densmap=True. 1000 points, used smoothScatter plot results. First two datasets simple simulation sets based Use t-SNE Effectively, specifically sections 2 (“Cluster sizes t-SNE plot mean nothing”) 6 (“topology, may need one plot”).","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"two_different_clusters","dir":"Articles","previous_headings":"densMAP","what":"two_different_clusters","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"dataset consists two 50D Gaussians 5,000 points . One clusters standard deviation 10x . used snedata generate data (via two_different_clusters_data function). Like t-SNE, default UMAP shows clusters size. densMAP shows one clusters smaller. good start.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"subset_clusters","dir":"Articles","previous_headings":"densMAP","what":"subset_clusters","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"dataset features two 50D gaussians 5,000 points , time overlap entirely. larger cluster standard deviation 50, smaller standard deviation 1. dataset used n_neighbors=150, much larger n_neighbors=30 setting used plots. consistent plots generated uwot. uwot precalculated exact nearest neighbors datasets dataset, using exact 30 nearest neighbors results smaller cluster embedded edge larger cluster. Presumably 30 nearest neighbors doesn’t result enough edges two clusters properly situate smaller cluster. approximate nearest neighbors routine used UMAP results edges sufficiently far away neighbors create neighborhood reflects global structure better. didn’t investigate thoroughly, sounds plausible . UMAP considers two clusters size. densMAP, ring-shaped structure found high perplexities t-SNE shows . least small cluster inside big one. Note re-run n_neighbors=30 results noticeably changed.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"s1k","dir":"Articles","previous_headings":"densMAP","what":"s1k","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"synthetic dataset like use: 1000 points fuzzy 10D simplex. can’t say like densMAP done . one point flung far away rest plot. can see coloring large local radius. perhaps indicates local radius calculation can affected edge effects.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"swissroll","dir":"Articles","previous_headings":"densMAP","what":"swissroll","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"swiss roll dataset devising. densMAP noticeably better job ripping swiss roll UMAP . expect local density fairly uniform across 2D manifold don’t know ’s priori reason thought densMAP better job .","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"scurvehole","dir":"Articles","previous_headings":"densMAP","what":"scurvehole","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"3D S-curve hole data set, used validate PaCMAP method (see also github repo).","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"iris","dir":"Articles","previous_headings":"densMAP","what":"iris","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"Ronald Fisher’s iris dataset.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"spenguins","dir":"Articles","previous_headings":"densMAP","what":"spenguins","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"Palmer Penguins. s spenguins stands scaled, filtered entries missing values Z-scaled inputs.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"mammoth","dir":"Articles","previous_headings":"densMAP","what":"mammoth","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"3D point cloud mammoth Smithsonian, Understanding UMAP, based work originally done Max Noichl.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"oli","dir":"Articles","previous_headings":"densMAP","what":"oli","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"Olivetti Faces images.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"frey","dir":"Articles","previous_headings":"densMAP","what":"frey","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"Frey Faces images.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"isofaces","dir":"Articles","previous_headings":"densMAP","what":"isofaces","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"faces dataset used Isomap (processed via gist).","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"mnist","dir":"Articles","previous_headings":"densMAP","what":"mnist","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"MNIST digits images. orange cluster 1 digits smaller radius exceptionally obvious coloring UMAP plot radius. noticeably shrinks densMAP goes work .","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"fashion","dir":"Articles","previous_headings":"densMAP","what":"fashion","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"Fashion MNIST images. Similarly MNIST digits, orange cluster (time images pants) noticeably smaller clusters.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"kuzushiji","dir":"Articles","previous_headings":"densMAP","what":"kuzushiji","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"Kuzushiji MNIST images.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"norb","dir":"Articles","previous_headings":"densMAP","what":"norb","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"Small NORB images. can see largest blue/purple ring-like structures broken UMAP, densMAP preserves . fact, densMAP plot one nicer UMAP layouts small NORB datasets ’ve seen.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"coil20","dir":"Articles","previous_headings":"densMAP","what":"coil20","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"COIL-20 images.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"coil100","dir":"Articles","previous_headings":"densMAP","what":"coil100","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"COIL-100 images.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"cifar10","dir":"Articles","previous_headings":"densMAP","what":"cifar10","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"CIFAR-10 images.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"macosko2015","dir":"Articles","previous_headings":"densMAP","what":"macosko2015","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"macosko2015 RNAseq data. dataset (RNA sequence dataset) isn’t lot fun visualize vanilla UMAP, densMAP improve matters.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"tasic2018","dir":"Articles","previous_headings":"densMAP","what":"tasic2018","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"RNAseq data Allen Brain Atlas originally reported Tasic co-workers. Another RNAseq dataset, densMAP interesting effect cluster sizes, clusters low density, effect squashing center plot.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"lamanno2020","dir":"Articles","previous_headings":"densMAP","what":"lamanno2020","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"Another RNASeq data, found via Picasso example notebook. think publication reference https://doi.org/10.1038/s41586-021-03775-x (published biorXiv 2020).","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"ng20","dir":"Articles","previous_headings":"densMAP","what":"ng20","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"20 Newsgroups dataset, manually converted dense 3000D PCA result: see gist. densMAP noticeably hard time mapping local radii 2D output. take away : densMAP good job maintaining correlation input output radii. time fails 20 Newsgroups dataset. good correlation doesn’t necessarily mean helpful static visualization. results clusters highly dispersed may reflect true relationship densities makes hard actually see cluster, rest data gets crushed middle plot. ’s necessarily problem interactive plot can pan zoom. can change regularization value dens_lambda. didn’t try reason doubt satisfying value can found datasets didn’t like output default settings. don’t currently want implement densMAP involves adding lots extra code, much C++ lazy.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"approximating-densmap","dir":"Articles","previous_headings":"","what":"Approximating densMAP","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"’s pitch approximation densMAP, named Lightweight Estimate Preservation Local Density (Leopold). input radii, part smooth k-nearest neighbor distances routine already calculate ρ\\rho distance nearest neighbor item dataset (subject local_connectivity constraint). Additionally, also calculate σ\\sigma normalization factor used neighbors distance greater ρ\\rho. must units distance, larger distances neighbors, larger σ\\sigma gets. propose following definition local radius: Ri=ρ+σ R_i = \\rho + \\sigma biggest difference definition local input radii haven’t bothered square distances (doesn’t make much difference leopold) values σ\\sigma ρ\\rho calculated symmetrization UMAP input edge weights. values missing influence observations outside initial k-nearest neighborhood graph. output radii, know looking output weight function parameters, increasing parameter makes clusters UMAP plot shrink, vice versa. let’s use measure output density, .e. inverse local radius. every point radius, value given weight points ii jj geometric mean two radii: wij=1/(1+dij2rirj) w_{ij} = 1 / \\left(1 + \\frac{d_{ij}^2}{\\sqrt{r_i r_j}} \\right) rir_i scaled version input radius RiR_i suitable lower dimensional space. similar sort scaling used Zelnik-Manor Perona “self-tuning” spectral clustering, also recommended local scaling reduce hubness high dimensional space Schnitzer co-workers. scaling input output space sophisticated: observed typical values bb used UMAP, usable range aa values 0.01 (diffuse clusters) 100 (tight clusters), just map inverse input local radii range. final adjustment good introduce parameter controls much range used output thus big disparity input radii reflected output. end, adjustable parameter leopold: dens_scale parameter. Set 1 uses available range . Set 0 get plain old UMAP back, fixed global value specified user (via spread min_dist parameters). Intermediate values get intermediate results. mind scaling radii straightforward, looking description maybe ’s . procedure : Define minimum log density ×10−2sa \\times 10^{-2s} aa UMAP parameter output ss den_scale parameter. Define maximum log density ×102sa \\times 10^{2s}. Range scale log(1/Ri)\\log \\left(1 / R_i \\right) minimum maximum log density. Call range scaled log density Δi\\Delta_i ai=exp(Δi)a_i = \\sqrt{ \\exp \\left( \\Delta_i \\right) }. leopold output weight function wij=1/(1+aiajdij2)w_{ij} = 1 / \\left( 1 + a_i a_j d_{ij}^2 \\right).","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"the-advantage-of-leopold","dir":"Articles","previous_headings":"Approximating densMAP","what":"The Advantage of leopold","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"main advantage : implement don’t write much new code: ρ\\rho σ\\sigma already calculated implementing per-observation value output function requires creating new version umap gradient routine expects vector values rather scalar. lot less work implementing densMAP.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"the-disadvantage-of-leopold","dir":"Articles","previous_headings":"Approximating densMAP","what":"The Disadvantage of leopold","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"’s bit approximate. really want technique makes dense clusters get smaller diffuse clusters get bigger. exact changes seem less important long vaguely sensible. don’t claim good sense expect see blob data intrinsic dimensionality dd radius rr embedded 2D versus one intrinsic dimensionality d+5d + 5 radius r/2r / 2. Even accurately embedded, even look sensible let alone helpful?","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"results","dir":"Articles","previous_headings":"","what":"Results","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"plots using leopold. used exact nearest neighbors results nearest neighbor calculations usually slowest part UMAP. following deviations default parameters used: min_dist = 0.1 closer Python UMAP results. consistency densMAP, n_neighbors = 30 except subset_clusters uses n_neighbors = 150. Also consistent densMAP, extra 200 epochs top usual default n_epochs used. plots : top left: output running leopold, Top left: leopold results colored typical label dataset. Top right: UMAP results point colored log input local radius, using ColorBrewer Spectral palette. Red means small radius, blue means large radius. show areas densMAP likely contract expand, respectively. Bottom left: plot log input local radii vs log output radii Pearson correlation . 1000 points, used smoothScatter plot results. Bottom right: plot log densmap input local radii leopold equivalent (ρ+σ\\rho + \\sigma). number title Pearson correlation. ’s particular reason two measures local radius , show similar trend. lot datasets, won’t anything say unless interesting contrast similarity densMAP.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"two_different_clusters-1","dir":"Articles","previous_headings":"Results","what":"two_different_clusters","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"leopold able produce two different sizes cluster, although size disparity much larger densMAP. due bimodal nature standard deviations dataset. One cluster gets one end radii scale (highly disperse) second cluster gets (highly compact). Turning dens_scale something like 0.2 works well data set. doesn’t really matter artificial dataset. long clusters right relative size happy.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"subset_clusters-1","dir":"Articles","previous_headings":"Results","what":"subset_clusters","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"shows interesting difference densMAP: densMAP, outer yellow ring uniform density, whereas leopold noticeable density gradient towards center. leave decide good bad thing.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"s1k-1","dir":"Articles","previous_headings":"Results","what":"s1k","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"point got pushed far away rest data densMAP plot far away leopold.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"swissroll-1","dir":"Articles","previous_headings":"Results","what":"swissroll","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"one scurvehole seem pretty comparable densMAP results.","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"mammoth-1","dir":"Articles","previous_headings":"Results","what":"mammoth","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"compare densMAP, extremities much diffuse leopold.","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"mnist-1","dir":"Articles","previous_headings":"Results","what":"mnist","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"main thing hoped see orange ‘1’ cluster shrink. . yellow ‘2’ cluster also noticeably grown. Changes subtle clusters, apart ‘1’ cluster densities quite similar. ’m happy .","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"fashion-1","dir":"Articles","previous_headings":"Results","what":"fashion","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"leopold behaves densMAP-like way shrinking orange cluster. clusters densMAP expands also larger , diffuse densMAP makes . personally prefer leopold layout, biased probably just ignore .","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"norb-1","dir":"Articles","previous_headings":"Results","what":"norb","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"layout fine, maybe bit neater UMAP one. still like densMAP version better.","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"macosko2015-1","dir":"Articles","previous_headings":"Results","what":"macosko2015","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"think layout leopold provides legible densMAP equivalent.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"tasic2018-1","dir":"Articles","previous_headings":"Results","what":"tasic2018","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"Another example leopold produces less compressed result, diffuse clusters (e.g. clusters bottom plot) expanded.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"ng20-1","dir":"Articles","previous_headings":"Results","what":"ng20","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"layout look super-great? . can leopold reproduce input radii output space, seems fact worse job densMAP. suffers less outliers densMAP something.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"probably clear don’t think ’s anything wrong densMAP. just currently lazy make changes uwot need implemented. Hence leopold’s creation. Neither leopold’s definition local radii results exactly map densMAP. don’t go looking reproduce densMAP plots exactly . think leopold results look fine. dense clusters get smaller diffuse clusters get larger, although latter case extent densMAP. However, isn’t big disadvantage perspective. densMAP better job terms correlation input output radii. Qualitatively, densMAP leopold results quite similar. correlations leopold definition local radii densMAP version pretty high real datasets. venture suggest encode quite similar information. want use leopold uwot: Set dens_scale parameter value 0 (exactly like UMAP) 1 (use full range useful values). Using leopold doesn’t seem require extra epochs don’t need worry run normal UMAP number epochs applying leopold. seem benefit larger value n_neighbors. re-run results n_neighbors = 15 default n_epochs noticed big effect compared results , showing already way many images page. can transform new data models generated dens_scale set. UMAP transforms new data, adjusted local connectivity used affects σ\\sigma ρ\\rho estimated values RiR_i precise like, effect seems quite small. Even don’t use local radii generate UMAP layout, coloring results log localr quite informative parts embedding expect shrink expand. conservative use leopold run UMAP normally, visualize results coloring localr obvious clusters lighter darker hue, run dens_scale = 1 current coordinates couple hundred epochs. ’s use iris:","code":"iris_leopold <- umap(iris, dens_scale = 1) # and if you want the local radii values iris_leopold <- umap(iris, dens_scale = 1, ret_extra = c(\"localr\"))"},{"path":"https://jlmelville.github.io/uwot/articles/locally-scaled-neighbors.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Locally Scaled Neighbors","text":"paper Self-Tuning Spectral Clustering, Zelnik-Manor Perona describe method local scaling determining affinity (similarity) two data points. defined : ̂ij=exp(−dij2σiσj) \\hat{}_{ij} = \\exp\\left(-\\frac{d^2_{ij}}{\\sigma_{} \\sigma_{j}}\\right) dijd_{ij} distance points ii jj, σi\\sigma_i σj\\sigma_j local scaling factors points ii jj respectively. Previous methods suggested empirically selecting fixed value σ\\sigma entire dataset, whereas Zelnik-Manor Perona suggest per-point scaling factor, based local density around point. choose local scaling factor, authors suggest “studying local statistics neighborhood” around point. studies use distance 7th neighbor. Although clearly connection local scaling factor perplexity parameter t-SNE, specific method used dimensionality reduction introduction TriMap used part weighting scheme triplet-based loss function, although local scaling factor modified average 4th-6th neighbor distances.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/locally-scaled-neighbors.html","id":"locally-scaled-nearest-neighbors","dir":"Articles","previous_headings":"","what":"Locally Scaled Nearest Neighbors","title":"Locally Scaled Neighbors","text":"Later, PaCMAP adopted TriMap’s local scaling way picking nearest neighbors rather using k-nearest neighbors directly. rationale given paper “scaling performed account fact neighborhoods different parts feature space significantly different magnitudes”. ’s : Select number neighbors kk. default, determined based number points dataset, order UMAP default n_neighbors = 15. Select number “extended” neighbors, k′k'. set 50. Find k+k′+1k + k' + 1 nearest neighbors point. + 1 bit account fact self-neighbor included nearest neighbor list (implementation detail PaCMAP code explicitly mentioned paper makes sense). extended nearest neighbors, calculate locally-scaled distances dij2/σiσjd^2_{ij}/\\sigma_{} \\sigma_{j} definition σi\\sigma_i TriMap. , self-neighbor, need use one neighbor mentioned paper, terms UMAP, actually use average distance fifth--seventh nearest neighbors define σi\\sigma_i. Return kk nearest neighbors based locally-scaled distances. PaCMAP actually make use distances near neighbors, paper stresses scaled distances “used selecting neighbors; used optimization”. curious whether using locally scaled neighbors UMAP neighbor graph provides benefit, although PaCMAP paper doesn’t mention effect discussion method’s performance. calculate extra 50 nearest neighbors bit computational burden, may motivated PaCMAP warn slow nearest neighbor search turn default PCA preprocessing step. However, calculating order ~65 nearest neighbors, typical UMAP run, typical (even little low) compared t-SNE run.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/locally-scaled-neighbors.html","id":"locally-scaled-nn-implementation","dir":"Articles","previous_headings":"","what":"Locally Scaled NN Implementation","title":"Locally Scaled Neighbors","text":"code generate LSNN graph use uwot. Use locally_scaled_knn create input dataframe, e.g. set nn_kind = \"exact\" can get exact nearest neighbors, slow. already dense k-nearest neighbor graph sufficient size, (e.g. least 51 neighbors per point), can use enn_to_lsnn convert locally scaled nearest neighbor graph size n_neighbors, e.g. can use UMAP like :","code":"lsnn15 <- locally_scaled_knn(data, n_neighbors = 15, n_extra = 50, n_threads = 6) lsnn15 <- enn_to_lsnn(enn, n_neighbors = 15) umap_res <- umap2(data, nn_method = lsnn15) locally_scaled_knn <- function(X,                                n_neighbors = 15,                                n_extra = 50,                                scale_from = 5,                                scale_to = 7,                                n_threads = 1,                                nn_kind = \"approx\",                                ret_knn = FALSE,                                verbose = FALSE) {   n <- nrow(X)   n_neighbors <- min(n_neighbors, n - 1)    enn <- extended_knn(     X,     n_neighbors,     n_extra = n_extra,     n_threads = n_threads,     nn_kind = nn_kind,     verbose = verbose   )    lsnn <- enn_to_lsnn(enn, n_neighbors, scale_from, scale_to)    if (ret_knn) {     return(list(lsnn = lsnn, knn = truncate_graph(enn, n_neighbors)))   }   lsnn }  enn_to_lsnn <- function(enn,                         n_neighbors,                         scale_from = 5,                         scale_to = 7,                         n_extra = 51) {   enn <- truncate_graph(enn, n_neighbors + n_extra)   n <- nrow(enn$idx)   n_neighbors <- min(n_neighbors, n - 1)    lsdist <- locally_scaled_distances(enn$idx,     enn$dist,     scale_from = scale_from,     scale_to = scale_to   )   enn_sorted <- sort_knn_graph(enn, sort_values = lsdist)   lsnn <- truncate_graph(enn_sorted, n_neighbors)   lsnn <- sort_knn_graph(lsnn)    lsnn }  extended_knn <- function(X,                          n_neighbors = 150,                          n_extra = 50,                          n_threads = 1,                          nn_kind = \"approx\",                          verbose = FALSE) {   n <- nrow(X)   n_neighbors_extra <- min(n_neighbors + n_extra, n - 1)   n_neighbors <- min(n_neighbors, n - 1)    if (nn_kind == \"exact\") {     knn <- rnndescent::brute_force_knn(       data = X,       k = n_neighbors_extra + 1,       n_threads = n_threads,       verbose = verbose     )   } else {     knn <- rnndescent::rnnd_knn(       data = X,       k = n_neighbors_extra + 1,       n_threads = n_threads,       verbose = verbose     )   }    knn }  # Zelnik-Manor, Pietro Perona. Self-tuning spectral clustering. In NIPS, 2004. locally_scaled_distances <- function(idx,                                      dist,                                      scale_from = 5,                                      scale_to = 7) {   if (scale_to < scale_from) {     stop(\"scale_to must be greater than or equal to scale_from\")   }   if (ncol(idx) != ncol(dist)) {     stop(\"idx and dist must have the same number of columns\")   }   if (ncol(idx) < scale_to) {     stop(\"idx and dist must have at least scale_to columns\")   }    sigma <- pmax(rowMeans(dist[, scale_from:scale_to]), 1e-10)   d2 <- dist * dist    # scale by sigma_i   d2_scaled_row <- sweep(d2, 1, sigma, FUN = \"/\")    # scale by sigma_j   sigma_j <- sigma[idx]   d2_scaled <- d2_scaled_row / sigma_j    d2_scaled }  truncate_graph <- function(graph, k = NULL) {   if (is.null(k)) {     return(graph)   }   if (k > ncol(graph$idx)) {     stop(\"k is greater than the number of neighbors\")   }   list(idx = graph$idx[, 1:k], dist = graph$dist[, 1:k]) }"},{"path":"https://jlmelville.github.io/uwot/articles/locally-scaled-neighbors.html","id":"datasets","dir":"Articles","previous_headings":"","what":"Datasets","title":"Locally Scaled Neighbors","text":"See examples article.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/locally-scaled-neighbors.html","id":"results","dir":"Articles","previous_headings":"","what":"Results","title":"Locally Scaled Neighbors","text":"Normal UMAP results left, LSNN results right. don’t see huge difference two results Maybe mammoth scurvehole results bit better LSNN. fashion ng20 outliers.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/locally-scaled-neighbors.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Locally Scaled Neighbors","text":"UMAP least, doesn’t seem like using locally-scaled neighbors makes enough difference ’s worth extra computation.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/lvish.html","id":"a-largevis-ish-method","dir":"Articles","previous_headings":"","what":"A LargeVis-ish method","title":"lvish","text":"LargeVis method (see also github), many respects bridge t-SNE UMAP. ’s sufficiently close UMAP uwot also offers LargeVis-like method, lvish: Although lvish like real LargeVis terms input weights, output weight function gradient, give results resemble real thing, note : Like real LargeVis, matrix input data normalized centering column entire matrix scaled dividing maximum absolute value. differs umap, scaling carried . Scaling can controlled scale parameter. Nearest neighbor results refined via neighbor expansion method. search_k parameter twice large Annoy’s default compensate. nearest neighbor index parameter, n_trees, dynamically chosen based data set size. LargeVis, ranges 10 (N < 100,000) 100 (N > 5,000,000). lvish default 50 cover datasets N = 5,000,000, combined default search_k, seems suitable datasets ’ve looked . Negative edges generated uniform sampling vertexes rather degree ^ 0.75. default number epochs dataset-dependent, generate number edge samples used default settings reference LargeVis implementation. normally results substantially longer run time umap. may able get away fewer epochs, using UMAP initialization init = \"spectral\", rather default Gaussian random initialization (init = \"lvrand\") can help. left-hand image result running official LargeVis implementation MNIST. image right running lvish default settings (apart setting n_threads = 8). Given initialized different random configurations, ’s reason believe identical, look pretty similar: default number neighbors 3 times perplexity, default perplexity = 50, nearest neighbor search needs find 150 nearest neighbors per data point, order magnitude larger UMAP defaults. leads less sparse input graph hence edges sample. Combined increased number epochs, expect lvish slower umap: default single-threaded settings, took 20 minutes embed MNIST data circumstances described “Performance” section. n_threads = 4, took 7 minutes. addition, storing extra edges requires lot memory umap defaults: R session increased around 3.2 GB, versus 1 GB umap. alternative usual Gaussian input weight function, can use k-nearest neighbor graph , setting kernel = \"knn\". give edge neighbors uniform weight equal 1/perplexity, leads row’s probability distribution target perplexity. matrix symmetrized usual way. advantage number neighbors reduced perplexity (indeed, n_neighbors parameter ignored setting), leads less memory usage faster runtime. can also get away setting perplexity much lower value usual kernel (e.g. perplexity = 15) get closer UMAP’s performance. use default LargeVis random initialization, still need epochs UMAP, can still expect see big improvement. Something like following works MNIST:","code":"# perplexity, init and n_epoch values shown are the defaults # use perplexity instead of n_neighbors to control local neighborhood size mnist_lv <- lvish(mnist, perplexity = 50, init = \"lvrand\", n_epochs = 5000,                   verbose = TRUE) # Make hilarious Lembas bread joke mnist_lv <- lvish(mnist, kernel = \"knn\", perplexity = 15, n_epochs = 1500,                   init = \"lvrand\", verbose = TRUE)"},{"path":"https://jlmelville.github.io/uwot/articles/lvish.html","id":"some-more-results","dir":"Articles","previous_headings":"","what":"Some More Results","title":"lvish","text":"details datasets, compare output UMAP t-SNE, see UMAP examples gallery.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/lvish.html","id":"gaussian-perplexity","dir":"Articles","previous_headings":"Some More Results","what":"Gaussian Perplexity","title":"lvish","text":"mentioned , default lvish uses Gaussian similarity function determine perplexities, just like t-SNE. results given . two images per dataset. left-hand image uses perplexity 15, similar sort settings UMAP uses. right-hand image perplexity 50, LargeVis default. non-default settings use pca = 100, reduces input dimensionality 100. Note default lvish uses random initialization much larger number epochs match LargeVis defaults. makes optimization take lot longer UMAP. LargeVis uses multiple threads optimization phase, lvish , ensure reproducibility results fixed random seed. get multi-threaded performance like LargeVis, add option, n_sgd_threads = \"auto\", e.g.: also suggest fix number epochs smaller value initially see provides adequate visualization.","code":"iris_lv15 <- lvish(iris, pca = 100, perplexity = 15) iris_lv50 <- lvish(iris, pca = 100, perplexity = 50) iris_lv15 <- lvish(iris, pca = 100, perplexity = 15, n_sgd_threads = \"auto\") iris_lv15 <- lvish(iris, pca = 100, perplexity = 15, n_sgd_threads = \"auto\", n_epochs = 500)"},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/lvish.html","id":"macosko2015","dir":"Articles","previous_headings":"Some More Results > Gaussian Perplexity","what":"macosko2015","title":"lvish","text":"Default initialization lvish, LargeVis t-SNE, random distribution. t-SNE, can see one issue sometimes clusters get split another cluster unable re-merge. MNIST easiest example image see . general, ’s huge difference effect increasing perplexity, larger datasets. smaller datasets ’s apparent resulting clusters tend spread larger perplexity values. norb (small NORB) dataset shows obvious difference, perplexity = 15 results clearly low, break structures apparent perplexity = 50. similar effect seen using UMAP, don’t think due random initialization lvish case. contributing factor likely initial PCA dimensionality reduction 100 dimensions aggressive NORB reduces nearest neighbor accuracy, recovered higher perplexities (requires finding near neighbors). hand, ’s hard see ’s going coil20 especially coil100 results. see going static images , apparent , contrast norb results, perplexity = 50 results high , loop structure clusters gets broken . coil100 coil20 results show issue using LargeVis (UMAP) isn’t normally problem t-SNE: extra repulsion cost function can often spread data quite far apart compared cluster sizes. t-SNE opposite problem clusters expanding large circular form makes discerning clusters harder datasets get larger, single static plot, find t-SNE results easier examine. UMAP lvish, may resort interactive means examining data, using embed_plotly function vizier. alternative lvish modify repulsion_strength parameter (referred gamma LargeVis). default value, 7 taken LargeVis paper seems chosen empiricially. results coil20 perplexity = 50 repulsion reduced repulsion_strength = 0.7 left image, repulsion_strength = 0.07 right: helps bit, limits: blue cluster right remains outlier, reducing repulsion_strength far causes loops shrink, can seen outlying blue black clusters right-hand plot.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/lvish.html","id":"knn-perplexity","dir":"Articles","previous_headings":"Some More Results","what":"KNN Perplexity","title":"lvish","text":"alternative using Gaussian perplexities, use k-nearest neighbor graph directly, involves setting similarity ii jj 1 ii k-nearest neighbors jj, 0 otherwise. usual t-SNE procedure symmetrizing (normalization step) carried . t-SNE implementations use kNN-derived perplexities, e.g. majorization-minimization approach Yang co-workers. advantage using kNN kernel get sparser set edges, due lvish using formula determining number iterations required, results shorter run time. results kNN perplexities used setting kernel = \"knn\" comparison, top row images use settings previous section Gaussian perplexities, bottom row show results using kNN kernel. run time embedding given image. used version 0.1.3 uwot CRAN.","code":"iris_lv15k <- lvish(iris, pca = 100, perplexity = 15, kernel = \"knn\") iris_lv50k <- lvish(iris, pca = 100, perplexity = 50, kernel = \"knn\")"},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/lvish.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"lvish","text":"smaller datasets, kNN kernel gives noticeably different results Gaussian perplexity, particularly perplexity = 50. iris, s1k oli, trend seems results expanded kNN kernel. frey, coil20 coil100, clusters separated. larger datasets, difference behavior less pronounced, although macosko2015 results show larger cluster separation well. good new cases, run times noticeably reduced. larger datasets, using kernel = \"knn\" seems ok choice reducing runtime lvish. also seems may want use smaller value perplexity Gaussian perplexity, reduces runtime. smaller datasets, results mixed. seems smaller perplexity case definitely preferred.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/metric-learning.html","id":"visualization","dir":"Articles","previous_headings":"","what":"Visualization","title":"Metric Learning with UMAP","text":"produce plots , used vizier package, can installed using: ’ll show commands produce plots displayed.","code":"devtools::install_github(\"jlmelville/vizier\")"},{"path":"https://jlmelville.github.io/uwot/articles/metric-learning.html","id":"supervised-learning","dir":"Articles","previous_headings":"","what":"Supervised Learning","title":"Metric Learning with UMAP","text":"’ll compare supervised result standard run UMAP: supervised learning, provide suitable vector labels y argument umap (tumap): Let’s take look results, unsupervised embedding left, supervised version right: Clearly, supervised UMAP done much better job separating classes, although also retained relative location clusters pretty well, .","code":"set.seed(1337) fashion_umap <- umap(fashion) set.seed(1337) fashion_sumap <- umap(fashion, y = fashion$Description) vizier::embed_plot(fashion_umap, fashion, cex = 0.5, title = \"Fashion UMAP\", alpha_scale = 0.075) vizier::embed_plot(fashion_sumap, fashion, cex = 0.5, title = \"Fashion Supervised UMAP\", alpha_scale = 0.075)"},{"path":"https://jlmelville.github.io/uwot/articles/metric-learning.html","id":"metric-learning","dir":"Articles","previous_headings":"","what":"Metric Learning","title":"Metric Learning with UMAP","text":"’s also possible use existing embedding embed new points. Fashion MNIST comes suggested split training (first 60,000 images) test (remaining 10,000 images) sets, ’ll use : Training proceeds running UMAP normally, need return just embedded coordinates. return enough information embed new data, need set ret_model flag run umap. return list. embedded coordinates can found embedding item.","code":"fashion_train <- head(fashion, 60000) fashion_test <- tail(fashion, 10000)"},{"path":"https://jlmelville.github.io/uwot/articles/metric-learning.html","id":"training","dir":"Articles","previous_headings":"Metric Learning","what":"Training","title":"Metric Learning with UMAP","text":"training, shall continue use standard UMAP: supervised UMAP: results shouldn’t different full-dataset embeddings, let’s take look anyway: Everything looks order . standard UMAP training plot flipped along y-axis compared full dataset, doesn’t matter.","code":"set.seed(1337) fashion_umap_train <- umap(fashion_train, ret_model = TRUE) set.seed(1337) fashion_sumap_train <- umap(fashion_train, ret_model = TRUE, y = fashion_train$Description) vizier::embed_plot(fashion_umap_train$embedding, fashion_train, cex = 0.5, title = \"Fashion Train UMAP\", alpha_scale = 0.075) vizier::embed_plot(fashion_sumap_train$embedding, fashion_train, cex = 0.5, title = \"Fashion Train Supervised UMAP\", alpha_scale = 0.075)"},{"path":"https://jlmelville.github.io/uwot/articles/metric-learning.html","id":"embedding-new-data","dir":"Articles","previous_headings":"Metric Learning","what":"Embedding New Data","title":"Metric Learning with UMAP","text":"embed new data, use umap_transform function. Pass new data trained UMAP model. ’s difference using standard UMAP model: supervised UMAP model: results: test data results obviously embedded similar way training data. particular interest test results supervised model, clusters stay well separated compared unsupervised results, although misclassifications shirts, t-shirts, coats pullover classes (green, blue red clusters right supervised UMAP plot).","code":"set.seed(1337) fashion_umap_test <- umap_transform(fashion_test, fashion_umap_train) set.seed(1337) fashion_sumap_test <- umap_transform(fashion_test, fashion_sumap_train) vizier::embed_plot(fashion_umap_test, fashion_test, cex = 0.5, title = \"Fashion Test UMAP\", alpha_scale = 0.075) vizier::embed_plot(fashion_sumap_test, fashion_test, cex = 0.5, title = \"Fashion Test Supervised UMAP\", alpha_scale = 0.075)"},{"path":"https://jlmelville.github.io/uwot/articles/metric-learning.html","id":"accuracy-results","dir":"Articles","previous_headings":"Metric Learning","what":"Accuracy Results","title":"Metric Learning with UMAP","text":"quantify improvement, can look accuracy predicting test set labels using embedded coordinates k-nearest neighbor classifier. variety ways can imagine using information model, two obvious ones use label nearest neighbor, (1NN) take vote using n_neighbors (case, 15) nearest neighbors (15NN). standard UMAP, 1NN accuracy 71%, 15NN accuracy 77%. Using supervised UMAP, accuracies improve 83% 84%, respectively. quantitatively, supervised UMAP big help correctly classifying test data. put numbers perspective, can carry similar calculations using input data directly. , 1NN accuracy 85% 15NN accuracy 84%. Possibly, lack improvement going 1 15 neighbors indicates different value n_neighbors parameter improve embedding, haven’t pursued . rate, ’s clear Fashion MNIST images embed well two dimensions, although supervised UMAP gets impressively close matching high dimensional results. Maybe supervised UMAP can even better suitable choice target_weight n_components top fiddling n_neighbors. Fashion MNIST website contains page shows accuracy using 129 scikit-learn methods, 15NN supervised UMAP accuracy puts us top 60, isn’t bad, considering hyperparameter search look 1NN 15NN. However, although highest accuracy reported page 89.7%, deep learning results achieve 90-97%.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/metric-learning.html","id":"supervised-umap-numerical-y","dir":"Articles","previous_headings":"","what":"Supervised UMAP: Numerical Y","title":"Metric Learning with UMAP","text":"’s example using supervised UMAP numerical target vector. shall use diamonds dataset comes ggplot2 package, similar size MNIST. 10 variables associated diamond: five numeric values related geometry diamonds (table, x, y, z depth), three factors measure quality diamond (cut, color clarity), price dollars. price seems like perfect candidate sort thing ’d want target vector, leaving nine variables used dimensionality reduction. uwot’s implementation UMAP uses numeric columns can find calculations, avoid including price non-supervised part UMAP, let’s create new data frame, initially geometric data: depth column related x, y z (albeit non-linearly) ’m going include . Additionally, factors cut, color clarity ordinal variables, .e. categories can ordered, can convert numeric scale include well: now dataset 53,940 rows 8 columns. 360 duplicates, doesn’t seem affect results particularly. Now, ’m saying trickiest dataset extract meaning . First, let’s look standard unsupervised results. starters, ’s plot first two principal components, using irlba package: different columns different units meaning, set scale. = TRUE equalize variances. color scheme “Spectral” palette ColorBrewer: red indicates low price blue high price. Despite majority dataset clumped together plot due outliers can’t really see, progression prices low high already pretty well captured two components. Anyway, let’s see UMAP . Like PCA, columns scaled equal variance (scale = TRUE): bad. high price diamonds clumped together little clusters middle plot. occasion, prefer layout ’s initialized PCA results, though: maintains global structure PCA result. Rather separately create PCA, can also use init = \"pca\" get results (uwot uses irlba internally , ’s loss speed). Onto supervised result. Results particularly affected choice initialization, simplicity ’ll just use standard spectral initialization: expected, embedding now even well-organized along price diamonds. visible gap lowest price diamonds (right) rest embedding. increase n_epochs parameter allow optimization proceed, gap increases substantially, making plot harder read. Adjusting n_epochs parameter, along target_n_neighbors target_weight parameters may required strike right balance. time writing, ’m aware many examples supervised UMAP numeric vector (fact none except thing just wrote) provide lot sage wisdom matter.","code":"library(ggplot2) ?diamonds dia <- diamonds[, c(\"carat\", \"x\", \"y\", \"z\", \"table\")] dia$cut <- as.numeric(diamonds$cut) dia$color <- as.numeric(diamonds$color) dia$clarity <- as.numeric(diamonds$clarity) dia_pca <- irlba::prcomp_irlba(dia, n = 2, scale. = TRUE) vizier::embed_plot(dia_pca$x, diamonds$price, title = \"Diamonds PCA\", color_scheme = \"RColorBrewer::Spectral\", alpha_scale = 0.1, cex = 0.5, pc_axes = TRUE) dia_umap <- umap(dia, scale = TRUE, verbose = TRUE) vizier::embed_plot(dia_umap, diamonds$price, title = \"Diamonds UMAP\", color_scheme = \"RColorBrewer::Spectral\", alpha_scale = 0.1, cex = 0.5, pc_axes = TRUE) dia_umap_from_pca <- umap(dia, scale = TRUE, verbose = TRUE, init = dia_pca$x) vizier::embed_plot(dia_umap_from_pca, diamonds$price, title = \"Diamonds UMAP (PCA init)\", color_scheme = \"RColorBrewer::Spectral\", alpha_scale = 0.1, cex = 0.5, pc_axes = TRUE) dia_sumap <- umap(dia, scale = TRUE, verbose = TRUE, y = diamonds$price) vizier::embed_plot(dia_sumap, diamonds$price, title = \"Diamonds Supervised UMAP\", color_scheme = \"RColorBrewer::Spectral\", alpha_scale = 0.1, cex = 0.5, pc_axes = TRUE)"},{"path":"https://jlmelville.github.io/uwot/articles/mixed-data-types.html","id":"indexing","dir":"Articles","previous_headings":"","what":"Indexing","title":"Mixed Data Types","text":"iris example shows, using column names can verbose. Integer indexing supported, equivalent using integer indexing columns iris : internally, uwot strips non-numeric columns data, use Z-scaling (.e. specify scale = \"Z\"), zero variance columns also removed. likely change index columns. really want use numeric column indexes, strongly advise using scale argument re-arranging data frame necessary non-numeric columns come numeric columns.","code":"metric = list(\"euclidean\" = 3:4, \"euclidean\" = 1:2)"},{"path":"https://jlmelville.github.io/uwot/articles/mixed-data-types.html","id":"categorical-columns","dir":"Articles","previous_headings":"","what":"Categorical columns","title":"Mixed Data Types","text":"supervised UMAP allows factor column used. may now also specify factor columns X data. Use special metric name \"categorical\". example, use Species factor standard UMAP iris along usual four numeric columns, use: Factor columns treated differently numeric columns: always treated separately, one column time. two factor columns, cat1, cat2, like included UMAP, write: convenience, can also write: doesn’t combine cat1 cat2 one block, just saves typing. way categorical data intersected simplicial set, X metric specifies categorical entries. must specify least one standard Annoy metrics numeric data. iris, following error: Specifying numeric columns required: Factor columns explicitly included metric still removed usual. Categorical data appear model returned ret_model = TRUE affect project data used umap_transform. can still use UMAP model project new data, factor columns new data ignored (effectively working like supervised UMAP).","code":"metric = list(\"euclidean\" = 1:4, \"categorical\" = \"Species\") metric = list(\"categorical\" = \"cat1\", \"categorical\" = \"cat2\", ...) metric = list(\"categorical\" = c(\"cat1\", \"cat2\"), ...) # wrong and bad metric = list(\"categorical\" = \"Species\") # OK metric = list(\"categorical\" = \"Species\", \"euclidean\" = 1:4)"},{"path":"https://jlmelville.github.io/uwot/articles/mixed-data-types.html","id":"overriding-global-options","dir":"Articles","previous_headings":"","what":"Overriding global options","title":"Mixed Data Types","text":"global parameters can overridden specific data block providing list value metric, containing vector columns unnamed element, -riding keyword arguments. example: case, first euclidean block reduced 40 dimensions PCA centering applied. second euclidean block PCA applied . manhattan block PCA applied , centering carried . Currently, pca pca_center supported overriding method, feature exists allow case mixed real-valued binary data, want carry PCA . ’s typical carry centering real-value data PCA, binary data.","code":"umap(     X,     pca = 40,     pca_center = TRUE,     metric = list(       euclidean = 1:200,       euclidean = list(201:300, pca = NULL),       manhattan = list(300:500, pca_center = FALSE)     )   )"},{"path":"https://jlmelville.github.io/uwot/articles/mixed-data-types.html","id":"y-data","dir":"Articles","previous_headings":"","what":"y data","title":"Mixed Data Types","text":"handling y data extended allow data frames, target_metric works like metric: multiple numeric blocks different metrics can specified, categorical data can specified categorical. However, unlike X, default behavior y include factor columns. numeric data found treated one block, multiple numeric columns want treated separately, specify column separately: suspect vast majority y data one column, default behavior fine time.","code":"target_metric = list(\"euclidean\" = 1, \"euclidean\" = 2, ...)"},{"path":"https://jlmelville.github.io/uwot/articles/mutual-nearest-neighbors.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"UMAP with Mutual Nearest Neighbors","text":"Traditionally, dimensionality reduction methods make use k-nearest neighbors graph exceptions: Trimap PaCMAP use extended version k-nearest neighbors, based “generalized” distances used self-tuning spectral clustering, PaCMAP goes also sample kth closest neighbors locally sampled batch. LocalMAP also adjusts neighbor graph dynamically embedding sampling distances embedded points. purposes producing better-separated clusters, Dalmia Sia (see also article UMAP docs) consider mutual nearest neighbors (augmented extra neighbors). way PaCMAP uses neighbors beyond architecture uwot handle easilt, mutual nearest neighbors graph something uwot can deal . detailed article uwot’s nearest neighbors format, can provide input sparse distance matrix represent nearest neighbors, giving substantial flexibility. However, ’s always easy take advantage . show code can generate suitably-massaged mutual nearest neighbors graph see effect UMAP.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/mutual-nearest-neighbors.html","id":"the-problem-with-symmetrized-k-nearest-neighbors","dir":"Articles","previous_headings":"","what":"The Problem With Symmetrized k-Nearest Neighbors","title":"UMAP with Mutual Nearest Neighbors","text":"Although kNN graph item k neighbors, workable dimensionality reduction working kNN graph input symmetrize graph, otherwise optimization difficult. outcome symmetrizing knn graph jj neighbor ii kNN graph, ii also guaranteed neighbor jj symmetrized graph vice versa. one item tends show lot kNN lists multiple items, symmetrization means can NN edges involving popular item. called hubs make search graph difficult (item likely show queries) given kk limited number edges distribute dataset, items getting fair share. likely lead outliers points get embedded far away main body data particularly strong bonds rest data.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/mutual-nearest-neighbors.html","id":"mutual-k-nearest-neighbors","dir":"Articles","previous_headings":"","what":"Mutual k-Nearest Neighbors","title":"UMAP with Mutual Nearest Neighbors","text":"can think mutual k-nearest neighbors graph (MNN graph) symmetrized kNN graph edges appear item ii neighbor jjand jj neighbor ii: symmetrized kNN graph described requirement ii neighbor jjor jj neighbor ii. means item can kk mutual neighbors. Hub problem solved.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/mutual-nearest-neighbors.html","id":"the-problem-with-mutual-k-nearest-neighbors","dir":"Articles","previous_headings":"","what":"The Problem with Mutual k-Nearest Neighbors","title":"UMAP with Mutual Nearest Neighbors","text":"downside MNN graph even well-behaved kNN graphs, MNN contain several items edges item: kk nearest neighbors, none neighbors reciprocate. UMAP requires local connectivity, handle use case Billy -mates items. result, Dalmia Sia paper go extra lengths augment MNN graph extra nodes can see repo. involves minimum spanning trees Djikstra’s algorithm add new nodes (distances via path length). One alternative consider take fully disconnected nodes re-add first nearest neighbor. cite paper de Souza, Rezende Batista , ’s extra detail paper, authors just note avoid disconnected nodes. said “adjacent neighbor” approach MNN augmentation work well, leading lot disconnect components small clusters points scattered embedding. surprising.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/mutual-nearest-neighbors.html","id":"adding-back-extra-neighbors-the-balanced-mutual-nearest-neighbors-graph","dir":"Articles","previous_headings":"","what":"Adding Back Extra Neighbors: The Balanced Mutual Nearest Neighbors Graph","title":"UMAP with Mutual Nearest Neighbors","text":"don’t currently fancy implementing path neighbor approach, propose following strategy: just add back one neighbor disconnected nodes. fact, probably well ensure every item dataset least mm neighbors m<km \\lt k, just items completely disconnected, can already see items one neighbor don’t good job. Obviously adding back neighbors recreate original symmetrized kNN graph, want keep mm small, bigger one. name concept going “Balanced MNN graph” “balanced” meant suggest trying ensure node left isolated MNN graph. specific numbers refer (kk, mm)-BMNN graph. welcome hearing existing literature different (probably better) name.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/mutual-nearest-neighbors.html","id":"creating-the-bmnn","dir":"Articles","previous_headings":"","what":"Creating the BMNN","title":"UMAP with Mutual Nearest Neighbors","text":"basic formula creating BMNN graph follows: Create kNN graph. Form equivalent MNN graph. nodes fewer mm neighbors? , stop. Otherwise: Add back mmth-nearest neighbor kNN graph nodes. Go step 3. loop steps 3-5 must terminate mm steps: worst, items 0 neighbors, add mm-th nearest neighbors. items 1-(mm-1) neighbors, don’t know mm-th nearest neighbor already neighbor. isn’t one step closer termination. , adding edge nothing. can’t add mm edges node none nodes get many edges added. Creating MNN sparse kNN representation easy, assuming distances symmetrical: Balancing MNN bit involved, especially want efficient anything disastrous like accidentally attempt turn sparse matrix dense. must go 5 lines code 100. code delight delectation. may officially export uwot day, remains experiment now copy paste. Input balance_mnn : nn_graph – dense representation k-nearest neighbors, .e. list two dense matrices, idx dist dimensions (n, k). See article nearest neighbor format . uwot exports format, rnndescent. k – optionally specify size l nearest neighbors use. omit , neighbors nn_graph used. generating nearest neighbors time-consuming, may wish create large neighbor graph (e.g. set k = 50 k = 150) use smaller subset need arises (e.g. 15 typical UMAP). nng helper function can . Pro-tip: use rnndescent::brute_force_knn generate exact nearest neighbors k = 150 save . now rather decrepit 8th gen Intel laptop still enough juice crank datasets 100,000 items 10,000 features 6 threads “reasonable” amount time, “reasonable” means anything hours, “going holiday two weeks”. m – minimum number neighbors ensure item . needs least two. One slightly weird thing specifying m actually get back one less edge value ask . dense format every item’s nearest neighbor distance zero. disappears convert sparse format. result dense k-nearest neighbor graph contains k−1k-1 different neighbors item. self-neighbor thing counting one neighbors always UMAP done , won’t changing habit lifetime. just remember reproduce “adjacent neighbor” method must specify m = 2. function won’t let go lower m = 2 anyway.","code":"sp_mutual <- function(sp) {   sp_t <- Matrix::t(sp)   res <- sqrt(sp * sp_t)   res } # create an m-balanced k-mutual nearest neighbors graph from the dense k-nearest # neighbors graph `nn-graph`. m must be less than or equal to k. If `k` is not # specified then all of the neighbors are used. balance_mnn <- function(nn_graph, m, k = NULL) {   if (m < 2) {     stop(\"m must be at least 2\")   }    if (!is.null(k)) {     nn_graph <- nng(nn_graph, k)   }   k <- ncol(nn_graph$idx)   if (m > k) {     stop(\"m must be less than or equal to k\")   }    # create the mutual neighbors as a sparse matrix   # each item's neighbors distances are stored by column   bmnn <- k_to_m(nn_graph)   bmnn_adj <- sp_to_adj(bmnn)   bmnn_edges <- colSums(bmnn_adj)   too_few_edges_mask <- bmnn_edges < (m - 1)    # add edges back from the knn graph to \"fill up\" any items with too few edges   # do it one column at a time to avoid over-filling any one item   for (l in seq(from = 2, to = m)) {     if (sum(too_few_edges_mask) == 0) {       break     } else {       message(\"Iter \", l, \" cols to process: \", sum(too_few_edges_mask))     }      knn_idxl <- nn_graph$idx[, l]     masked_knn_distl <- nn_graph$dist[, l] * too_few_edges_mask     masked_knn <- vec_to_sp(knn_idxl, masked_knn_distl)     knn_adj <- sp_to_adj(masked_knn)      mk_adj <- bmnn_adj + knn_adj     bmnn <- bmnn + masked_knn     bmnn@x <- bmnn@x / mk_adj@x      bmnn_adj <- sp_to_adj(bmnn)     bmnn_edges <- colSums(bmnn_adj)     too_few_edges_mask <- bmnn_edges < (m - 1)   }    bmnn }  ## Helper functions  # truncate a dense neighbor graph to the first k neighbors nng <- function(graph, k) {     list(idx = graph$idx[, 1:k], dist = graph$dist[, 1:k]) }  # convert a dense knn graph to a sparse matrix format # NB neighbors are stored by column nng_to_sp <- function(graph, nbrs = NULL) {   if (!is.null(nbrs)) {     graph <- nng(graph, nbrs)   }    idx <- graph$idx   dist <- graph$dist   n_nbrs <- ncol(idx)   n_row <- nrow(idx)   n_ref <- nrow(idx)   i <- as.vector(idx)   j <- rep(1:n_row, times = n_nbrs)    x <- as.vector(dist)   keep_indices <- !is.na(x)   i <- i[keep_indices]   j <- j[keep_indices]   x <- x[keep_indices]   res <- Matrix::sparseMatrix(i = i, j = j, x = x, dims = c(n_row,        n_ref), repr = repr)   Matrix::drop0(res) }  # convert a sparse knn matrix to the mutual nearest neighbors graph sp_mutual <- function(sp) {   sp_t <- Matrix::t(sp)   res <- sqrt(sp * sp_t)   res }  # convert a dense kNN graph to a mutual nearest neighbors graph k_to_m <- function(graph, nbrs = NULL) {   sp <- nng_to_sp(graph, nbrs)   res <- sp_mutual(sp)   res }  # convert sparse matrix to an adjacency matrix (1 if there's an edge, 0 otherwise) sp_to_adj <- function(sp) {   spi <- sp   spi@x <- rep(1, length(spi@x))   spi }  # convert the vector of values x to a sparse matrix with where the row is given # by the equivalent value in the vector i, and the column is the index of the # value in the vector x. The resulting matrix is a square matrix with the same  # number of rows as the length of x. vec_to_sp <- function(i, x) {   n_row <- length(i)   j <- 1:n_row    keep_indices <- !is.na(x)   i <- i[keep_indices]   j <- j[keep_indices]   x <- x[keep_indices]   res <- Matrix::sparseMatrix(     i = i,     j = j,     x = x,     dims = c(n_row, n_row),     repr = \"C\"   )   Matrix::drop0(res) }"},{"path":"https://jlmelville.github.io/uwot/articles/mutual-nearest-neighbors.html","id":"disconnections-and-the-question-of-initialization","dir":"Articles","previous_headings":"","what":"Disconnections and the Question of Initialization","title":"UMAP with Mutual Nearest Neighbors","text":"Dalmia Sia note using MNN graph leads many disconnected components, can cause issues default spectral initialization UMAP. approach creates connected graph get round problem. alternative pursue results always initialize first two components PCA. Even case kNN graph, can still get disconnected components given value n_neighbors, uwot fall back PCA case anyway.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/mutual-nearest-neighbors.html","id":"working-with-the-bmnn","dir":"Articles","previous_headings":"","what":"Working with the BMNN","title":"UMAP with Mutual Nearest Neighbors","text":"Putting together, recommended workflow : work presented Dalmia Sia, multiple choices k tried (10-50) changes min_dist. stick k = 15 results shown .","code":"library(rnndescent) librart(uwot) # get the approximate knn: use as many threads as you want data_knn_15 <- rnndescent::rnnd_knn(data, k = 15, n_threads = 6) # or if you have time on your hands # data_knn_15 <- rnndescent::brute_force_knn(data, k = 15, n_threads = 6)  pca_init <- umap2(data, nn_method = data_knn_15, init = \"pca\", n_epochs = 0)  # Normal UMAP data_umap <- umap2(data, nn_method = data_knn_15, init = pca_init)  # Create the (15, 5)-balanced MNN data_bmnn_15_5 <- balance_mnn(data_knn_15, k = 15, m = 5)  # UMAP with BMNN data_bmnn_umap <- umap2(data, nn_method = data_bmnn_15_5, init = pca_init)"},{"path":"https://jlmelville.github.io/uwot/articles/mutual-nearest-neighbors.html","id":"mnist-example","dir":"Articles","previous_headings":"","what":"MNIST Example","title":"UMAP with Mutual Nearest Neighbors","text":"BMNN graph produce results differ kNN graph? Let’s compare UMAP embeddings MNIST digits Fashion MNIST datasets using kNN graph BMNN graph. left right original UMAP embedding, embedding using (15, 2)-BMNN graph give similar results MNN+adjacent neighbors approach used Dalmia Sia, embedding using (15, 5)-BMNN graph (hope) give “connected” embedding. can click images see (slightly) larger version. Although ’s easy see unless click image, (15,2)-BMNN results show sprinkling outlier points throughout embedding. confirms observations Dalmia Sia doesn’t seem like promising approach. won’t bother showing results (15,2)-BMNN graphs going forward. (15,5)-BMNN hand seems quite well behaved. Now, say “put doesn’t look different normal UMAP results” agree . perhaps slightly separation clusters finer detail? Possibly ’s entirely within expected variation given UMAP run. Fortunately datasets see difference.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/mutual-nearest-neighbors.html","id":"coil-20-and-coil-100","dir":"Articles","previous_headings":"","what":"COIL-20 and COIL-100","title":"UMAP with Mutual Nearest Neighbors","text":"Embedding COIL-20. COIL-100 show series loops, typically get bit tangled together mangled. ever datasets might benefit focusing mutual neighbors, ’s two. longer feel like lying say can see difference two embeddings. BMNN graph definitely seems untangled loops bit .","code":""},{"path":"https://jlmelville.github.io/uwot/articles/mutual-nearest-neighbors.html","id":"disappointing-results","dir":"Articles","previous_headings":"","what":"Disappointing Results","title":"UMAP with Mutual Nearest Neighbors","text":"datasets UMAP struggles produce useful results . include: image dataset CIFAR-10 just wouldn’t expect looking Euclidean distance raw pixel space produce anything value (doesn’t). text dataset 20Newsgroups attempted process UMAP past: version cleaned, TF-IDF applied L2 normalized. Clusters appear aren’t well-separated. appears Dalmia Sia paper different processing (also PaCMAP paper different processing ) without looking terribly attractive. scRNA-seq data macosko2015, raw form around 3000 columns. shows extreme hubness: MNN graph (k = 15) results 89% dataset entirely disconnected! corresponding problem quite careful large m BMNN: vast majority edges kNN graph pointing towards hubs, even small number additions kNN edges back can lead hub BMNN. symmetrized kNN graph hub 10,000 edges associated (around 20% dataset). BMNN graph m = 5 reduces , hub still 4,000 edges. macosko2015 visualizes nicely PCA hubness greatly reduced (agnostic representation faithfully represents biological reality). also include dataset reducing 100 dimensions PCA. Small NORB image dataset objects, challenging datasets listed . fact, slightly COIL-ish already shows loops structure UMAP. using MNN graph improve matters COIL-20 COIL-100? Ok, turns BMNN graph doesn’t really help cases. norb particularly disappointing noticeably makes things worse. macosko2015, pretty much nothing happens, except big blue round cluster got slight mottling seems like kind fine(ish) structure forming, might translate clusters. couldn’t tell biologically relevant though. tested whether hubness present kNN creeps back BMNN determinant embedding creating BMNN edges added back order shortest distance, smallest k-occurrence, .e. items appear least often kNN graph. reduces maximum edge count BMNN around 200 (nearly 20-fold drop), embedding looks similar vanilla UMAP result: include image evidence long-held belief hubs kNN graph problem UMAP, good visualization stare next time start going road. macosko2015pca100 shows little change also. 20NG cifar10 effect make main blob data compressed middle plot, due extra small clusters now located edges. attractive, indicating anomalous clustering small subsets, might worth investigating . example, duplicates close duplicates. haven’t actually done investigation though can’t say ’s case.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/mutual-nearest-neighbors.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"UMAP with Mutual Nearest Neighbors","text":"Using BMNN approximate MNN mixed bag. cases can help, others either little, can make things worse – although ’s always chance diagnostic unusual structure data. Also worth noting BMNN graph much sparser symmetrized kNN graph UMAP uses default. examples (15, 5)-BMNN graph around 1/3-1/2 number edges kNN graph. can give moderate speedup UMAP embedding step: around 25-50%. BMNN results certainly visibly different seen Dalmia Sia paper. ? need tweak k m specifically dataset although makes BMNN approach unappealing. also spectral initialization using BMNN approach give different results, order , need strategy form fully connected graph Laplacian. general useful thing generate neighbor graph, seems like something suited graph nearest neighbor package (like rnndescent) uwot . Certainly BMNN panacea datasets don’t show much structure, pretty forlorn hope. advice UMAP result show kind structure, ’s worth trying BMNN ’s difficult time-consuming generate.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/nearest-neighbors-format.html","id":"nearest-neighbor-graph-format","dir":"Articles","previous_headings":"","what":"Nearest Neighbor Graph Format","title":"Nearest Neighbor Format","text":"format expected nn_method list containing following two entries: idx: matrix dimension n_vertices x n_neighbors, row contains indexes (starting 1) nearest neighbors item (vertex) dataset. item always nearest neighbor , first element row always . isn’t either using really weird non-metric distance approximate nearest neighbor method returning way approximate results. either case, expect bad results. dist: matrix dimension n_vertices x n_neighbors, row contains distances nearest neighbors item (vertex) dataset, item always nearest neighbor , first element row always 0.0.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/nearest-neighbors-format.html","id":"sparse-distance-matrix-format","dir":"Articles","previous_headings":"","what":"Sparse Distance Matrix Format","title":"Nearest Neighbor Format","text":"Alternatively, can pass sparse distance matrix : format dgCMatrix (typical sparse matrix format). non-zero entries distances. put another way: neighbor distances arranged non-zero entries ith column matrix contains distances observation nearest neighbors. advantage using sparse distance matrix: restricted fixed value n_neighbors observation. column can contain different number non-zero distances. See paper Dalmia Sia might want . graph edge weight calculation adjusted account different number neighbors observation. must least one neighbor observation. Explicit zero distances removed matrix. contrast use nearest neighbor list matrix format typically zero distance observation found part nearest neighbor search routine. sparse distance matrix approach account zero self-distance implicit. keep explicit zero distances observations set small non-zero value, e.g. 1e-10. slight disadvantage using distance matrix distances need sorted. Sparse distance matrix input currently supported lvish method. use pre-computed nearest neighbor data, aware : can’t use pre-computed nearest neighbor data also use metric. can explicitly set X NULL, long don’t try use initialization method makes use X (init = \"pca\" init = \"spca\"). can transform new data setting ret_model = TRUE. must provide umap_transform distances new data original data via nn_method parameter. ’s example using pre-computed nearest neighbor data using even-numbered observations iris build initial model transforming odd-numbered observations. relies internal uwot functions promise stable API (.e. may example may broken read ), gives general idea:","code":"iris_even <- iris[seq(2, nrow(iris), 2), ] iris_odd <- iris[seq(1, nrow(iris), 2), ]  iris_even_nn <- uwot:::annoy_nn(   X = uwot:::x2m(iris_even),   k = 15,   metric = \"euclidean\",   ret_index = TRUE )  iris_odd_nn <- annoy_search(   X = uwot:::x2m(iris_odd),   k = 15,   ann = iris_even_nn$index )  # Delete the Annoy index, force the transform method to use the nn distances  # directly iris_even_nn$index <- NULL  iris_even_umap <-   umap(     X = NULL,     nn_method = iris_even_nn,     ret_model = TRUE   )  iris_odd_transform <-   umap_transform(X = NULL, iris_even_umap, nn_method = iris_odd_nn)"},{"path":"https://jlmelville.github.io/uwot/articles/nearest-neighbors-format.html","id":"exporting-nearest-neighbor-data-from-uwot","dir":"Articles","previous_headings":"","what":"Exporting nearest neighbor data from uwot","title":"Nearest Neighbor Format","text":"set ret_nn = TRUE, return value umap list, nn item contains nearest neighbor data format can used nn_method. handy going running UMAP multiple times data n_neighbors scale settings, nearest neighbor calculation can time-consuming part calculation. Normally contents nn list, value nearest neighbor data. name type metric generated data. example, ’s first items iris 5-NN data look like: reason specify ret_nn supplying precomputed nearest neighbor data nn_method, returned data identical passed , list item names precomputed.","code":"lapply(umap(iris, ret_nn = TRUE, n_neighbors = 5)$nn$euclidean, head)  $`idx`      [,1] [,2] [,3] [,4] [,5] [1,]    1   18    5   40   29 [2,]    2   35   46   13   10 [3,]    3   48    4    7   13 [4,]    4   48   30   31    3 [5,]    5   38    1   18   41 [6,]    6   19   11   49   45  $dist      [,1]      [,2]      [,3]      [,4]      [,5] [1,]    0 0.1000000 0.1414214 0.1414214 0.1414214 [2,]    0 0.1414214 0.1414214 0.1414214 0.1732051 [3,]    0 0.1414214 0.2449490 0.2645751 0.2645751 [4,]    0 0.1414214 0.1732051 0.2236068 0.2449490 [5,]    0 0.1414214 0.1414214 0.1732051 0.1732051 [6,]    0 0.3316625 0.3464102 0.3605551 0.3741657"},{"path":"https://jlmelville.github.io/uwot/articles/nearest-neighbors-format.html","id":"multiple-neighbor-data","dir":"Articles","previous_headings":"","what":"Multiple neighbor data","title":"Nearest Neighbor Format","text":"discussed Mixed Data Types article, can apply multiple distance metrics different parts matrix data frame input data. , ret_nn return neighbor data. list nn now contain many items metrics, order specified. instance, metric argument : nn list contain two list entries. first called euclidean second cosine. access multiple distance metrics, may also provide multiple precomputed neighbor data nn_method format: list lists, sublist format described (.e. two matrices, idx dist). names list items ignored, don’t need set . Roughly, something like : different neighbor data must number neighbors, .e. number columns matrices must .","code":"metric = list(\"euclidean\" = c(\"Petal.Width\", \"Petal.Length\"),               \"cosine\" = c(\"Sepal.Width\", \"Sepal.Length\")) nn_metric1 <- list(idx = matrix(...), dist = matrix(...)) nn_metric2 <- list(idx = matrix(...), dist = matrix(...)) umap_res <- umap(nn_method = list(nn_metric1, nn_metric2), ...)"},{"path":"https://jlmelville.github.io/uwot/articles/nearest-neighbors-format.html","id":"numeric-y","dir":"Articles","previous_headings":"","what":"Numeric y","title":"Nearest Neighbor Format","text":"using supervised UMAP numeric y, can also pass nearest neighbor data y, using format . case nearest neighbors respect data y. Note pass categorical y nearest neighbor data. processing data goes different code path doesn’t directly calculate nearest neighbors: y factor, small number levels, number neighbors item can vastly larger n_neighbors. Nearest neighbor data y returned umap re-use.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/pacmap-comparison.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"PaCMAP Comparison","text":"PaCMAP (paper, repo) highly-regarded dimensionality method, firmly LargeVis/UMAP family. ’s main point differentiation related methods balances local global structure without needing tune hyperparameters. PaCMAP authors’ blog post explicitly states: PaCMAP parameters designed tuned. Bold words! Also PaCMAP pretty fast. Alas native R package PaCMAP (far know). can course call via reticulate, PaCMAP repo notebook demonstrating . ’s bit unsatisfactory. spent lot time studying Python code can see notes method want details. upshot feel qualified say actually quite hard integrate PaCMAP uwot.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/pacmap-comparison.html","id":"turning-umap-into-pacmap","dir":"Articles","previous_headings":"","what":"Turning UMAP into PaCMAP?","title":"PaCMAP Comparison","text":"can bend UMAP make behave like PaCMAP? high-level overview important ways PaCMAP differs UMAP: Nearest neighbors chosen using method originating self-tuning spectral clustering also used Trimap. rescales nearest neighbor distances based local densities. doesn’t seem make massive difference results testing won’t worry implementing . special class “mid-near” items, points away nearest neighbors closer randomly-sampled pairs. language contrastive learning, can think “semi-positive” examples. Use mid-near pairs requires introducing new loss term gradient type, main problem integrating PaCMAP uwot. Plus different weights applied component loss changes optimization process. require even reworking uwot. Fortunately, seems like benefit (using mid-near items general) initialize embedding randomly. Let’s just . negative examples item sampled re-used epochs, unlike UMAP always selecting different vertices. ’re also going probably just speed optimization rather necessary working method whole. default, input data 100 columns, Truncated SVD applied reduce dimensionality 100. ’m really fan default setting, can easily implement setting pca = 100. seem important part PaCMAP’s speed improvements . Initialization PCA, also easily implemented uwot init = \"pca\". PaCMAP shrinks multiplying coordinates 0.01. supported setting init = \"pacpca\" really want , don’t think ’s necessary (actually good idea). loss function gradient PaCMAP rather different UMAP. general gentle UMAP’s cause less tearing twisting manifold structure embedded coordinates. real-world datasets looked , isn’t huge effect gradient. can see reasonable changes UMAP PaCMAP responses “yeah, don’t fancy changing ”. put cards table, think major source differences UMAP PaCMAP : Using PCA initialization (important difference). Using PCA reduce high-dimensional data 100 dimensions.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/pacmap-comparison.html","id":"methods","dir":"Articles","previous_headings":"","what":"Methods","title":"PaCMAP Comparison","text":"make simple used: PaCMAP default settings. used Python 3.12 PaCMAP 0.7.3. patch releases since , meaningful effect results shown . UMAP, used default umap2 parameters. HNSW used nearest neighbor method. didn’t make effort match number neighbors number epochs . think pretty comparable default settings. PCA performed using irlba. make results easier compare, output coordinates rigid-body rotated align PCA results using Kabsch algorithm (coordinates also additionally flipped along one dimension, result lowest error chosen).","code":""},{"path":"https://jlmelville.github.io/uwot/articles/pacmap-comparison.html","id":"datasets","dir":"Articles","previous_headings":"","what":"Datasets","title":"PaCMAP Comparison","text":"tried use variety datasets, including many used PaCMAP authors paper blog-post. Many Python code PaCMAP repo useful. See examples article details datasets. One extra dataset also look curve2d, simple 2D curve. used example paper even start original coordinates scaled constant, dimensionality reduction methods , rather nothing, still distort overall shape. Python code generate curve https://github.com/YingfanWang/PaCMAP/blob/master/experiments/run_experiments.py.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/pacmap-comparison.html","id":"results","dir":"Articles","previous_headings":"","what":"Results","title":"PaCMAP Comparison","text":"’s table embeddings . ased feelings PCA key lot PaCMAP’s differences UMAP, three different settings UMAP. left right settings used : UMAP default parameters (UMAP). UMAP initialized PCA (UMAP PCA), .e. init = \"pca\". UMAP input data reduced 100 dimensions via PCA, 100 columns (UMAP-PCA100) also initialized PCA, .e.  init = \"pca\", pca = 100. Finally right embeddings running PaCMAP. made special effort account variability runs either UMAP PaCMAP. Based run many, many times, confident results representative methods’ performance. javascript enabled, can click image embiggen . Headline results point view going left right, UMAP results get like PaCMAP. many cases default UMAP results don’t seem different PaCMAP results. lot datasets, differences, already discussed length PaCMAP paper, act enormous ego provide commentary, lucky things. UMAP certainly makescurve2d lot twistier PaCMAP. mammoth looks different. ’s brutally spatchcocked UMAP, ’s ripped shreds. However, ’s undeniable human eye definitely perceives side-view PaCMAP provides “natural”. Also scurvehole demonstrates UMAP tendency rip low-dimensional manifolds PaCMAP. said dataset often get ripped PaCMAP (ends – less often – area around hold can ripped), although degree ripping pronounced UMAP. Let’s also look isoswiss. UMAP better PaCMAP good job nearly unfolding . ’s bit hard see PaCMAP done , ’s undone PCA initialization keeps data rolled . unrolling, ’s tough ask method works iteratively updating pairs points large scale rearrangement unfolding, instead top part embedding ripped. can see thing happening lesser extent scurvehole two ends bit twisted. issue PCA initialization local enough. end world? . ’s just bit shame sacrifice unfolding manifold examples given “M” PaCMAP also stands “manifold”. low-dimensional datasets. love see methods adapt intrinsic dimensionality data working , PaCMAP’s gentler approach definitely superior . sure representative kind data people actually working . whether lots nice low-dimensional manifolds waiting unfolded open question. , PCA good enough initialization datasets. ’ll also briefly note something bit odd spheres. Neither UMAP PaCMAP good job recognizing ten clusters embedded larger sphere. Perhaps surprising due local nature methods. UMAP gives result ten clusters, representing ten spheres, plus points cluster local , PaCMAP eleven clusters. large blob side seems just background cluster. Interesting, maybe might like result better due detecting sphere, even “containment” relationship hasn’t preserved. However, seen artifactual behavior Annoy sometimes lots degenerate distances parameters haven’t set correctly. possibility artifact nearest neighbor method issues. something rule haven’t done . coil100 interesting case think “multiply PCA coordinates 0.01” approach PaCMAP may give slightly inconsistent initialization occasion. UMAP gives circular-looking result, whereas PaCMAP elongated, circular structures noticeably right body embedding. case PaCMAP result influenced PCA initialization: overall extent initialization quite large value: side length bounding box initialized coordinates around ~200 units vs ~30 PCA initialization mnist. leads large interpoint distances enemy SNE-like methods: gradients smaller usual hence initial structure kept. seen good thing, might want keep PCA-inspired layout focus local optimization. However, ’s something happens consistently PaCMAP, really depends initial scaling input dataset. better scaling input coordinates help , UMAP approach scaling coordinates cube length 10. datasets, one stands different UMAP PaCMAP eyes macosko2015. PaCMAP result looks much nicer . like dataset results affected PCA. Now let’s see using PCA initialization UMAP. obvious place look mammoth see results now much closer PaCMAP’s. true curve2d isoswiss. scurvehole also overall layout much closer PaCMAP, degree twisting ripping definitely worse UMAP. Elsewhere, things aren’t changed much. admit bit annoyed result coil20 one ring left now stuck inside larger ring isn’t case spectral initialization PaCMAP get . UMAP + PCA result coil100 also circular shape spectral initialization rather elongated shape PaCMAP result argue evidence control initial scaling PaCMAP helpful. also say terms overall layout, whether mnist cluster layout UMAP UMAP+PCA like PaCMAP result bit toss-. well methods work mnist last thing anyone worry . really leaves macosko2015 whch still bears little resemblance PaCMAP result. third column use PCA reduce initial dimensionality data 100 use first two principal components initial coordinates. give difference results compared UMAP+PCA results input data 100 columns, case datasets top table. much happens except macosko2015, now looks much like PaCMAP results. terribly surprising know dataset highly affected running PCA first.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/pacmap-comparison.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"PaCMAP Comparison","text":"want PaCMAP-like experience uwot using pca = 100, init = \"pca\" good way go. Even lower-dimensional synthetic data, PCA give results closer PaCMAP, although may look bit raggedy.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/pacmap-comparison.html","id":"what-about-random-initialization","dir":"Articles","previous_headings":"Conclusions","what":"What about random initialization?","title":"PaCMAP Comparison","text":"One PaCMAP’s selling points get consistent results even don’t use PCA initialization. grant certainly point PaCMAP’s favor. hand, ’s limited number circumstances can’t just use PCA initialization. PaCMAP paper doesn’t go detail circumstances , just give ideas. main one might data doesn’t really lend PCA. Probably binary data obvious case , although PaCMAP supports Hamming distance binary data options bit limited anyway. Another example might data access k-nearest neighbors underlying ambient data. case, able generate mid-near pairs needed PaCMAP’s performance luck anyway. However, sure scenarios exist. cases, PaCMAP good choice. cases can make UMAP want uwot easily enough.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/pacmap-comparison.html","id":"ok-but-will-it-be-as-fast","dir":"Articles","previous_headings":"Conclusions","what":"Ok, but will it be as fast?","title":"PaCMAP Comparison","text":"bottleneck methods speed nearest neighbor search turn dependent input dimensionality. PCA process sufficiently fast overall speed PaCMAP approach outperform UMAP approach. uwot find irlba tends bit slower truncated SVD available sklearn Python used PaCMAP depends linear algebra library system. Due internal differences, still anticipate PaCMAP bit faster UMAP conditions. practice, difference isn’t large experience (seconds mnist, example). speed matters recommend using: = 1, b = 1 umap2 don’t mind t-UMAP-like appearance. use approx_pow = TRUE want control min_dist spread (b directly). negative_sample_rate = 4 reduce amount negative sampling line PaCMAP effectively shouldn’t affect things much. consider using rng_type = \"deterministic\", use random number generator negative sampling. want pseudo-random sampling, rng_type = \"tausworthe\" faster default. hand, speed really matters, PaCMAP give vital seconds back without fiddle UMAP defaults.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/performance.html","id":"performance","dir":"Articles","previous_headings":"","what":"Performance","title":"Performance","text":"December 31 2018 Updated timings, keeping better track versions numbers. get feel performance uwot, timings processing MNIST dataset, compared methods. wouldn’t take seriously, show uwot competitive methods. notes numbers generated: ran Windows machine, using R 3.5.2 Python 3.7.0. official LargeVis implementation built Visual Studio 2017 Community Edition may properly optimized (VS solution available fork). R packages, MNIST data downloaded via snedata package. Python packages, sklearn.datasets.fetch_mldata('MNIST original') used. LargeVis source code contains MNIST example data already present. FIt-SNE, used provided Windows binary via R wrapper (hence used MNIST data snedata package). reported time second FIt-SNE entry table includes 13 seconds takes reduce dimensionality 50 via PCA, using irlba (package dimension reduction used Rtsne last reported time uwot). default openTSNE uses FFT approach FIt-SNE , don’t know ’s much slower, apart use numpy version FFT rather FFTW library, understanding shouldn’t make much difference dataset size MNIST. Perhaps Windows thing. uwot, bottleneck typical settings nearest neighbor search, currently provided Annoy, whereas Python implementation uses pynndescent, nearest neighbor descent approach. optimization side things, uwot defaults conservative. Using approx_pow = TRUE uses fastPrecisePow approximation pow function suggested Martin Ankerl. think seem like typical values b (0.7 0.9) squared distance (0-1000), found maximum relative error 0.06. However, haven’t done much testing, beyond looking see results examples page obviously worsened. Results table approx_pow = TRUE show worthwhile improvement. Using n_sgd_threads 1 thread give reproducible results, behave worse LargeVis regard, many visualization needs, also worth trying.","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/pycompare.html","id":"norb","dir":"Articles","previous_headings":"Results","what":"norb","title":"Python Comparison","text":"’s result seems worth commenting . uwot results show slightly less structure, large blue loop seems broken . suspect PCA preprocessing 100 dimensions used uwot aggressive might throwing away much variance dataset. Compared ground truth 15 nearest neighbors using FNN, Annoy results pca = 100 82% accurate. Without PCA, accuracy goes 99% uwot results much closer UMAP results. image , left. Meanwhile, use PCA results input UMAP, output now looks bit like uwot results. ’s image lower right. Unfortunately, high dimensionality norb makes running Annoy directly slow indeed: single-thread, took 4 half hours, versus around 5 minutes PCA case. revisit nearest neighbor accuracies later. Note also norb one datasets graph contains two separate components, initialization output coordinates different uwot UMAP, UMAP’s “meta-embedding” approach better retaining local structure.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/pycompare.html","id":"macosko2015","dir":"Articles","previous_headings":"Results","what":"macosko2015","title":"Python Comparison","text":"result shows largest deviation uwot UMAP. big cyan cluster obviously different shape two plots. source difference seems nearest neighbor results. turns uwot UMAP trouble find good approximations nearest neighbors dataset, UMAP lot better. get detail nearest neighbor accuracies across datasets next section. now, two plots. left-hand plot uwot results uses accurate nearest neighbor data calculated UMAP: uwot results now resemble UMAP results, reassuring. right hand plot uses exact nearest neighbor results calculated via FNN. Even though UMAP nearest neighbor data better uwot produced, can still see obvious difference two plots, ’s clear data set presents challenge approximate nearest neighbor methods considered .","code":""},{"path":"https://jlmelville.github.io/uwot/articles/pycompare.html","id":"cifar10","dir":"Articles","previous_headings":"Results","what":"cifar10","title":"Python Comparison","text":"Based results dataset UMAP examples gallery, wasn’t expecting results feast eyes. seeing difference results macosko2015, just relieved two results look disappointing similar way.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/pycompare.html","id":"nearest-neighbor-accuracy","dir":"Articles","previous_headings":"","what":"Nearest Neighbor Accuracy","title":"Python Comparison","text":"norb macosko2015 results suggest use PCA nearest neighbor search methods present biggest source difference uwot UMAP. details. small datasets, implementations means less 4,096 observations, uwot UMAP calculate exact nearest neighbors. larger datasets, uwot uses Annoy method, UMAP uses pynndescent, uses random projection trees, followed nearest neighbor descent. larger datasets, used FNN package generate exact 15 nearest neighbors compared results uwot UMAP. UMAP, used default settings. uwot, applied pca = 100 currently use high dimensional datasets. third column shows amount variance explained keeping 100 components. UMAP pretty incredible job nearly every dataset, ’s fast . doesn’t need use PCA reduce dimensionality. obvious exception macosko2015. unable find combination parameters pynndescent produced good results, e.g. increasing number trees RP tree initialization number iterations nearest neighbor descent phase. n_trees=250, able get 54.9% accuracy, started get memory errors much beyond setting, nearest neighbor descent tends converge 5-6 iterations anyway. better may require manipulation sampling rate candidate list size used nearest neighbor descent, can quickly cause large increase memory run time aren’t careful. Also, can’t fiddle parameters UMAP version 0.3.8 anyway. uwot results… well, ’re less impressive UMAP’s. tasic2018 results pretty bad, macosko2015 results can fairly described horrendous. accuracies show rough correlation percentage variance explained 100 components. Results tasic2018 especially macosko2015 show 100 components may enough extract meaningful variation datasets, might explain inability find true nearest neighbors. uwot defaults Annoy search somewhat arbitrary, copied LargeVis. package used nearest neighbor descent improve accuracy, increasing Annoy’s n_trees parameter can give small improvement datasets, increase computation time probably worth small increase accuracy ’ve seen trying . seems cases current defaults don’t work well, ’s much information thrown away PCA pre-processing. tasic2018 macosko2015 transcriptomics datasets clearly represent interesting challenge Annoy, macosko2015 troubles even pynndescent. datasets image datasets. Possibly way data scaled prepared biology datasets makes life harder approximate nearest neighbors? ’s hard know start comes unpicking causes macosko2015 perform differently, histogram nearest neighbor distances macosko2015 tasic2018 (top row), misbehave Annoy, bottom row histograms mnist fashion, better behaved: biomodal distribution macosko2015 tasic2018 seem similar mnist fashion. said, tasic2018 behaves perfectly well pynndescent method, see obvious answers .","code":""},{"path":"https://jlmelville.github.io/uwot/articles/pycompare.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"Python Comparison","text":"good news datasets, differences uwot’s UMAP implementation Python UMAP implementation makes little difference. bad news couple cases see differences. biggest offender macosko2015 neither method perfect job. Nonetheless, UMAP able calculate nearest neighbors accurately quickly uwot. One source difference use Annoy nearest neighbor calculation method. seem solidly slower pynndescent, experience, can reach high accuracies. even takes . March 8 2020: Windows, used issue Annoy index larger 2GB disk couldn’t read back . Make sure using version RcppAnnoy 0.0.15 later avoid . time search Annoy index seems start scaling quite badly dimensionality > 1000, can get away picking 500-1000 random features use , try . 1,152 random features (.e. 1/16th pixels innorb), visual results aren’t much worse approach nearest neighbor search runs 100 times faster (50 minutes vs 20 seconds): Otherwise, high-dimensional dataset, consider PCA. don’t solid advice number components retain use PCA preprocessing. Less usual t-SNE default 50 probably unwise. use pca = 100 examples, ’ve seen, can still sufficiently perturbs pairwise distances input data give differences output. nice use components get better trade-accuracy speed, even fast partial PCA routines used uwot (courtesy irlba), extracting 100 components can quite time-consuming.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/pycompare.html","id":"what-about-t-sne","dir":"Articles","previous_headings":"Conclusions","what":"What about t-SNE?","title":"Python Comparison","text":"uwot dimensionality reduction use PCA Annoy approximate nearest neighbor search. LargeVis uses , t-SNE package Fit-SNE Annoy option openTSNE seems least considered (see e.g. https://github.com/pavlin-policar/openTSNE/issues/28). also standard practice t-SNE reduce dimensionality: Rtsne, wrapper around de-facto standard Barnes Hut t-SNE routine. keep 50 components via PCA default. PCA issue effect t-SNE? Well, little bit. results norb macosko2015 (openTSNE’s documentation discovered dataset, happens) t-SNE results Rtsne, using perplexity = 15. images left use PCA reduce initial dimensionality 100, ones right use raw input data. norb results seem fairly unaffected, also see macosko2015 ’s change main large cluster. Usually, don’t set perplexity low 15, also differences perplexity set typical value, like 50? Also, default PCA dimensionality setting Rtsne 50, 100, t-SNE plots repeated perplexity = 50 either pca = TRUE, partial_pca = TRUE, initial_dims = 50 left, pca = FALSE right: , norb dataset less affected macosko2015 latter definite effect preprocessing PCA. something bear mind applying PCA data even t-SNE. balance, ’d probably live effect applying PCA data, just like uwot, Rtsne input processing can take hours high dimensional datasets. Finally, people consider results obtained PCA reliable using “raw” input, PCA can considered way denoise data, lower-variance components assumed irrelevant. might bit biased, UMAP plots macosko2015, think default uwot one shows best separation clusters, maybe ’s something . also reason tabulated amount variance 100 components explained dataset. datasets keep large majority variance 100 components, case macosko2015, prepared argue ok throw away 60% variance data. ’m sure can done, need solid, ideally biology-based justification. future version uwot hopefully contain implementation nearest neighbor descent optimization reduce gap nearest neighbor accuracy UMAP.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/rnndescent-umap.html","id":"installing-rnndescent","dir":"Articles","previous_headings":"","what":"Installing rnndescent","title":"Using rnndescent for nearest neighbors","text":"Now install rnndescent CRAN:","code":"install.packages(\"rnndescent\") library(rnndescent)"},{"path":"https://jlmelville.github.io/uwot/articles/rnndescent-umap.html","id":"umap-with-nndescent","dir":"Articles","previous_headings":"","what":"UMAP with nndescent","title":"Using rnndescent for nearest neighbors","text":"uwot can now use rnndescent nearest neighbor search set nn_method = \"nndescent\". settings used give reasonable results batch mode (although feel free change n_sgd_threads however many threads feel comfortable system using) return model can use embed test set data. grant rnndescent lot say goes business. can always set verbose = FALSE.","code":"library(uwot) fashion_train_umap <-   umap(     X = fashion_train,     nn_method = \"nndescent\",     batch = TRUE,     n_epochs = 500,     n_sgd_threads = 6,     ret_model = TRUE,     verbose = TRUE   ) UMAP embedding parameters a = 1.896 b = 0.8006 Converting dataframe to numerical matrix Read 60000 rows and found 784 numeric columns Using alt metric 'sqeuclidean' for 'euclidean' Initializing neighbors using 'tree' method Calculating rp tree k-nearest neighbors with k = 15 n_trees = 21 max leaf size = 15 margin = 'explicit' using 6 threads Using euclidean margin calculation 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----] *************************************************** Extracting leaf array from forest Creating knn using 164273 leaves 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----] *************************************************** Running nearest neighbor descent for 16 iterations using 6 threads 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----] *************************************************** Convergence: c = 132 tol = 900 Finished Keeping 1 best search trees using 6 threads 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----] *************************************************** Min score: 2.41727 Max score: 2.4626 Mean score: 2.44337 Using alt metric 'sqeuclidean' for 'euclidean' Converting graph to sparse format Diversifying forward graph Occlusion pruning with probability: 1 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----] *************************************************** Diversifying reduced # edges from 840000 to 240155 (0.02333% to 0.006671% sparse) Degree pruning reverse graph to max degree: 22 Degree pruning to max 22 reduced # edges from 240155 to 239741 (0.006671% to 0.006659% sparse) Diversifying reverse graph Occlusion pruning with probability: 1 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----] *************************************************** Diversifying reduced # edges from 239741 to 214714 (0.006659% to 0.005964% sparse) Merging diversified forward and reverse graph Degree pruning merged graph to max degree: 22 Degree pruning to max 22 reduced # edges from 302918 to 302882 (0.008414% to 0.008413% sparse) Finished preparing search graph Commencing smooth kNN distance calibration using 8 threads with target n_neighbors = 15 Initializing from normalized Laplacian + noise (using irlba) Commencing optimization for 500 epochs, with 1359492 positive edges using 8 threads Using method 'umap' Optimizing with Adam alpha = 1 beta1 = 0.5 beta2 = 0.9 eps = 1e-07 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----| **************************************************| Optimization finished"},{"path":"https://jlmelville.github.io/uwot/articles/rnndescent-umap.html","id":"transforming-test-data","dir":"Articles","previous_headings":"UMAP with nndescent","what":"Transforming test data","title":"Using rnndescent for nearest neighbors","text":"Now UMAP model can transform test set data. Now notice looks exactly call make used Annoy nearest neighors: information needed uwot work use rnndescent querying new neighbors encapsulated fashion_train_umap model generated. rnndescent lot chattier using Annoy.","code":"fashion_test_umap <-   umap_transform(     X = fashion_test,     model = fashion_train_umap,     n_sgd_threads = 6,     verbose = TRUE   ) Read 10000 rows and found 784 numeric columns Processing block 1 of 1 Reading metric data from forest Using alt metric 'sqeuclidean' for 'euclidean' Querying rp forest for k = 15 with caching using 6 threads 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----] *************************************************** Finished Searching nearest neighbor graph with epsilon = 0.1 and max_search_fraction = 1 using 6 threads Graph contains missing data: filling with random neighbors Finished random fill 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----] *************************************************** min distance calculation = 0 (0.00%) of reference data max distance calculation = 1810 (3.02%) of reference data avg distance calculation = 172 (0.29%) of reference data Finished Commencing smooth kNN distance calibration using 6 threads with target n_neighbors = 15 Initializing by weighted average of neighbor coordinates using 6 threads Commencing optimization for 167 epochs, with 150000 positive edges using 6 threads Using method 'umap' Optimizing with Adam alpha = 1 beta1 = 0.5 beta2 = 0.9 eps = 1e-07 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----| **************************************************| Finished"},{"path":"https://jlmelville.github.io/uwot/articles/rnndescent-umap.html","id":"plotting-the-results","dir":"Articles","previous_headings":"UMAP with nndescent","what":"Plotting the results","title":"Using rnndescent for nearest neighbors","text":"Now take look results, using ggplot2 plotting, Polychrome suitable categorical palette. following code creates palette 10 (hopefully) visually distinct colors map point type fashion item represents. found Description factor column original data. results:  results typical Fashion MNIST result UMAP. example, see first image part Python UMAP documentation. looks like rnndescent default settings good job dataset.","code":"install.packages(c(\"ggplot2\", \"Polychrome\")) library(ggplot2) library(Polychrome) palette <- as.vector(Polychrome::createPalette(   length(levels(fashion$Description)) + 2,   seedcolors = c(\"#ffffff\", \"#000000\"),   range = c(10, 90) )[-(1:2)]) ggplot(   data.frame(fashion_train_umap$embedding, Description = fashion_train$Description),   aes(x = X1, y = X2, color = Description) ) +   geom_point(alpha = 0.1, size = 1.0) +   scale_color_manual(values = palette) +   theme_minimal() +   labs(     title = \"Fashion MNIST training set UMAP\",     x = \"\",     y = \"\",     color = \"Description\"   ) +   theme(legend.position = \"right\") +   guides(color = guide_legend(override.aes = list(size = 5, alpha = 1))) ggplot(   data.frame(fashion_test_umap, Description = fashion_test$Description),   aes(x = X1, y = X2, color = Description) ) +   geom_point(alpha = 0.4, size = 1.0) +   scale_color_manual(values = palette) +   theme_minimal() +   labs(     title = \"Fashion MNIST test set UMAP\",     x = \"\",     y = \"\",     color = \"Description\"   ) +   theme(legend.position = \"right\") +   guides(color = guide_legend(override.aes = list(size = 5, alpha = 1)))"},{"path":"https://jlmelville.github.io/uwot/articles/rnndescent-umap.html","id":"a-minor-advantage-of-using-rnndescent","dir":"Articles","previous_headings":"","what":"A Minor Advantage of using rnndescent","title":"Using rnndescent for nearest neighbors","text":"use nn_method = \"nndescent\" UMAP model returned ret_model = TRUE can saved loaded using standard R functions saveRDS readRDS. don’t need use uwot-specific save_uwot load_uwot, need worry unloading model unload_uwot, must Annoy-based UMAP models. consequence rnndescent storing index-related data pure R (wrapping existing C++ classes), aware can lead much larger models disk RAM.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/rnndescent-umap.html","id":"using-rnndescent-externally","dir":"Articles","previous_headings":"","what":"Using rnndescent externally","title":"Using rnndescent for nearest neighbors","text":"want control behavior rnndescent can use directly create nearest neighbor graph pass result nn_method argument umap. isn’t usually necessary want control recommend using nn_args argument umap, list arguments pass directly rnndescent. decide use rnndescent directly, following section mainly uses default argument demonstrates workflow can customized needs. resulting UMAP plots essentially identical output using nn_mmethod = \"nndescent\".","code":""},{"path":"https://jlmelville.github.io/uwot/articles/rnndescent-umap.html","id":"build-an-index-for-the-training-data","dir":"Articles","previous_headings":"Using rnndescent externally","what":"Build an index for the training data","title":"Using rnndescent for nearest neighbors","text":"First, build search index using training data. use many threads (n_threads) feel comfortable . can also set verbose = TRUE, building index see lot output won’t reproduce . k parameter number nearest neighbors looking . index tuned work well number neighbors. metric parameter distance metric use, default Euclidean, don’t need specify . Finally, stochastic aspect index building. may get slightly different results , leaving seed unset must live inherently random nature index building process. ’s returned index? bunch stuff: support searching index. want graph. list two elements, idx dist, contain nearest neighbor indices distances respectively. nearest neighbor item , expect first column idx sequence 1, 2, 3, … first column dist zeros. nearest neighbor methods like HNSW Annoy, run separate query step get k-nearest neighbors data used build index. One advantages rnndescent can get data directly index without separate query step. Note can still query index want , results might bit accurate, quietly confident results index sufficient UMAP.","code":"fashion_index <- rnnd_build(fashion_train, k = 15, n_threads = 6) names(fashion_index) [1] \"graph\"           \"prep\"            \"data\"            \"original_metric\" \"use_alt_metric\"  \"search_forest\"   \"search_graph\" fashion_index$graph$idx[1:3, 1:3] [,1]  [,2]  [,3] [1,]    1 25720 27656 [2,]    2 42565 37551 [3,]    3 53514 35425 fashion_index$graph$dist[1:3, 1:3] [,1]      [,2]      [,3] [1,]    0 1188.7826 1215.3440 [2,]    0 1048.0482 1068.3951 [3,]    0  532.6199  632.1653"},{"path":"https://jlmelville.github.io/uwot/articles/rnndescent-umap.html","id":"query-the-test-data","dir":"Articles","previous_headings":"Using rnndescent externally","what":"Query the test data","title":"Using rnndescent for nearest neighbors","text":"get test set neighbors, query index test data: querying test data training data, nearest neighbor test item test item . now nearest neighbor data need. good news ’s already format uwot can use can proceed straight running UMAP.","code":"fashion_test_query_neighbors <-   rnnd_query(     index = fashion_index,     query = fashion_test,     k = 15,     n_threads = 6,     verbose = TRUE   ) fashion_test_query_neighbors$idx[1:3, 1:3] [,1]  [,2]  [,3] [1,] 18095 53940 18353 [2,]  8573 31349  3885 [3,]   286 38144  3422 fashion_test_query_neighbors$dist[1:3, 1:3] [,1]      [,2]      [,3] [1,]  482.2966  681.9905  708.4991 [2,] 1308.0019 1329.3134 1382.7317 [3,]  466.0322  538.5378  555.8795"},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/rnndescent-umap.html","id":"umap-on-training-data","dir":"Articles","previous_headings":"Using rnndescent nearest neighbors with UMAP","what":"UMAP on training data","title":"Using rnndescent for nearest neighbors","text":"use pre-computed nearest neighbor data uwot pass nn_method parameter. case, graph item fashion_index. See HSNW article details parameters, designed give pretty typical UMAP results.","code":"fashion_train_umap <-   umap(     X = NULL,     nn_method = fashion_index$graph,     batch = TRUE,     n_epochs = 500,     n_sgd_threads = 6,     ret_model = TRUE,     verbose = TRUE   ) UMAP embedding parameters a = 1.896 b = 0.8006 Commencing smooth kNN distance calibration using 6 threads with target n_neighbors = 15 Initializing from normalized Laplacian + noise (using irlba) Commencing optimization for 500 epochs, with 1359454 positive edges using 6 threads Using method 'umap' Optimizing with Adam alpha = 1 beta1 = 0.5 beta2 = 0.9 eps = 1e-07 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----| **************************************************| Optimization finished Note: model requested with precomputed neighbors. For transforming new data, distance data must be provided separately"},{"path":"https://jlmelville.github.io/uwot/articles/rnndescent-umap.html","id":"transforming-test-data-1","dir":"Articles","previous_headings":"Using rnndescent nearest neighbors with UMAP","what":"Transforming test data","title":"Using rnndescent for nearest neighbors","text":"Now UMAP model can transform test set data. don’t need pass test set data except neighbors nn_method: point can plot results, resemble shown earlier.","code":"fashion_test_umap <-   umap_transform(     X = NULL,     model = fashion_train_umap,     nn_method = fashion_test_query_neighbors,     n_sgd_threads = 6,     verbose = TRUE   ) Read 10000 rows Processing block 1 of 1 Commencing smooth kNN distance calibration using 6 threads with target n_neighbors = 15 Initializing by weighted average of neighbor coordinates using 6 threads Commencing optimization for 167 epochs, with 150000 positive edges using 6 threads Using method 'umap' Optimizing with Adam alpha = 1 beta1 = 0.5 beta2 = 0.9 eps = 1e-07 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----| **************************************************| Finished"},{"path":"https://jlmelville.github.io/uwot/articles/rnndescent-umap.html","id":"a-potentially-simpler-workflow","dir":"Articles","previous_headings":"Using rnndescent nearest neighbors with UMAP","what":"A potentially simpler workflow","title":"Using rnndescent for nearest neighbors","text":"don’t want transform new data, can make life bit easier. Instead rnnd_build, use rnnd_knn behaves lot like rnnd_build doesn’t preserve index index preparation. saves time nearest neighbor results returned directly return result. Let’s use full Fashion MNIST results example: can pass fashion_knn nn_method: (spared log output). don’t need pass ret_model = TRUE can’t transform new data. UMAP plot full Fashion MNIST dataset: Everything looks expected. However, think might want transform new data future, need completely restart process, rnnd_build using ret_model = TRUE.","code":"fashion_knn <- rnnd_knn(fashion, k = 15, n_threads = 6) names(fashion_knn) [1] \"idx\"  \"dist\" fashion_umap <-   umap(     X = NULL,     nn_method = fashion_knn,     batch = TRUE,     n_epochs = 500,     n_sgd_threads = 6,     verbose = TRUE   ) ggplot(   data.frame(fashion_umap, Description = fashion$Description),   aes(x = X1, y = X2, color = Description) ) +   geom_point(alpha = 0.1, size = 1.0) +   scale_color_manual(values = palette) +   theme_minimal() +   labs(     title = \"Fashion MNIST UMAP\",     x = \"\",     y = \"\",     color = \"Description\"   ) +   theme(legend.position = \"right\") +   guides(color = guide_legend(override.aes = list(size = 5, alpha = 1)))"},{"path":"https://jlmelville.github.io/uwot/articles/rnndescent-umap.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"Using rnndescent for nearest neighbors","text":"want use rnndescent uwot, : Install rnndescent use nn_method = \"nndescent\". handles cases. want use precomputed nearest neighbors, : Run rnndescent::rnnd_build training data. Run uwot::umap nn_method set graph item result rnnd_build remember set ret_model = TRUE. transform new data: Run rnndescent::rnnd_query test data, using result rnnd_build index parameter. Run uwot::umap_transform nn_method set result rnnd_query. don’t want transform new data, ’s even easier: Run rnndescent::rnnd_knn training data. Run uwot::umap nn_method set result rnnd_knn.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"text-analysis-of-the-20-newsgroups-dataset","dir":"Articles","previous_headings":"","what":"Text Analysis of the 20 Newsgroups Dataset","title":"Working with Sparse Data","text":"great use case working sparse data TF-IDF text analysis. Despite prevalence deep-learning-based text embedding, TF-IDF simple, far less compute intensive can surprisingly effective. dimensionality reduction methods don’t work well sparse data, need input data dense. uwot different, one Python UMAP’s unique features can work sparse data, due use PyNNDescent nearest neighbor search. Python UMAP sparse data tutorial uses 20 Newsgroups dataset long wanted replicate R. main barriers lack straight-forward way download data lack fast approximate nearest neighbor search package can work sparse data. first problem solved snedata package, recently added download_twenty_newsgroups function. took large chunk one afternoon evening. second problem solved replicating PyNNDescent package R. took bit longer, mere 4 years later, rnndescent package finally usable state. Consequently, uwot package can now make use rnndescent support sparse input via umap2 function. let’s get started. Apart Python example , good resources text analysis dataset can found chapter 9 text mining R juicily-titled Python notebook 20 Newsgroups Secrets.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"downloading-the-20-newsgroups-dataset","dir":"Articles","previous_headings":"","what":"Downloading the 20 Newsgroups Dataset","title":"Working with Sparse Data","text":"20 Newsgroups dataset (nearly) 20,000 usenet posts sampled (nearly) equally 20 different newsgroups. webpage linked contains information data’s provenance structure. can use snedata::download_twenty_newsgroups download : subset argument can used download training test set data use . verbose argument useful see ’s : take minutes download . ’s tar.gz file unfortunately, unaware way stream tar data directly R, download_twenty_newsgroups download file temporary directory, extract everything, attempt clean afterwards. something goes wrong log directory downloaded . data returned data frame 6 columns: Id column unique identifier post, FileId filename combined Subset, Text post text, Subset either \"train\" \"test\", Label integer label newsgroup Newsgroup name newsgroup. columns interested Text Newsgroup, data manipulation need , Id can keep track rows. ’s first row without Text column (can safely ignore row id): ’s first characters text: lot came , invite investigate (fill terminal ) . Notably, snedata doesn’t processing posts, headers still , footers quotations previous posts current poster might replying .","code":"devtools::install_github(\"jlmelville/snedata\")  ng20 <- snedata::download_twenty_newsgroups(subset = \"all\", verbose = TRUE) dim(ng20) [1] 18846     6 names(ng20) [1] \"Id\"        \"FileId\"    \"Text\"      \"Subset\"    \"Label\"     \"Newsgroup\" ng20[1, -3] Id FileId Subset Label   Newsgroup alt.atheism.1 train_1  49960  train     0 alt.atheism substr(ng20[1, ]$Text, 1, 72) [1] \"From: mathew <mathew@mantis.co.uk>\\nSubject: Alt.Atheism FAQ: Atheist Res\""},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"preprocessing-the-data","dir":"Articles","previous_headings":"","what":"Preprocessing the Data","title":"Working with Sparse Data","text":"Unless get luck pre-cleaned datasets, text data usually requires fair amount processing turn vector numbers UMAP can work , need text wrangling start . ’ll need install dplyr, tidyr stringr, : Text Mining R good place start get ideas , bear mind dataset structured bit differently, let’s temporarily expand dataset splitting text new lines like tidy text mining example row line original post: Now first row looks like: first filtering remove headers (anything first blank line) footers (assumed anything first line repeated hyphens): attempts removed quoted text: functions bit different ones used python side scikit-learn – see functions strip_newsgroup_header, strip_newsgroup_quoting strip_newsgroup_footer, R functions good enough tidy text modelers, ’s good enough . Also, tidy text book, explicitly remove two items “contained large amount non-text content”. Specifically images, can find “20 Newsgroups Secrets” notebook linked (don’t get excited). seems binary data posts, try catch , end filtering later. tidy text example goes examine word frequencies newsgroup whole, go back analyzing per post basis. Now, split posts lines, need unsplit , also re-associate columns original ng20 data: Back original structure. processing isn’t perfect, decent job. Now can actual text processing.","code":"install.packages(c(\"dplyr\", \"tidyr\", \"stringr\")) library(dplyr) library(tidyr) library(stringr) ng20spl <- ng20 |> separate_longer_delim(`Text`, delim = \"\\n\") dim(ng20spl) [1] 834780      6 ng20spl[1, ] Id FileId                               Text Subset Label   Newsgroup 1 train_1  49960 From: mathew <mathew@mantis.co.uk>  train     0 alt.atheism cleaned_text <- ng20spl |>   group_by(`Newsgroup`, `Id`) |>   filter(     cumsum(`Text` == \"\") > 0,     cumsum(str_detect(`Text`, \"^--\")) == 0   ) |>   ungroup() dim(cleaned_text) [1] 599249      6 cleaned_text <- cleaned_text |>   filter(     str_detect(`Text`, \"^[^>]+[A-Za-z\\\\d]\") |       `Text` == \"\",     !str_detect(`Text`, \"writes(:|\\\\.\\\\.\\\\.)$\"),     !str_detect(`Text`, \"^In article <\")   ) dim(cleaned_text) [1] 445080      6 text_unsplit <-   cleaned_text |>   group_by(`Id`) |>   summarise(`Text` = paste(`Text`, collapse = \" \")) text_unsplit <- ng20 |>   select(-`Text`) |>   left_join(text_unsplit, by = \"Id\") text_unsplit <-   text_unsplit[, c(\"Id\", \"FileId\", \"Text\", \"Subset\", \"Label\", \"Newsgroup\")] dim(text_unsplit) [1] 18846     6"},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"text-mining","dir":"Articles","previous_headings":"","what":"Text Mining","title":"Working with Sparse Data","text":"’m going use tm package text mining: going create “corpus” text apply various processing steps normalize text, handling whitespace, case, removing punctuation . create Corpus initially, also convert text encoding latin1 UTF-8 via iconv function. far can tell, 20 Newsgroups data latin1 encoding (sklearn also reads encoding), convert UTF-8 rest processing. still documents odd formatting characters tm seems deal without issue.","code":"install.packages(\"tm\") library(tm) corpus <-   Corpus(VectorSource(iconv(text_unsplit$Text, \"latin1\", \"UTF-8\"))) |>   tm_map(content_transformer(tolower)) |>   tm_map(removePunctuation) |>   tm_map(removeNumbers) |>   tm_map(removeWords, stopwords(\"english\")) |>   tm_map(stripWhitespace) length(corpus) [1] 18846"},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"filtering-out-bad-documents","dir":"Articles","previous_headings":"Text Mining","what":"Filtering Out Bad Documents","title":"Working with Sparse Data","text":"point, take bit inspiration 20 Newsgroups Secrets start looking distribution document lengths terms number words number characters. corpus really mainly English text expect see kind rough relationship sorts values deviations likely anomalies need investigating. bit diversion business hand, might well see can come reasonable filter values using simple methods. Clearly quite skew terms number words. messages now zero words, one nearly 7000. median less 50 words. Let’s look distribution number words, stop 95% data avoid outliers: 95% data contains 250 words less. massive document? Let’s (carefully) take look: Ah, ’s FAQ. Ok makes sense. Now let’s look distribution raw length documents terms number characters: Seems reminiscent word count distribution. let’s see related (’d assume related never know): Ok ’s good relation, makes documents don’t appear main trendline suspicious. easy way deal define average word length: NA’s documents zero words. looks like another rather skewed distribution: 95% data average word length 8 less. gives us bit feel might reasonable filter settings get rid unsuitable data. filters established 20 Newsgroups Secrets document length based word seem reasonable : less 10 words suspicious 2000 words also suspicious. avoid making even longer needs , won’t investigating suspicious documents article. Ok, think ’ve done enough move .","code":"count_words <- function(doc) {   doc |>     strsplit(\" \") |>     unlist() |>     length() }  nwords <- sapply(corpus, count_words) summary(nwords) Min. 1st Qu.  Median    Mean 3rd Qu.    Max.     0.00   24.00   46.00   84.42   86.00 6592.00 hist(nwords[nwords <= quantile(nwords, 0.95)], main = \"0-95% word count distribution\") substr(ng20[which.max(nwords), ]$Text, 0, 80) [1] \"From: jeh@cmkrnl.com\\nSubject: Electrical wiring FAQ (was: A question about 120VA\" nchars <- sapply(corpus, function(doc) {   doc |> str_length() }) summary(nchars) Min. 1st Qu.  Median    Mean 3rd Qu.    Max.      0.0   158.0   310.0   594.6   593.0 67441.0 plot(nwords, nchars, main = \"Number of words vs number of characters\") avg_word_lengths <- nchars / nwords summary(avg_word_lengths) Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's    1.000   6.294   6.750   6.720   7.213  27.533      77 hist(avg_word_lengths[avg_word_lengths <= quantile(avg_word_lengths, 0.95, na.rm = TRUE)],   main = \"0-95% average word length distribution\" ) is_suspiciously_short <- function(doc) {   doc |> count_words() <= 9 } suspiciously_short_indices <- sapply(corpus, is_suspiciously_short) corpus <- corpus[!suspiciously_short_indices] text_unsplit <- text_unsplit[!suspiciously_short_indices, ] length(corpus) [1] 17708 is_suspiciously_long <- function(doc) {   doc |> count_words() >= 2000 } suspiciously_long_indices <- sapply(corpus, is_suspiciously_long) corpus <- corpus[!suspiciously_long_indices] text_unsplit <- text_unsplit[!suspiciously_long_indices, ] length(corpus) [1] 17675 avg_word_len <- function(doc) {   (doc |> str_length()) / (doc |> count_words()) } has_suspiciously_short_words <- function(doc) {   doc |> avg_word_len() < 4 }  suspiciously_short_word_indices <- sapply(corpus, has_suspiciously_short_words) corpus <- corpus[!suspiciously_short_word_indices] text_unsplit <- text_unsplit[!suspiciously_short_word_indices, ] length(corpus) [1] 17670 has_suspiciously_long_words <- function(doc) {   doc |> avg_word_len() > 15 } suspiciously_long_word_indices <- sapply(corpus, has_suspiciously_long_words) corpus <- corpus[!suspiciously_long_word_indices] text_unsplit <- text_unsplit[!suspiciously_long_word_indices, ] length(corpus) [1] 17666"},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"tf-idf","dir":"Articles","previous_headings":"","what":"TF-IDF","title":"Working with Sparse Data","text":"next step convert corpus matrix TF-IDF values: Nearly 90,000 dimensions, one weighted word frequency. sparse : less 0.1% matrix non-zero. One way proceed point use SVD can work sparse matrices, irlba, turn dense representation far fewer dimensions. However slow takes lot memory spend time working many dimensions use, slows things even don’t get right first time. Fortunately, now uwot can use rnndescent, can work sparse data directly.","code":"tfidf <- weightTfIdf(DocumentTermMatrix(corpus)) dim(tfidf) [1] 17666 88894 Matrix::nnzero(tfidf) / prod(dim(tfidf)) 0.0006920568"},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"umap-with-nn-descent","dir":"Articles","previous_headings":"","what":"UMAP with NN-Descent","title":"Working with Sparse Data","text":"use uwot sparse data need convert TF-IDF matrix format Matrix package rnndescent can handle: historical reasons breaking backwards compatibility, can’t use umap function sparse input data. Instead use umap2 function, newer version umap slightly better defaults can take sparse matrix input. also need load rnndescent package. terms non-default options, ’m going use batch = TRUE setting, longer usual optimization, non-zero dens_scale model original density differences data (see article LEOPOLD details). also save nearest neighbors ret_nn = TRUE can compare exact results calculate later. follow along code , get exact results plot , due stochastic nature UMAP. see something similar (don’t, please file issue).","code":"library(Matrix) tfidf_sp <-   sparseMatrix(     i = tfidf$i,     j = tfidf$j,     x = tfidf$v,     dims = dim(tfidf)   ) install.packages(\"rnndescent\") library(rnndescent) library(uwot) ng20_umap <-   umap2(     X = tfidf_sp,     nn_method = \"nndescent\",     metric = \"hellinger\",     n_epochs = 1000,     batch = TRUE,     dens_scale = 0.5,     ret_nn = TRUE,     verbose = TRUE   ) Using nndescent for nearest neighbor search UMAP embedding parameters a = 1.577 b = 0.8951 Read 17666 rows and found 88894 numeric columns Using alt metric 'alternative-hellinger' for 'hellinger' Initializing neighbors using 'tree' method Calculating rp tree k-nearest neighbors with k = 15 n_trees = 17 max leaf size = 15 margin = 'explicit' using 6 threads Using angular margin calculation 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----] *************************************************** Extracting leaf array from forest Creating knn using 32172 leaves 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----] *************************************************** Running nearest neighbor descent for 14 iterations using 6 threads 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----] *************************************************** Convergence: c = 170 tol = 264.99 Finished Commencing smooth kNN distance calibration using 6 threads with target n_neighbors = 15 17 smooth knn distance failures Initializing from normalized Laplacian + noise (using irlba) Range-scaling initial input columns to 0-10 Commencing optimization for 1000 epochs, with 389106 positive edges using 6 threads Using method 'leopold' Optimizing with Adam alpha = 1 beta1 = 0.5 beta2 = 0.9 eps = 1e-07 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----| **************************************************| Optimization finished"},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"plot-the-results","dir":"Articles","previous_headings":"","what":"Plot the Results","title":"Working with Sparse Data","text":"going bit challenging plot, 20 unique colors 20 newsgroups. palettes don’t offer anything size, going use Polychrome package generate palette 20 colors hopefully reasonably distinguishable. similar approach Python package glasbey. also rotate coordinates align along principal axes: Ok, now make plot ggplot2: Although can’t compare result easily Python example, one plots training test set separately, isn’t legend, think done pretty good job separating least 20 newsgroups.","code":"library(Polychrome) set.seed(42)  palette <- as.vector(Polychrome::createPalette(   length(levels(text_unsplit$Newsgroup)) + 2,   seedcolors = c(\"#ffffff\", \"#000000\"),   range = c(10, 90) )[-(1:2)]) ng20_umap_rotated <- prcomp(ng20_umap$embedding)$x library(ggplot2)  ggplot(   data.frame(ng20_umap_rotated, Newsgroup = text_unsplit$Newsgroup),   aes(x = PC1, y = PC2, color = Newsgroup) ) +   geom_point(alpha = 0.4, size = 0.5) +   scale_color_manual(values = palette) +   theme_minimal() +   labs(     title = \"Sparse Hellinger NN-Descent 20Newsgroups UMAP\",     x = \"\",     y = \"\",     color = \"Newsgroup\"   ) +   theme(legend.position = \"right\") +   guides(color = guide_legend(override.aes = list(size = 5, alpha = 1)))"},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"finding-exact-nearest-neighbors","dir":"Articles","previous_headings":"","what":"Finding Exact Nearest Neighbors","title":"Working with Sparse Data","text":"uwot supported using rnndescent internally sparse data, article documented process using rnndesent directly 20 Newsgroups data integrating umap function. still perfectly viable approach, especially want use exact nearest neighbors. uwot uses nearest neighbor descent method find approximate nearest neighbors, faster exact. follows ’ll use rnndescent find exact nearest neighbors brute force method see sort difference makes UMAP result. ’ll assume ’ve following along far, L1-normalized TF-IDF matrix tfidf_spl1 rnndescent loaded. Now let’s use brute-force search rnndescent. dataset small enough us handle exact search comfortably, least enough cores: took 35 seconds machine, whereas nearest neighbor descent took 5 seconds. said, case happy pay price extra 30 seconds avoid issues due approximation.","code":"tfidfl1_hell_bf <-   brute_force_knn(     tfidf_sp,     k = 15,     metric = \"hellinger\",     n_threads = 6,     verbose = TRUE   ) Using alt metric 'alternative-hellinger' for 'hellinger' Calculating brute force k-nearest neighbors with k = 15 using 6 threads 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----] *************************************************** Finished"},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"umap-with-exact-neighbors","dir":"Articles","previous_headings":"","what":"UMAP with Exact Neighbors","title":"Working with Sparse Data","text":"can now run UMAP exact neighbors. use parameters conjunction umap2 approximate nearest neighbors:","code":"ng20_umap_exact <-   umap2(     X = NULL,     nn_method = tfidfl1_hell_bf,     n_epochs = 1000,     batch = TRUE,     dens_scale = 0.5,     verbose = TRUE   ) UMAP embedding parameters a = 1.577 b = 0.8951 Commencing smooth kNN distance calibration using 6 threads with target n_neighbors = 15 16 smooth knn distance failures Initializing from normalized Laplacian + noise (using irlba) Range-scaling initial input columns to 0-10 Commencing optimization for 1000 epochs, with 377272 positive edges using 6 threads Using method 'leopold' Optimizing with Adam alpha = 1 beta1 = 0.5 beta2 = 0.9 eps = 1e-07 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----| **************************************************| Optimization finished"},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"plot-the-exact-neighbors-umap-results","dir":"Articles","previous_headings":"","what":"Plot the Exact Neighbors UMAP Results","title":"Working with Sparse Data","text":"Actually, although plots share lot features noticeable differences cluster placements.","code":"ggplot(   data.frame(prcomp(ng20_umap_exact)$x, Newsgroup = text_unsplit$Newsgroup),   aes(x = PC1, y = PC2, color = Newsgroup) ) +   geom_point(alpha = 0.4, size = 0.5) +   scale_color_manual(values = palette) +   theme_minimal() +   labs(     title = \"Sparse Hellinger Exact Neighbors 20Newsgroups UMAP\",     x = \"\",     y = \"\",     color = \"Newsgroup\"   ) +   theme(legend.position = \"right\") +   guides(color = guide_legend(override.aes = list(size = 5, alpha = 1)))"},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"hubness","dir":"Articles","previous_headings":"","what":"Hubness","title":"Working with Sparse Data","text":"’s going ? want, can re-run UMAP exact nearest neighbors times, tell us much result varies due random selection negative sampling. leave exercise . tried , plot pretty stable (sci.space cluster moves two locations). reason differences going performance approximate nearest neighbors. gone trouble generating exact nearest neighbors can answer question well actually reproducing exact neighbors? rnndescent neighbor_overlap function can use return value 0 (overlap two sets neighbors) 1 (perfect overlap): 75% accuracy, ’s less 90% usually happier . ’m totally surprised high dimensional datasets tend contain “hubs” (items appear nearest neighbor lists many items) tend make hard approximate nearest neighbor method good exploration. combination dataset, n_neighbors metric result neighbors hub. rnndescent can help us k_occur function count k-occurrence item: number times item appears nearest neighbor lists items (aka number reverse neighbors). hubness dataset can defined maximum number times item appears k-occurrence list, going normalize respect n_neighbors: dataset item gets fair share reverse neighbors result maximum k-occurrence n_neighbors, case normalized hubness 1. much larger value . personal experience normalized hubness exceeds 10, approximate nearest neighbors methods struggle well, least looking relatively low values n_neighbors default settings. Incidentally, don’t need exact nearest neighbors detect hubness, approximate results also act good diagnostic: necessarily good reproducing result exact neighbors, certainly tell something .","code":"neighbor_overlap(ng20_umap$nn$hellinger, tfidfl1_hell_bf) [1] 0.7484169 tfidfl1_hell_bf |> k_occur() |> max() / 15 20.13333 ng20_umap$nn$hellinger |> k_occur() |> max() / 15 30.46667"},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"umap-with-more-neighbors","dir":"Articles","previous_headings":"","what":"UMAP with More Neighbors","title":"Working with Sparse Data","text":"going ? cases, usually increasing n_neighbors helps. ’m going look n_neighbors = 50. also want equivalent brute force results see helped: Let’s see ’s hubness: different n_neighbors = 15 result, confidence nearest neighbor descent able handle level hubness given larger n_neighbors value. let’s find . similar level hubness reported, importantly, ’s accuracy like? 96%! Much better. Let’s see UMAP plot looks: Well, clusters spread bit (expected due 3-fold increase n_neighbors) also continued move around bit compared n_neighbors = 15 result. Hmm, let’s see least agreement exact case n_neighbors = 50: , clusters move compared approximate case. seems locations sci.crypt rec.sports.baseball/rec.sports.hockey pair particularly sensitive.","code":"tfidfl1_hell_bf_50 <-     brute_force_knn(         tfidf_sp,         k = 50,         metric = \"hellinger\",         n_threads = 6,         verbose = TRUE     ) tfidfl1_hell_bf_50 |> k_occur() |> max() / 50 [1] 20.1 ng20_umap_50 <-     umap2(         X = tfidf_sp,         nn_method = \"nndescent\",         metric = \"hellinger\",         n_neighbors = 50,         n_epochs = 1000,         batch = TRUE,         dens_scale = 0.5,         ret_nn = TRUE,         verbose = TRUE     ) ng20_umap_50$nn$hellinger |> k_occur() |> max() / 50 [1] 21.12 neighbor_overlap(ng20_umap_50$nn$hellinger, tfidfl1_hell_bf_50) [1] 0.9553753 ggplot(   data.frame(prcomp(ng20_umap_50$embedding)$x, Newsgroup = text_unsplit$Newsgroup),   aes(x = PC1, y = PC2, color = Newsgroup) ) +   geom_point(alpha = 0.4, size = 0.5) +   scale_color_manual(values = palette) +   theme_minimal() +   labs(     title = \"Sparse Hellinger Approximate 50-Neighbors 20Newsgroups UMAP\",     x = \"\",     y = \"\",     color = \"Newsgroup\"   ) +   theme(legend.position = \"right\") +   guides(color = guide_legend(override.aes = list(size = 5, alpha = 1))) ng20_umap_exact_50 <-     umap2(         X = NULL,         nn_method = tfidfl1_hell_bf_50,         n_epochs = 1000,         batch = TRUE,         dens_scale = 0.5,         verbose = TRUE     )  ggplot(     data.frame(prcomp(ng20_umap_exact_50)$x, Newsgroup = text_unsplit$Newsgroup),     aes(x = PC1, y = PC2, color = Newsgroup) ) +     geom_point(alpha = 0.4, size = 0.5) +     scale_color_manual(values = palette) +     theme_minimal() +     labs(         title = \"Sparse Hellinger Exact 50-Neighbors 20Newsgroups UMAP\",         x = \"\",         y = \"\",         color = \"Newsgroup\"     ) +     theme(legend.position = \"right\") +     guides(color = guide_legend(override.aes = list(size = 5, alpha = 1)))"},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"umap-with-even-more-neighbors","dir":"Articles","previous_headings":"","what":"UMAP with Even More Neighbors","title":"Working with Sparse Data","text":"next step increase n_neighbors even see results stable. maximum usually consider n_neighbors = 150: sort number used methods like t-SNE fairly large perplexity 50. Fortunately, testing results value n_neighbors actually stable don’t need go higher time. get something visually resembles original n_neighbors = 15 result, strategy run UMAP progressively smaller values n_neighbors using output previous run initialization. isn’t time consuming need nearest neighbor search optimizations final step can use faster t-UMAP gradient, used umap2 detects = 1 b = 1 passed. following uses n_neighbors = 150, n_neighbors = 50 finished n_neighbors = 15: strategy can work well can find suitable starting value n_neighbors isn’t onerous. Bear mind quite apart computational cost finding large number neighbors consequent increase number edges need optimizing, “global” setting always better short-circuiting manifold structure data. seems worked case, though, repeated runs give layout much stable original n_neighbors = 15 case.","code":"ng20_umap_150 <- umap2(   X = tfidf_sp,   nn_method = \"nndescent\",   metric = \"hellinger\",   n_epochs = 1000,   batch = TRUE,   verbose = TRUE,   n_neighbors = 150,   a = 1,   b = 1,   ret_nn = TRUE )  ng20_umap_150_50 <- umap2(   X = tfidf_sp,   nn_method = list(     idx = ng20_umap_150$nn$hellinger$idx[, 1:50],     dist = ng20_umap_150$nn$hellinger$dist[, 1:50]   ),   n_epochs = 1000,   batch = TRUE,   verbose = TRUE,   a = 1,   b = 1,   init = ng20_umap_150$embedding )  ng20_umap_150_50_15 <- umap2(   X = tfidf_sp,   nn_method = list(     idx = ng20_umap_150$nn$hellinger$idx[, 1:15],     dist = ng20_umap_150$nn$hellinger$dist[, 1:15]   ),   n_epochs = 1000,   batch = TRUE,   verbose = TRUE,   dens_scale = 0.5,   init = ng20_umap_150_50 ) ggplot(     data.frame(prcomp(ng20_umap_150_50_15)$x, Newsgroup = text_unsplit$Newsgroup),     aes(x = PC1, y = PC2, color = Newsgroup) ) +     geom_point(alpha = 0.4, size = 0.5) +     scale_color_manual(values = palette) +     theme_minimal() +     labs(         title = \"Sparse Hellinger Exact 150-to-15 20Newsgroups UMAP\",         x = \"\",         y = \"\",         color = \"Newsgroup\"     ) +     theme(legend.position = \"right\") +     guides(color = guide_legend(override.aes = list(size = 5, alpha = 1)))"},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Working with Sparse Data","text":"install rnndescent use umap2 function, ’s possible embed sparse high-dimensional datasets using UMAP can even use relatively exotic metrics like Hellinger divergence. aware high-dimensional datasets can contain hubs. low values n_neighbors, may get high nearest neighbor accuracies used . rnndescent functions can help detect hubness. Increasing n_neighbors can help, still wary -interpreting relative positions clusters output. Yes, know everyone says bears repeating temptation always . recommend re-running UMAP results multiple times given settings, even exact nearest neighbors, also repeat different values n_neighbors (e.g. 50 /150). “Annealing” n_neighbors value series optimizations can work well.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"UMAP Examples","text":"December 26 2024: Images (mainly) reworked make better use space tables aligned consistently allowing easier comparison UMAP t-SNE differ. t-SNE output also improved. also use default settings. Plus datasets. December 29 2018 New, better settings t-SNE, better plots couple new datasets. Removed neighborhood preservation values ’ve double checked working correctly. examples output uwot’s implementation UMAP, compared t-SNE output. see, UMAP’s output results compact, separated clusters compared t-SNE.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"data-preparation","dir":"Articles","previous_headings":"Introduction","what":"Data preparation","title":"UMAP Examples","text":"details datasets, follow links. Somewhat detail also given smallvis documentation. iris already using R. s1k part sneer package. frey, oli, mnist, fashion, kuzushiji, norb cifar10 can downloaded via snedata. coil20 coil100 can fetched via coil20. time generated document (late December 2018), kuzushiji dataset duplicate -black images needed filtering. seems remedied early February 2019. re-ran UMAP t-SNE fixed dataset, results weren’t noticeably different. record, clean-routines ran :","code":"mnist <- snedata::download_mnist()  # For some functions we need to strip out non-numeric columns and convert data to matrix x2m <- function(X) {   if (!methods::is(X, \"matrix\")) {     m <- as.matrix(X[, which(vapply(X, is.numeric, logical(1)))])   }   else {     m <- X   }   m } # Remove all-black images in Kuzushiji MNIST (https://github.com/rois-codh/kmnist/issues/1) kuzushiji <- kuzushiji[-which(apply(x2m(kuzushiji), 1, sum) == 0), ] # Remove duplicate images (https://github.com/rois-codh/kmnist/issues/5) kuzushiji <- kuzushiji[-which(duplicated(x2m(kuzushiji))), ]"},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"settings","dir":"Articles","previous_headings":"Introduction","what":"Settings","title":"UMAP Examples","text":"UMAP used default settings umap2 function one exception: pre-calculated exact nearest neighbors datasets hand, used rather approximate nearest neighbors. umap2 see umap2 article. t-SNE, used Python package openTSNE (version 1.0.2) default settings.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"visualization","dir":"Articles","previous_headings":"Introduction > Settings","what":"Visualization","title":"UMAP Examples","text":"visualization, used vizier package.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"iris","dir":"Articles","previous_headings":"Introduction","what":"iris","title":"UMAP Examples","text":"standard iris dataset, known loved .","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"s1k","dir":"Articles","previous_headings":"Introduction","what":"s1k","title":"UMAP Examples","text":"9-dimensional fuzzy simplex, created testing t-SNE related methods, original sneer package.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"oli-or-olivetti","dir":"Articles","previous_headings":"Introduction","what":"oli (or olivetti)","title":"UMAP Examples","text":"ORL database faces.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"frey","dir":"Articles","previous_headings":"Introduction","what":"frey","title":"UMAP Examples","text":"Images Brendan Frey’s face, far know originating page belonging Saul Roweis.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"isofaces","dir":"Articles","previous_headings":"Introduction","what":"isofaces","title":"UMAP Examples","text":"Yet faces, time dataset used Isomap, consisting images face different rotations lighting conditions. Unfortunately, ’s longer available MIT website, can found via Wayback Machine. wrote gist processing data R. images points colored left--right pose angle.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"coil20","dir":"Articles","previous_headings":"Introduction","what":"coil20","title":"UMAP Examples","text":"COIL-20 Columbia Object Image Library.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"coil100","dir":"Articles","previous_headings":"Introduction","what":"coil100","title":"UMAP Examples","text":"COIL-100 Columbia Object Image Library.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"swiss-roll-isoswiss","dir":"Articles","previous_headings":"Introduction","what":"swiss roll (isoswiss)","title":"UMAP Examples","text":"Swiss Roll data used Isomap. famous dataset, perhaps representative typical real world datasets. t-SNE known handle well, UMAP makes impressive go unfolding .","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"mnist","dir":"Articles","previous_headings":"Introduction","what":"mnist","title":"UMAP Examples","text":"MNIST database handwritten digits.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"fashion","dir":"Articles","previous_headings":"Introduction","what":"fashion","title":"UMAP Examples","text":"Fashion MNIST database, images fashion objects.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"kuzushiji-kmnist","dir":"Articles","previous_headings":"Introduction","what":"kuzushiji (KMNIST)","title":"UMAP Examples","text":"Kuzushiji MNIST database, images cursive Japanese handwriting.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"norb","dir":"Articles","previous_headings":"Introduction","what":"norb","title":"UMAP Examples","text":"small NORB dataset, pairs images 50 toys photographed different angles different lighting conditions.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"cifar10","dir":"Articles","previous_headings":"Introduction","what":"cifar10","title":"UMAP Examples","text":"CIFAR-10 dataset, consisting 60000 32 x 32 color images evenly divided across 10 classes (e.g. airplane, cat, truck, bird). t-SNE applied CIFAR-10 Barnes-Hut t-SNE paper, main paper, results passing convolutional neural network published. t-SNE original pixel data given supplementary information (PDF) oddly hard find link via JMLR article , less widely-cited preliminary investigation BH t-SNE. outlying purple cluster (isn’t easy see) center bottom UMAP plot. see thing Python implementation, don’t think bug uwot (although also said previous version page, turned bug uwot. current result really closer Python version now, though). cluster images automobiles seem variations image. cluster present t-SNE plot (bottom left), comfortably close rest data. existence near-duplicates CIFAR-10 doesn’t seem widely known appreciated quite recently, see instance twitter thread paper Recht co-workers. manipulations line practice data augmentation popular deep learning, need aware avoid test set results contaminated. images seem like good argument applying UMAP t-SNE dataset way spot sort thing. visualization CIFAR-10 isn’t successful UMAP t-SNE, results using activations convnet, similar used BH t-SNE paper. convnet, used keras implementation, taken Machine Learning Action blog. Features Z-scaled carried blog, used 100 epochs batch size 128 also used RMSprop (PDF) optimizer favored Deep Learning Python book (lr=1e-4 decay=1e-6). Without data augmentation, gave test set error 0.1655, slightly lower test set result given BH t-SNE paper (used different architecture without benefit extra 4 years deep learning research). retraining 60000 images, flattened output final max-pool layer used, giving 2048 features (BH t-SNE paper network 1024 output activations). UMAP t-SNE results . UMAP results, used t-UMAP settings scaled PCA initialization. Results don’t look quite good BH t-SNE paper, still improvement. orange cluster automobiles remains outlier, even activation space. can also see BH t-SNE paper lower image Figure 5 (orange cluster bottom, slightly left center).","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"tasic2018","dir":"Articles","previous_headings":"Introduction","what":"tasic2018","title":"UMAP Examples","text":"tasic2018 dataset transcriptomics dataset mouse brain cell RNA-seq data Allen Brain Atlas (originally reported Tasic co-workers. gene expression data 14,249 cells primary visual cortex, 9,573 cells anterior lateral motor cortex give dataset size n = 23,822 overall. Expression data 45,768 genes obtained original data, dataset used follows pre-processing treatment Kobak Berens applied normalization log transformation kept top 3000 variable genes. data can generated Allen Brain Atlas website processed Python following instructions Berens lab notebook. output data CSV format reading R assembling data frame following extra exporting code:","code":"np.savetxt(\"path/to/allen-visp-alm/tasic2018-log3k.csv\", logCPM, delimiter=\",\") np.savetxt(\"path/to/allen-visp-alm/tasic2018-areas.csv\", tasic2018.areas, delimiter=\",\", fmt = \"%d\") np.savetxt(\"path/to/allen-visp-alm/tasic2018-genes.csv\", tasic2018.genes[selectedGenes], delimiter=\",\", fmt='%s') np.savetxt(\"path/to/allen-visp-alm/tasic2018-clusters.csv\", tasic2018.clusters, delimiter=\",\", fmt='%d') np.savetxt(\"path/to/allen-visp-alm/tasic2018-cluster-names.csv\", tasic2018.clusterNames, delimiter=\",\", fmt='%s') np.savetxt(\"path/to/allen-visp-alm/tasic2018-cluster-colors.csv\", tasic2018.clusterColors, delimiter=\",\", fmt='%s')"},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"macosko2015","dir":"Articles","previous_headings":"Introduction","what":"macosko2015","title":"UMAP Examples","text":"Another transcriptomics data set, used example openTSNE. contains data 44,808 cells mouse retina. raw data fetched similarly shell script Hemberg Lab data prepared using openTSNE notebook Pavlin Policar. Similarly tasic2018 dataset, data log normalized 3,000 variable genes retained. exported data (without Z-scaling PCA dimensionality reduction), CSV file, e.g.: Results impressive looking (especially UMAP). include example ’s common scRNA-seq packages apply PCA gene expression data UMAP. case, reduced dimensionality 100. transformation, results look different (much better separation clusters): good news results look better nearest neighbor search step t-SNE UMAP go faster, often carried . bad news now justify biological basis PCA pre-processing, clearly changed relationship different measurements. 100 dimensions? datasets ’ve seen PCA pre-processing doesn’t noticeable effect, like dataset.","code":"np.savetxt(\"/path/to/macosko2015/macosko2015-log3k.csv\", x, delimiter=\",\") # Use these as column names np.savetxt(\"/path/to/macosko2015/macosko2015-genenames.csv\", data.T.columns.values[gene_mask].astype(str), delimiter=\",\", fmt = \"%s\") np.savetxt(\"/path/to/macosko2015/macosko2015-clusterids.csv\", cluster_ids.values.astype(int), delimiter=\",\", fmt = \"%d\")"},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"s-curve-with-a-hole-scurvehole","dir":"Articles","previous_headings":"Introduction","what":"S-curve With a Hole (scurvehole)","title":"UMAP Examples","text":"Used dimensionality reduction method PaCMAP. generated script https://github.com/YingfanWang/PaCMAP/blob/master/experiments/run_experiments.py. script mentioned ’s S-curve hole middle, amenable unfolded 2D. Similar isowiss.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"mammoth","dir":"Articles","previous_headings":"Introduction","what":"mammoth","title":"UMAP Examples","text":"3D point cloud mammoth Smithsonian. 50,000 point sample used used Understanding UMAP available download github repo. also 10,000 point sample available repo, use 50,000 point version. far can tell, first used dimensionality reduction Max Noichl https://github.com/MNoichl/UMAP-examples-mammoth-/blob/9e82eb3ee5b99020d74e99d2060856d49e8b9f85/umammoth.ipynb. Following example notebook, colors dataset generated assigning 12 clusters 3D data via Agglomerative Clustering method sklearn. mammoth highly associated (least mind) PaCMAP method. [PaCMAP paper](paper makes extensive use .","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"hierarchical","dir":"Articles","previous_headings":"Introduction","what":"hierarchical","title":"UMAP Examples","text":"Another dataset used PaCMAP paper, unable find code generate PaCMAP repo. paper describes thusly: ’consists 5 “macro clusters” 5 “meso clusters,” meso subclusters 5 “micro clusters.” Thus, 5x5x5 = 125 micro clusters total. colored point based true macro clusters, shading meso cluster.”. Get ? ’s detail Appendix B paper think able turn Python probably generated correct data. think ’s correct. find somewhere upload code eventually.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"spheres","dir":"Articles","previous_headings":"Introduction","what":"spheres","title":"UMAP Examples","text":"dataset topological autoencoders paper. Another synthetic hierarchical dataset, 10 high-dimensional spheres embedded larger sphere. dimensionality reduction methods focus local measures like nearest neighbors trouble sorts datasets, representing global structure scale challenging.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"newsgroups-ng20","dir":"Articles","previous_headings":"Introduction","what":"20 Newsgroups (ng20)","title":"UMAP Examples","text":"20 Newsgroups dataset, containing sample Usenet posts (ask parents maybe grandparents point) 20 different newsgroups. details see Sparse Data Article. refer ng20 lot due unable begin variables number. many dimensionality reduction methods unable handle sparse data, also sometimes use dense version dataset, ng20pacmap can found numpy format PaCMAP repo https://github.com/YingfanWang/PaCMAP/tree/master/data 20NG.npy (.e. numpy format). Labels also available 20NG_labels.npy guess say initial data subjected TF-IDF followed PCA 100 dimensions convert sparse dense format, don’t know sure, know pre-processing normalization carried . image ng20pacmap.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-for-tsne.html","id":"asymmetric-sne","dir":"Articles","previous_headings":"","what":"(Asymmetric) SNE","title":"UMAP for t-SNE","text":"Given NN observations high dimensional data, pair, 𝐱𝐢\\mathbf{x_{}} 𝐱𝐣\\mathbf{x_{j}}, SNE defines similarity (aka affinity weight) , vj|iv_{j|}, using Gaussian kernel function: vj|=exp(−βirij2) v_{j|} = \\exp(-\\beta_{} r_{ij}^2) rijr_{ij} distance 𝐱𝐢\\mathbf{x_{}} 𝐱𝐣\\mathbf{x_{j}} βi\\beta_{} must determined method (’ll get back ). notation vj|iv_{j|} rather vijv_{ij}, indicate quantity symmetric, .e. vj|≠vi|jv_{j|} \\neq v_{|j}. ’ve borrowed notation conditional versus joint probability definitions used symmetric SNE (see ) ’ll also need quantities probabilities. rijr_{ij} notation indicates distances symmetric convention used symmetric values. weights normalized form NN probability distributions: pj|=vj|∑kNvk|p_{j|} = \\frac{v_{j|}}{\\sum_{k}^{N} v_{k|}} βi\\beta_{} chosen finding value results probability distribution specific perplexity. perplexity chosen user, interpreted continuous version number nearest neighbors, generally chosen take values 5 50. pj|ip_{j|} conditional probability, interpreted meaning “probability pick item jj similar item ii, given ’ve already picked ii”. output space embedded coordinates, similarity points 𝐲𝐢\\mathbf{y_i} 𝐲𝐣\\mathbf{y_j} also defined Gaussian: wij=exp(−dij2) w_{ij} = \\exp(-d_{ij}^2) dijd_{ij} Euclidean distance 𝐲𝐢\\mathbf{y_i} 𝐲𝐣\\mathbf{y_j}. β\\beta weight definition weights symmetric. output probabilities, qj|iq_{j|} calculated wijw_{ij} way go vj|iv_{j|} pj|ip_{j|}, creating NN probability distributions. Due normalizing rows, qj|iq_{j|} asymmetric despite symmetric weights generated . SNE cost function sum Kullback-Leibler divergences NN distributions: CSNE=∑∑jNpj|ilogpj|iqj|C_{SNE} = \\sum_{}^{N} \\sum_{j}^{N} p_{j|} \\log \\frac{p_{j|}}{q_{j|}} (follows), weights probabilities =ji = j defined. don’t want clutter notation , assume excluded sums.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-for-tsne.html","id":"symmetric-sne","dir":"Articles","previous_headings":"","what":"Symmetric SNE","title":"UMAP for t-SNE","text":"Symmetric SNE, input probability matrix symmetrized averaging pj|ip_{j|} pi|jp_{|j} re-normalized pairs points, create single (joint) probability distribution, pijp_{ij}: pij=pj|+pi|j2N p_{ij} = \\frac{p_{j|} + p_{|j}}{2N} output probabilities, qijq_{ij} now defined normalizing output weights pairs, creating single probability distribution: qij=wij∑kN∑lNwkl q_{ij} = \\frac{w_{ij}}{\\sum_{k}^N \\sum_{l}^N w_{kl}} cost function SSNE : CSSNE=∑∑jNpijlogpijqij C_{SSNE} = \\sum_{}^{N} \\sum_{j}^{N} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-for-tsne.html","id":"t-sne","dir":"Articles","previous_headings":"","what":"t-SNE","title":"UMAP for t-SNE","text":"purposes discussion, t-SNE differs symmetric SNE weight function: wij=11+dij2 w_{ij} = \\frac{1}{1 + d_{ij}^2}","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-for-tsne.html","id":"sne-optimization","dir":"Articles","previous_headings":"","what":"SNE Optimization","title":"UMAP for t-SNE","text":"Optimization t-SNE proceeds : Calibrate input probabilities, pijp_{ij} according desired perplexity (needs done ). Iteratively: Calculate pairwise distances, dijd_{ij} 𝐲𝐢\\mathbf{y_{}} 𝐲𝐣\\mathbf{y_{j}} Calculate weights, wijw_{ij} Calculate output probabilities qijq_{ij} Use gradient descent update 𝐲𝐢\\mathbf{y_{}} 𝐲𝐣\\mathbf{y_{j}} fundamentally O(N2)O(N^2) need calculate pairwise distances: t-SNE gradient requires qijq_{ij} calculated normalization step converts wijw_{ij} qijq_{ij} requires wijw_{ij} calculated also need distances. Approaches like Barnes-Hut t-SNE others (e.g. Flt-SNE) attempt improve taking advantage t-SNE gradient: attractive part gradient depends pijp_{ij}, constant large neighbors close input space. Therefore ’s necessary calculate attractive gradient nearest neighbors 𝐱𝐢\\mathbf{x_i}. Barnes-Hut t-SNE, number nearest neighbors used three times whatever perplexity . larger datasets, perplexity 50 common, usually looking 150-nearest neighbors point. repulsive part gradient dependent qijq_{ij} changes iteration, improvements focus grouping together points distant output space treating single point purposes gradient calculation. able tell perusing publications linked , approaches increasing sophistication complexity.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-for-tsne.html","id":"largevis","dir":"Articles","previous_headings":"","what":"LargeVis","title":"UMAP for t-SNE","text":"LargeVis takes different approach: re-uses lot definitions t-SNE, makes sufficient modifications ’s possible use stochastic gradient descent. Also, rather talk probabilities, LargeVis uses language graph theory. observation dataset now considered vertex node similarity weight edge two vertices. Conceptually ’re still talking elements matrix, start slipping language “edges” “vertices”. key change cost function, now maximum likelihood function: LLV=∑(,j)∈Epijlogwij+γ∑(,j)∈E‾log(1−wij) L_{LV} = \\sum_{ \\left(, j\\right) \\E} p_{ij} \\log w_{ij}  +\\gamma \\sum_{\\left(, j\\right) \\\\bar{E}} \\log \\left(1 - w_{ij} \\right) pijp_{ij} wijw_{ij} t-SNE (authors try alternative wijw_{ij} definitions, aren’t effective). new concepts γ\\gamma EE. γ\\gamma user-defined positive scalar weight repulsive versus attractive forces. default reference implementation 7. EE set edges non-zero weight. graph theory way talk nearest neighbors input space. Just Barnes-Hut t-SNE, find set nearest neighbors point 𝐱𝐢\\mathbf{x_i} define input weights probabilities pairs points nearest neighbors. official Barnes-Hut t-SNE implementation, LargeVis reference implementation uses default perplexity 50, default number nearest neighbors 3 times perplexity. cost function therefore consists two disjoint contributions: nearest neighbors input space contribute attractive part cost function (first part). Everything else contributes second, repulsive part. key advantage cost function KL divergence doesn’t contain qijq_{ij}. output normalization, don’t need calculate output pairwise distances. cost function amenable stochastic gradient descent techniques.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-for-tsne.html","id":"the-largevis-sampling-strategy","dir":"Articles","previous_headings":"","what":"The LargeVis sampling strategy","title":"UMAP for t-SNE","text":"calculate stochastic gradient, LargeVis following: Samples edge EE, .e. chooses ii jj pij≠0p_{ij} \\neq 0. called “positive edge” LargeVis paper. ii jj used calculate attractive part gradient. Samples one NN vertices, let’s call kk. datasets grow larger, probability kk EE grows smaller, ’s actually checked (k≠ik \\neq ). used calculate repulsive part gradient. called “negative samples” LargeVis paper. Repeat negative sample step number times. default LargeVis sample 5 negatives positive edge. coordinates ii, jj various kk updated according gradients. concludes one iteration SGD. attractive repulsive gradients LargeVis respectively: ∂LLV∂𝐲𝐢+=−21+dij2pij(𝐲𝐢−𝐲𝐣) \\frac{\\partial L_{LV}}{\\partial \\mathbf{y_i}}^+ = \\frac{-2}{1 + d_{ij}^2}p_{ij} \\left(\\mathbf{y_i - y_j}\\right) ∂LLV∂𝐲𝐢−=2γ(0.1+dij2)(1+dij2)(𝐲𝐢−𝐲𝐣) \\frac{\\partial L_{LV}}{\\partial \\mathbf{y_i}}^- = \\frac{2\\gamma}{\\left(0.1 + d_{ij}^2\\right)\\left(1 + d_{ij}^2\\right)} \\left(\\mathbf{y_i - y_j}\\right) value 0.1 appears repulsive gradient prevent division zero. Sampling edges vertices uniform. attractive gradient, authors note factor pijp_{ij} appears means magnitude gradient can differ hugely samples extent choosing appropriate learning rate can difficult. Instead sample edges proportionally pijp_{ij} gradient calculation, treat edge weights equal. attractive gradient used part LargeVis SGD therefore: ∂LLV∂𝐲𝐢+=−21+dij2(𝐲𝐢−𝐲𝐣) \\frac{\\partial L_{LV}}{\\partial \\mathbf{y_i}}^+ = \\frac{-2}{1 + d_{ij}^2} \\left(\\mathbf{y_i - y_j}\\right) pijp_{ij} doesn’t appear repulsive part gradient, seem uniform sampling work negative sampling. However, vertices sampled using “noisy” distribution proportional degree ^ 0.75, degree vertex sum weights edges incident . doesn’t seem theoretical reason use degree ^ 0.75. ’s based results field word embeddings: LargeVis authors reference skip-gram paper, power also shows GloVE. cases justified purely empirically. uwot version LargeVis (lvish) samples negative edges uniformly, doesn’t seem cause problems.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-for-tsne.html","id":"umap-at-last","dir":"Articles","previous_headings":"","what":"UMAP (at last)","title":"UMAP for t-SNE","text":"UMAP cost function cross-entropy two fuzzy sets, can represented symmetric weight matrices: CUMAP=∑ij[vijlog(vijwij)+(1−vij)log(1−vij1−wij)] C_{UMAP} =  \\sum_{ij} \\left[ v_{ij} \\log \\left( \\frac{v_{ij}}{w_{ij}} \\right) +  (1 - v_{ij}) \\log \\left( \\frac{1 - v_{ij}}{1 - w_{ij}} \\right) \\right] vijv_{ij} symmetrized input affinities, probabilities. graph interpretation weights edges graph still applies, though. arrived differently t-SNE LargeVis. unsymmetrized UMAP input weights given : vj|=exp[−(rij−ρi)/σi] v_{j|} = \\exp \\left[ -\\left( r_{ij} - \\rho_{} \\right) / \\sigma_{} \\right] rijr_{ij} input distances, ρi\\rho_{} distance nearest neighbor (ignoring zero distances neighbors duplicates) σi\\sigma_{} analogous βi\\beta_{} perplexity calibration used SNE. case, σi\\sigma_{} determined ∑jvj|=log2k\\sum_{j} v_{j|} = \\log_{2} k kk number nearest neighbors. January 1 2020: assume connection local scaling advocated self-tuning spectral clustering, given spectral decomposition affinity graph default initialization method UMAP. weights symmetrized slightly different method SNE: vij=(vj|+vi|j)−vj|ivi|j v_{ij} = \\left(v_{j|} + v_{|j}\\right) - v_{j|}v_{|j} matrix operation: Vsymm=V+VT−V∘VT V_{symm} = V + V^{T} - V \\circ V^{T} TT indicates transpose ∘\\circ Hadamard (.e. entry-wise) product. effectively carries fuzzy set union. output weights given : wij=1/(1+adij2b) w_{ij} = 1 / \\left(1 + ad_{ij}^{2b}\\right) aa bb determined non-linear least squares fit based min_dist spread parameters control tightness squashing function. setting =1a = 1 b=1b = 1 get t-SNE weighting back. current UMAP defaults result = 1.929 b = 0.7915. April 7 2019: Actually, got wrong. UMAP defaults use min_dist = 0.1, spread = 1, results =1.577a = 1.577 b=0.8951b = 0.8951. use min_dist = 0.001, spread = 1 get result =1.929a = 1.929 b=0.7915b = 0.7915. attractive repulsive UMAP gradient expressions , respectively: ∂CUMAP∂𝐲𝐢+=−2abdij2(b−1)1+adij2bvij(𝐲𝐢−𝐲𝐣) \\frac{\\partial C_{UMAP}}{\\partial \\mathbf{y_i}}^+ =  \\frac{-2abd_{ij}^{2\\left(b - 1\\right)}}{1 + ad_{ij}^{2b}}  v_{ij} \\left(\\mathbf{y_i - y_j}\\right) ∂CUMAP∂𝐲𝐢−=2b(0.001+dij2)(1+adij2b)(1−vij)(𝐲𝐢−𝐲𝐣) \\frac{\\partial C_{UMAP}}{\\partial \\mathbf{y_i}}^- =  \\frac{2b}{\\left(0.001 + d_{ij}^2\\right)\\left(1 + ad_{ij}^{2b}\\right)}\\left(1 - v_{ij}\\right)\\left(\\mathbf{y_i - y_j}\\right) (April 13 2020: previous versions document completely messed expression omitting factor 2 repulsive gradient equation missed important aas bbs. also affected SGD version two equations . Thank Dmitry Kobak spotting .) complex-looking LargeVis gradient, obvious similarities, become clearer set =1a=1 b=1b=1, get back t-SNE/LargeVis output weight function. 0.001 term denominator repulsive gradient plays role 0.1 LargeVis gradient (preventing division zero). UMAP uses sampling strategy LargeVis, sampling positive edges proportional weight edge (case vijv_{ij}), value gradient calculated assuming vij=1v_{ij} = 1 edges. SGD purposes, attractive gradient UMAP : ∂CUMAP∂𝐲𝐢+=−2abdij2(b−1)1+adij2b(𝐲𝐢−𝐲𝐣)=−2abdij2bdij2(1+adij2b)(𝐲𝐢−𝐲𝐣) \\frac{\\partial C_{UMAP}}{\\partial \\mathbf{y_i}}^+ =  \\frac{-2abd_{ij}^{2\\left(b - 1\\right)}}{1 + ad_{ij}^{2b}}\\left(\\mathbf{y_i - y_j}\\right) = \\frac{-2abd_{ij}^{2b}}{d_{ij}^2 \\left(1 + ad_{ij}^{2b}\\right)}\\left(\\mathbf{y_i - y_j}\\right) final expression might computationally convenient saves extra power calculation. repulsive part gradient contains 1−vij1 - v_{ij} term, vij=0v_{ij} = 0 pairs edges, term effectively disappears, leaving: ∂CUMAP∂𝐲𝐢−=2b(0.001+dij2)(1+adij2b)(𝐲𝐢−𝐲𝐣) \\frac{\\partial C_{UMAP}}{\\partial \\mathbf{y_i}}^- =  \\frac{2b}{\\left(0.001 + d_{ij}^2\\right)\\left(1 + ad_{ij}^{2b}\\right)}   \\left(\\mathbf{y_i - y_j}\\right) Unlike LargeVis, negative sampling UMAP uses uniform distribution. ’s worth considering , although LargeVis uses pijp_{ij} UMAP uses vijv_{ij} cost functions, difference isn’t important, sampling proportionally pijp_{ij} exactly sampling proportionally vijv_{ij}. fact, look LargeVis reference implementation (lvish uwot), input affinities symmetrized, divided NN. Nonetheless, affinities used construct sampling probabilities, presence γ\\gamma parameter repulsive part gradient means effectively using pijp_{ij} LargeVis cost function optimized, vijv_{ij}.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-for-tsne.html","id":"minor-umap-variations","dir":"Articles","previous_headings":"","what":"Minor UMAP Variations","title":"UMAP for t-SNE","text":"extra parameters UMAP make minor changes modified default-values: local_connectivity affects ρi\\rho_{} using distance local_connectivityth non-zero near neighbor (interpolating distances local_connectivity non-integral). set_op_mix_ratio changes form symmetrization fuzzy set union fuzzy set intersection just vj|ivi|jv_{j|}v_{|j}, can also blend two. gamma works exactly like value LargeVis, -weighting repulsive contribution gradient. 2 August 2018: follow parameter longer appears reference UMAP implementation: bandwidth affects vj|iv_{j|} multiplying value σi\\sigma_{}: vj|=exp[−(rij−ρi)/βσi] v_{j|} = \\exp \\left[ -\\left( r_{ij} - \\rho_{} \\right) / \\beta \\sigma_{} \\right] bandwidth represented β\\beta. value σi\\sigma_{} determined using calculations vj|iv_{j|} without β\\beta, recalculating vj|iv_{j|} using bandwidth. ’m sure useful changed defaults. thanks Dmitry Kobak helpful discussions typo-spotting.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-simple.html","id":"grid","dir":"Articles","previous_headings":"","what":"Grid","title":"UMAP on some simple datasets","text":"2D grid regularly spaced points.  Like t-SNE, UMAP tends expand denser regions data, bigger gap points middle grid.","code":"grid2d <- snedata::grid_data(n = 20)"},{"path":"https://jlmelville.github.io/uwot/articles/umap-simple.html","id":"clusters","dir":"Articles","previous_headings":"","what":"2 Clusters","title":"UMAP on some simple datasets","text":"Two 2D Gaussian clusters equal variance, 50 points . Setting n_neighbors low clearly gives results local.","code":"gauss2d <- snedata::two_clusters_data(n = 50, dim = 2)"},{"path":"https://jlmelville.github.io/uwot/articles/umap-simple.html","id":"cluster-densities","dir":"Articles","previous_headings":"","what":"Cluster Densities","title":"UMAP on some simple datasets","text":"example, one clusters (yellow one) much denser (hence smaller) . like t-SNE, UMAP reproduce relative cluster densities.","code":"gauss2d_scale <- snedata::two_different_clusters_data(n = 75, scale = 10, dim = 2)"},{"path":"https://jlmelville.github.io/uwot/articles/umap-simple.html","id":"cluster-size","dir":"Articles","previous_headings":"","what":"Cluster Size","title":"UMAP on some simple datasets","text":"aside, two clusters density different numbers points? example two clusters equal sizes (100 points ), orange cluster contains 1000 points: example can see UMAP display clusters members larger. can implications visualization minority class interested .","code":"x100a <- snedata::gaussian_data(n = 100, dim = 50, color = \"blue\") x1000b <- snedata::gaussian_data(n = 1000, dim = 50, color = \"orange\") x1000b[, 1:50] <- x1000b[, 1:50] <- x1000b[, 1:50] + 10 x200 <- rbind(x100a, x100b) x1100 <- rbind(x100a, x1000b)"},{"path":"https://jlmelville.github.io/uwot/articles/umap-simple.html","id":"distances-between-clusters","dir":"Articles","previous_headings":"","what":"Distances Between Clusters","title":"UMAP on some simple datasets","text":"example, back gaussians variances, now one (green one) much away two. ’s really value n_neighbors correct relative distances reproduced. hand, least don’t see strange distortion size green cluster high values n_neighbors, t-SNE results start showing distortions high perplexity. repeat larger number points cluster: Results consistent sensible value n_neighbors, ’s clear UMAP reproduce relative distances case.","code":"gauss_3clusters <- snedata::three_clusters_data(n = 50, dim = 2) gauss_3clusters200 <- snedata::three_clusters_data(n = 200, dim = 2)"},{"path":"https://jlmelville.github.io/uwot/articles/umap-simple.html","id":"random-noise","dir":"Articles","previous_headings":"","what":"Random Noise","title":"UMAP on some simple datasets","text":"single high-dimensional Guassian: , see t-SNE-like behavior: density points projection even linear projection provided PCA. ’s also clear low values n_neighbors mislead seeing large numbers small clusters aren’t really .","code":"gauss100d <- snedata::gaussian_data(n = 500, dim = 100, color = \"#003399\")"},{"path":"https://jlmelville.github.io/uwot/articles/umap-simple.html","id":"elongated-shapes","dir":"Articles","previous_headings":"","what":"Elongated Shapes","title":"UMAP on some simple datasets","text":"ellipsoidal cluster: , UMAP behaves pretty well long n_neighbors sufficiently high. Now, two ellipsoidal clusters: density distortion effect also apparent , causing clusters curve.","code":"gauss_long <- snedata::long_gaussian_data(n = 100, dim = 50, color = \"#003399\") gauss_2long <- snedata::long_cluster_data(n = 75)"},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/umap-simple.html","id":"containment","dir":"Articles","previous_headings":"Topology","what":"Containment","title":"UMAP on some simple datasets","text":"dataset, two 50D gaussian clusters, centered location, PCA plot top left row shows, blue cluster much smaller variance “contained” inside yellow cluster. last difference t-SNE results. t-SNE, containment relationship can displayed suitable choice perplexity, cost yellow cluster gaining ring-like shape. UMAP, however, stubbornly refuses show anything sort, blue cluster expanded overlap yellow cluster even higher values n_neighbors.","code":"subset50d <- snedata::subset_clusters_data(n = 75, dim = 50)"},{"path":"https://jlmelville.github.io/uwot/articles/umap-simple.html","id":"linked-rings","dir":"Articles","previous_headings":"Topology","what":"Linked Rings","title":"UMAP on some simple datasets","text":"2D linked rings, embedding 3D (one right angles ). t-SNE results show rings separate low perplexity, linked high perplexities. UMAP results always unlinked except low value n_neighbors, even seems artifact number epochs random seed. set n_epochs higher, rings invariably unlinked.","code":"linked_rings <- snedata::link_data(n = 100)"},{"path":"https://jlmelville.github.io/uwot/articles/umap-simple.html","id":"trefoil-knot","dir":"Articles","previous_headings":"Topology","what":"Trefoil Knot","title":"UMAP on some simple datasets","text":"Results quite similar t-SNE results. low values n_neighbors, knot unfolded circle, higher values, folded form appears. linked rings, results get low values n_neighbors consistent higher values n_epochs. make ? Mainly, UMAP results bit consistent t-SNE, sense changing n_neighbors doesn’t lead different results way changing perplexity t-SNE, although effects mainly restricted three cluster containment example. may see advantage t-SNE. Personally, bit skeptical see sort thing real world datasets. ’s also worth noting , like t-SNE: Inter-cluster distances reproduced. Relative cluster sizes reproduced. Denser regions space expanded less dense parts.","code":"trefoil <- snedata::trefoil_data(n = 150)"},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"basic-umap","dir":"Articles","previous_headings":"","what":"Basic UMAP","title":"uwot","text":"defaults umap function work datasets. scaling input data done, non-numeric columns ignored:","code":"set.seed(42) iris_umap <- umap(iris) plot_umap(iris_umap)"},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"parameters","dir":"Articles","previous_headings":"Basic UMAP","what":"Parameters","title":"uwot","text":"uwot accumulated many parameters time, time handful need worry . important ones :","code":""},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"min_dist","dir":"Articles","previous_headings":"Basic UMAP > Parameters","what":"min_dist","title":"uwot","text":"mainly aesthetic parameter, defines close points can get output space. smaller value tends make clusters output compact. experiment values 0 1, although don’t choose exactly zero. default 0.01, seems like ’s bit small iris. Let’s crank min_dist 0.3:  made clusters bigger closer together, ’ll use min_dist = 0.3 examples iris.","code":"set.seed(42) iris_umap_md05 <- umap(iris, min_dist = 0.3) plot_umap(iris_umap_md05)"},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"n_neighbors","dir":"Articles","previous_headings":"Basic UMAP > Parameters","what":"n_neighbors","title":"uwot","text":"defines number items dataset define neighborhood around point. Set low get fragmented layout. Set high get something miss lot local structure. ’s result 5 neighbors:  ’s hugely different default 15 neighbors, clusters bit broken . pronounced difference going way looking 100 neighbors:  much uniform appearance results. ’s always worth trying different values n_neighbors, especially larger values, although larger values n_neighbors lead longer run times. Sometimes small clusters think meaningful may fact artifacts setting n_neighbors small, starting larger value looking effect reducing n_neighbors can help avoid interpreting results.","code":"set.seed(42) iris_umap_nbrs5 <- umap(iris, n_neighbors = 5, min_dist = 0.3) plot_umap(iris_umap_nbrs5) set.seed(42) iris_umap_nbrs100 <- umap(iris, n_neighbors = 100, min_dist = 0.3) plot_umap(iris_umap_nbrs100)"},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"init","dir":"Articles","previous_headings":"Basic UMAP > Parameters","what":"init","title":"uwot","text":"default initialization UMAP use spectral initialization, acts upon (symmetrized) k-nearest neighbor graph determined choice n_neighbors. usually good choice, involves sparse matrix, can sometimes bit sparse, leads numerical difficulties manifest slow run times even hanging calculations. dataset causes issues, can either try increasing n_neighbors seen cases inconvenient terms CPU RAM usage. alternative use first two principal components data, least uses data provide give solid global picture data UMAP can refine. ’s appropriate every dataset, cases, ’s perfectly good alternative. gotcha depending scaling data, initial coordinates can large inter-point distances. UMAP optimize well, output scaled small standard deviation. set init = \"spca\", , although aligned UMAP coordinate initialization, recommend also set init_sdev = \"range\" well. init_sdev can also take numerical value standard deviation. Values 1e-4 10 reasonable, recommend stick default \"range\".  doesn’t big effect iris, ’s good know option: can also smooth effect changing n_neighbors initial coordinates standard spectral initialization, can make easier see effect changing n_neighbors final result. init options know : \"random\": worst comes worst, can always fall back randomly assigning initial coordinates. really want avoid can though, take longer optimize coordinates quality, need increase n_epochs compensate. Even , ’s much likely end minimum less desirable one based good initialization. make interpreting results harder, likely end different clusters beings split mixed . coordinates like another method, can pass matrix. remember probably want scale init_sdev though.","code":"set.seed(42) iris_umap_spca <-   umap(iris,     init = \"spca\",     init_sdev = \"range\",     min_dist = 0.3   ) plot_umap(iris_umap_spca)"},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"dens_scale","dir":"Articles","previous_headings":"Basic UMAP > Parameters","what":"dens_scale","title":"uwot","text":"dens_scale parameter varies 0 1 controls much relative densities input data attempted preserved output.  shrunk black cluster left plot (species setosa), reflect density setosa points less spread input data two species. dens_scale please read dedicated article.","code":"set.seed(42) iris_umapds <- umap(iris, min_dist = 0.3, dens_scale = 0.5) plot_umap(iris_umapds)"},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"embedding-new-data","dir":"Articles","previous_headings":"","what":"Embedding New Data","title":"uwot","text":"embedding, can use embed new data, although need remember ask “model” return. Instead just coordinates, now get back list contains extra parameters need transforming new data. coordinates still available $embedding component. Let’s try building UMAP just setosa versicolor iris species:  Next, can use umap_transform embed new points:  green points top-right show embedded data. Note original (black red) clusters get optimized . haven’t perfectly reproduced full UMAP, virginica points located less right place, close versicolor items. Just like machine learning method, must careful choose training set.","code":"set.seed(42)  iris_train <- iris[iris$Species %in% c(\"setosa\", \"versicolor\"), ] iris_train_umap <-   umap(iris_train, min_dist = 0.3, ret_model = TRUE) plot(   iris_train_umap$embedding,   col = iris_train$Species,   xlab = \"\",   ylab = \"\",   main = \"UMAP setosa + versicolor\" ) iris_test <- iris[iris$Species == \"virginica\", ] set.seed(42) iris_test_umap <- umap_transform(iris_test, iris_train_umap) plot(   rbind(iris_train_umap$embedding, iris_test_umap),   col = iris$Species,   xlab = \"\",   ylab = \"\",   main = \"UMAP transform virginica\" )"},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"supported-distances","dir":"Articles","previous_headings":"","what":"Supported Distances","title":"uwot","text":"small (N < 4096) Euclidean distance, exact nearest neighbors found using FNN package. Otherwise, approximate nearest neighbors found using RcppAnnoy. supported distance metrics (set metric parameter) : Euclidean Cosine Pearson Correlation (correlation) Manhattan Hamming Exactly constitutes cosine distance can differ packages. uwot tries follow Python version UMAP defines , 1 minus cosine similarity. differs slightly Annoy defines angular distance, aware uwot internally converts Annoy version distance. Also aware Pearson correlation distance cosine distance applied row-centered vectors. need metrics, can generate nearest neighbor info externally, can pass data directly uwot via nn_method parameter. Please note Hamming support lot slower metrics. recommend using hundred features, even expect take several minutes index building phase situations Euclidean metric take seconds.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"multi-threading-support","dir":"Articles","previous_headings":"","what":"Multi-threading support","title":"uwot","text":"Parallelization can used nearest neighbor index search, smooth knn/perplexity calibration, optimization, approach LargeVis takes. can () adjust number threads via n_threads, controls nearest neighbor smooth knn calibration, n_sgd_threads parameter, controls number threads used optimization. n_threads, default number available cores. n_sgd_threads default 0, ensures reproducibility results fixed seed.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"python-comparison","dir":"Articles","previous_headings":"","what":"Python Comparison","title":"uwot","text":"datasets ’ve tried , results look least reminiscent obtained using official Python implementation. results 70,000 MNIST digits (downloaded using snedata package). , result using official Python UMAP implementation (via reticulate package). result using uwot. MNIST UMAP (Python) MNIST UMAP (R) project documentation contains examples, comparison Python.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"nearest-neighbor-calculation","dir":"Articles","previous_headings":"Limitations and Other Issues","what":"Nearest Neighbor Calculation","title":"uwot","text":"uwot leans heavily Annoy library approximate nearest neighbor search. result, compared Python version UMAP, uwot much limited support different distance measurements, support sparse matrix data input. However, uwot let pass nearest neighbor data. access nearest neighbor methods, can generate data can used uwot. See Nearest Neighbor Data Format article. can calculate distance matrix data, can pass dist object. larger distance matrices, can pass sparseMatrix (Matrix package). Experience COIL-100, 49,152 features, suggests Annoy definitely struggle datasets dimensionality. Even 3000 dimensions can cause problems, although difficulty specific Annoy. Reducing dimensionality PCA intermediate dimensionality (e.g. 100) can help. Use e.g. pca = 100 . can also slow platforms without good linear algebra support assure 100 principal components won’t throwing away excessive amounts information.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"spectral-initialization","dir":"Articles","previous_headings":"Limitations and Other Issues","what":"Spectral Initialization","title":"uwot","text":"spectral initialization default umap (Laplacian Eigenmap initialization, init = \"laplacian\") can sometimes run problems. fails converge fall back random initialization, occasion ’ve seen take extremely long time (couple hours) converge. Recent changes hopefully reduced chance happening, initialization taking minutes, suggest stopping calculation using scaled PCA (init = \"spca\") instead.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"supporting-libraries","dir":"Articles","previous_headings":"","what":"Supporting Libraries","title":"uwot","text":"credit following packages lot hard work: Coordinate initialization uses RSpectra eigendecomposition normalized Laplacian. optional PCA initialization initial dimensionality reduction uses irlba. smooth k-nearest neighbor distance stochastic gradient descent optimization routines written C++ (using Rcpp, aping Python code closely possible. multi-threading code based RcppParallel.","code":""},{"path":"https://jlmelville.github.io/uwot/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"James Melville. Author, maintainer, copyright holder. Aaron Lun. Contributor. Mohamed Nadhir Djekidel. Contributor. Yuhan Hao. Contributor. Dirk Eddelbuettel. Contributor. Wouter van der Bijl. Contributor.","code":""},{"path":"https://jlmelville.github.io/uwot/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Melville J (2024). uwot: Uniform Manifold Approximation Projection (UMAP) Method Dimensionality Reduction. R package version 0.2.2.9000, https://jlmelville.github.io/uwot/, https://github.com/jlmelville/uwot.","code":"@Manual{,   title = {uwot: The Uniform Manifold Approximation and Projection (UMAP) Method for Dimensionality Reduction},   author = {James Melville},   year = {2024},   note = {R package version 0.2.2.9000,     https://jlmelville.github.io/uwot/},   url = {https://github.com/jlmelville/uwot}, }"},{"path":"https://jlmelville.github.io/uwot/index.html","id":"uwot","dir":"","previous_headings":"","what":"The Uniform Manifold Approximation and Projection (UMAP) Method for Dimensionality Reduction","title":"The Uniform Manifold Approximation and Projection (UMAP) Method for Dimensionality Reduction","text":"R implementation Uniform Manifold Approximation Projection (UMAP) method dimensionality reduction McInnes et al. (2018). Also included supervised metric (--sample) learning extensions basic method. Translated Python implementation.","code":""},{"path":"https://jlmelville.github.io/uwot/index.html","id":"news","dir":"","previous_headings":"","what":"News","title":"The Uniform Manifold Approximation and Projection (UMAP) Method for Dimensionality Reduction","text":"April 21 2024 ordained prophecy, version 0.2.2 uwot released CRAN. RSpectra back main dependency thought worked clever scheme avoid failures seen installations irlba/Matrix interactions. releases fixes problem systems access (including GitHub Actions CI) CRAN checks remain failing. embarrassing. said, issues, ’s possible new release help . April 18 2024 Version 0.2.1 uwot released CRAN. features aware : RcppHNSW rnndescent now supported optional dependencies. install load , can use alternative RcppAnnoy nearest neighbor search faster. Also, new umap2 function added, updated defaults compared umap. Please see updated new articles HNSW, rnndescent, working sparse data umap2. consider worthy moving 0.1.x 0.2.x, interests full disclosure, -going irlba problems caused CRAN check failure, might onto 0.2.2 sooner ’d like.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/index.html","id":"from-cran","dir":"","previous_headings":"Installing","what":"From CRAN","title":"The Uniform Manifold Approximation and Projection (UMAP) Method for Dimensionality Reduction","text":"","code":"install.packages(\"uwot\")"},{"path":"https://jlmelville.github.io/uwot/index.html","id":"from-github","dir":"","previous_headings":"Installing","what":"From github","title":"The Uniform Manifold Approximation and Projection (UMAP) Method for Dimensionality Reduction","text":"uwot makes use C++ code must compiled. may carry extra steps able build package: Windows: install Rtools ensure C:\\Rtools\\bin path. Mac OS X: using custom ~/.R/Makevars may cause linking errors. sort thing potential problem platforms seems bite Mac owners . R Mac OS X FAQ may helpful work can get away . safe side, advise building uwot without custom Makevars.","code":"install.packages(\"devtools\") devtools::install_github(\"jlmelville/uwot\")"},{"path":"https://jlmelville.github.io/uwot/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"The Uniform Manifold Approximation and Projection (UMAP) Method for Dimensionality Reduction","text":"","code":"library(uwot)  # umap2 is a version of the umap() function with better defaults iris_umap <- umap2(iris)  # but you can still use the umap function (which most of the existing  # documentation does) iris_umap <- umap(iris)  # Load mnist from somewhere, e.g. # devtools::install_github(\"jlmelville/snedata\") # mnist <- snedata::download_mnist()  mnist_umap <- umap(mnist, n_neighbors = 15, min_dist = 0.001, verbose = TRUE) plot(   mnist_umap,   cex = 0.1,   col = grDevices::rainbow(n = length(levels(mnist$Label)))[as.integer(mnist$Label)] |>     grDevices::adjustcolor(alpha.f = 0.1),   main = \"R uwot::umap\",   xlab = \"\",   ylab = \"\" )  # I recommend the following optional packages # for faster or more flexible nearest neighbor search: install.packages(c(\"RcppHNSW\", \"rnndescent\")) library(RcppHNSW) library(rnndescent)  # Installing RcppHNSW will allow the use of the usually faster HNSW method: mnist_umap_hnsw <- umap(mnist, n_neighbors = 15, min_dist = 0.001,                          nn_method = \"hnsw\") # nndescent is also available mnist_umap_nnd <- umap(mnist, n_neighbors = 15, min_dist = 0.001,                         nn_method = \"nndescent\") # umap2 will choose HNSW by default if available mnist_umap2 <- umap2(mnist)"},{"path":"https://jlmelville.github.io/uwot/index.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"The Uniform Manifold Approximation and Projection (UMAP) Method for Dimensionality Reduction","text":"https://jlmelville.github.io/uwot/. examples see get started doc. plenty articles describing various aspects package.","code":""},{"path":"https://jlmelville.github.io/uwot/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"The Uniform Manifold Approximation and Projection (UMAP) Method for Dimensionality Reduction","text":"GPLv3 later.","code":""},{"path":"https://jlmelville.github.io/uwot/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"The Uniform Manifold Approximation and Projection (UMAP) Method for Dimensionality Reduction","text":"want cite use uwot, use output running citation(\"uwot\") (can R package).","code":""},{"path":"https://jlmelville.github.io/uwot/index.html","id":"see-also","dir":"","previous_headings":"","what":"See Also","title":"The Uniform Manifold Approximation and Projection (UMAP) Method for Dimensionality Reduction","text":"UMAP reference implementation publication. UMAP R package (see also github repo), predates uwot’s arrival CRAN. Another R package umapr, longer maintained. umappp full C++ implementation, yaumap provides R wrapper. batch implementation umappp basis uwot’s attempt . uwot uses RcppProgress package show text-based progress bar verbose = TRUE.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/load_uwot.html","id":null,"dir":"Reference","previous_headings":"","what":"Save or Load a Model — load_uwot","title":"Save or Load a Model — load_uwot","text":"Functions write UMAP model file, restore.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/load_uwot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save or Load a Model — load_uwot","text":"","code":"load_uwot(file, verbose = FALSE)"},{"path":"https://jlmelville.github.io/uwot/reference/load_uwot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save or Load a Model — load_uwot","text":"file name file model saved read . verbose TRUE, log information console.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/load_uwot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save or Load a Model — load_uwot","text":"model saved file, use   umap_transform. Additionally, contains extra item:   mod_dir, contains path temporary working directory   used loading model. directory removed   model unloaded using unload_uwot.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/reference/load_uwot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Save or Load a Model — load_uwot","text":"","code":"library(RSpectra)  iris_train <- iris[c(1:10, 51:60), ] iris_test <- iris[100:110, ]  # create model model <- umap(iris_train, ret_model = TRUE, n_epochs = 20)  # save without unloading: this leaves behind a temporary working directory model_file <- tempfile(\"iris_umap\") model <- save_uwot(model, file = model_file)  # The model can continue to be used test_embedding <- umap_transform(iris_test, model)  # To manually unload the model from memory when finished and to clean up # the working directory (this doesn't touch your model file) unload_uwot(model)  # At this point, model cannot be used with umap_transform, this would fail: # test_embedding2 <- umap_transform(iris_test, model)  # restore the model: this also creates a temporary working directory model2 <- load_uwot(file = model_file) test_embedding2 <- umap_transform(iris_test, model2)  # Unload and clean up the loaded model temp directory unload_uwot(model2)  # clean up the model file unlink(model_file)  # save with unloading: this deletes the temporary working directory but # doesn't allow the model to be re-used model3 <- umap(iris_train, ret_model = TRUE, n_epochs = 20) model_file3 <- tempfile(\"iris_umap\") model3 <- save_uwot(model3, file = model_file3, unload = TRUE)"},{"path":"https://jlmelville.github.io/uwot/reference/lvish.html","id":null,"dir":"Reference","previous_headings":"","what":"Dimensionality Reduction with a LargeVis-like method — lvish","title":"Dimensionality Reduction with a LargeVis-like method — lvish","text":"Carry dimensionality reduction dataset using method similar LargeVis (Tang et al., 2016).","code":""},{"path":"https://jlmelville.github.io/uwot/reference/lvish.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dimensionality Reduction with a LargeVis-like method — lvish","text":"","code":"lvish(   X,   perplexity = 50,   n_neighbors = perplexity * 3,   n_components = 2,   metric = \"euclidean\",   n_epochs = -1,   learning_rate = 1,   scale = \"maxabs\",   init = \"lvrandom\",   init_sdev = NULL,   repulsion_strength = 7,   negative_sample_rate = 5,   nn_method = NULL,   n_trees = 50,   search_k = 2 * n_neighbors * n_trees,   n_threads = NULL,   n_sgd_threads = 0,   grain_size = 1,   kernel = \"gauss\",   pca = NULL,   pca_center = TRUE,   pcg_rand = TRUE,   fast_sgd = FALSE,   ret_nn = FALSE,   ret_extra = c(),   tmpdir = tempdir(),   verbose = getOption(\"verbose\", TRUE),   batch = FALSE,   opt_args = NULL,   epoch_callback = NULL,   pca_method = NULL,   binary_edge_weights = FALSE,   nn_args = list(),   rng_type = NULL )"},{"path":"https://jlmelville.github.io/uwot/reference/lvish.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dimensionality Reduction with a LargeVis-like method — lvish","text":"X Input data. Can data.frame, matrix, dist object sparseMatrix. Matrix data frames contain one observation per row. Data frames non-numeric columns removed, although factor columns used explicitly included via metric (see help metric details). sparse matrix interpreted distance matrix, assumed symmetric, can also pass explicitly upper lower triangular sparse matrix save storage. must least n_neighbors non-zero distances row. implicit explicit zero entries ignored. Set zero distances want keep arbitrarily small non-zero value (e.g. 1e-10). X can also NULL pre-computed nearest neighbor data passed nn_method, init \"spca\" \"pca\". perplexity Controls size local neighborhood used manifold approximation. analogous n_neighbors umap. Change , rather n_neighbors. n_neighbors number neighbors use calculating perplexity. Usually set three times value perplexity. Must least large perplexity. n_components dimension space embed . defaults 2 provide easy visualization, can reasonably set integer value range 2 100. metric Type distance metric use find nearest neighbors.  nn_method = \"annoy\" can one : \"euclidean\" (default) \"cosine\" \"manhattan\" \"hamming\" \"correlation\" (distance based Pearson correlation) \"categorical\" (see ) nn_method = \"hnsw\" can one : \"euclidean\" \"cosine\" \"correlation\" rnndescent installed nn_method = \"nndescent\" specified many metrics avaiable, including: \"braycurtis\" \"canberra\" \"chebyshev\" \"dice\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" \"symmetrickl\" \"tsss\" \"yule\" details see package documentation rnndescent. nn_method = \"fnn\", distance metric always \"euclidean\". X data frame matrix, multiple metrics can specified, passing list argument, name item list one metric names . value list item vector giving names integer ids columns included calculation, e.g. metric = list(euclidean = 1:4, manhattan = 5:10). metric calculation results separate fuzzy simplicial set, intersected together produce final set. Metric names can repeated. non-numeric columns removed data frame, safer use column names integer ids. Factor columns can also used specifying metric name \"categorical\". Factor columns treated different numeric columns although multiple factor columns can specified vector, factor column specified processed individually. specify non-factor column, coerced factor. given data block, may override pca pca_center arguments block, providing list one unnamed item containing column names ids, pca pca_center overrides named items, e.g. metric = list(euclidean = 1:4, manhattan = list(5:10, pca_center = FALSE)). exists allow mixed binary real-valued data included PCA applied , centering applied real-valued data (typical apply centering binary data PCA applied). n_epochs Number epochs use optimization embedded coordinates. default calculate number epochs dynamically based dataset size, give number edge samples LargeVis defaults. usually substantially larger UMAP defaults. n_epochs = 0, coordinates determined \"init\" returned. learning_rate Initial learning rate used optimization coordinates. scale Scaling apply X data frame matrix: \"none\" FALSE NULL scaling. \"Z\" \"scale\" TRUE Scale column   zero mean variance 1. \"maxabs\" Center column mean 0, divide   element maximum absolute value entire matrix. \"range\" Range scale entire matrix, smallest   element 0 largest 1. \"colrange\" Scale column range (0,1). lvish, default \"maxabs\", consistency LargeVis. init Type initialization coordinates. Options : \"spectral\" Spectral embedding using normalized Laplacian    fuzzy 1-skeleton, Gaussian noise added. \"normlaplacian\". Spectral embedding using normalized    Laplacian fuzzy 1-skeleton, without noise. \"random\". Coordinates assigned using uniform random    distribution -10 10. \"lvrandom\". Coordinates assigned using Gaussian    distribution standard deviation 1e-4, used LargeVis    (Tang et al., 2016) t-SNE. \"laplacian\". Spectral embedding using Laplacian Eigenmap    (Belkin Niyogi, 2002). \"pca\". first two principal components PCA    X X data frame, 2-dimensional classical    MDS X class \"dist\". \"spca\". Like \"pca\", dimension scaled    standard deviation 1e-4, give distribution similar    used t-SNE LargeVis. alias init = \"pca\",    init_sdev = 1e-4. \"agspectral\" \"approximate global\" modification    \"spectral\" edges graph value 1,    sets random number edges (negative_sample_rate edges per    vertex) 0.1, approximate effect non-local affinities. matrix initial coordinates. spectral initializations, (\"spectral\", \"normlaplacian\", \"laplacian\", \"agspectral\"), one connected component identified, spectral initialization attempted. Instead PCA-based initialization attempted. verbose = TRUE number connected components logged console. existence multiple connected components implies global view data attained initialization. Increasing value n_neighbors may help. init_sdev non-NULL, scales dimension initialized coordinates (including user-supplied matrix) standard deviation. default scaling carried , except init = \"spca\", case value 0.0001. Scaling input may help unscaled versions result initial coordinates large inter-point distances outliers. usually results small gradients optimization little progress made layout. Shrinking initial embedding rescaling can help circumstances. Scaling result init = \"pca\" usually recommended init = \"spca\" alias init = \"pca\", init_sdev = 1e-4 spectral initializations scaled versions usually necessary unless using large value n_neighbors (e.g. n_neighbors = 150 higher). compatibility recent versions Python UMAP package, using init = \"spectral\", also set init_sdev = \"range\", range scale columns containing initial data 0-10. set default maintain backwards compatibility previous versions uwot. repulsion_strength Weighting applied negative samples low dimensional embedding optimization. Values higher one result greater weight given negative samples. negative_sample_rate number negative edge/1-simplex samples use per positive edge/1-simplex sample optimizing low dimensional embedding. nn_method Method finding nearest neighbors. Options : \"fnn\". Use exact nearest neighbors via     FNN package. \"annoy\" Use approximate nearest neighbors via     RcppAnnoy package. \"hnsw\" Use approximate nearest neighbors     Hierarchical Navigable Small World (HNSW) method (Malkov Yashunin,     2018) via     RcppHNSW package.     RcppHNSW dependency package: option     available installed RcppHNSW . Also,     HNSW supports following arguments metric:     \"euclidean\", \"cosine\" \"correlation\". \"nndescent\" Use approximate nearest neighbors     Nearest Neighbor Descent method (Dong et al., 2011) via     rnndescent     package. rnndescent dependency package:     option available installed rnndescent     . default, X less 4,096 vertices, exact nearest neighbors found. Otherwise, approximate nearest neighbors used. may also pass precalculated nearest neighbor data argument. must list consisting two elements: \"idx\". n_vertices x n_neighbors matrix   containing integer indexes nearest neighbors X.   vertex considered nearest neighbor, .e.   idx[, 1] == 1:n_vertices. \"dist\". n_vertices x n_neighbors matrix   containing distances nearest neighbors. Multiple nearest neighbor data (e.g. two different precomputed metrics) can passed passing list containing nearest neighbor data lists items. n_neighbors parameter ignored using precomputed nearest neighbor data. n_trees Number trees build constructing nearest neighbor index. trees specified, larger index, better results. search_k, determines accuracy Annoy nearest neighbor search. used nn_method \"annoy\". Sensible values 10 100. search_k Number nodes search neighbor retrieval. larger k, accurate results, longer search takes. n_trees, determines accuracy Annoy nearest neighbor search. used nn_method \"annoy\". n_threads Number threads use (except stochastic gradient descent). Default half number concurrent threads supported system. nearest neighbor search, applies nn_method = \"annoy\". n_threads > 1, Annoy index temporarily written disk location determined tempfile. n_sgd_threads Number threads use stochastic gradient descent. set > 1, aware batch = FALSE, results reproducible, even set.seed called fixed seed running. Set \"auto\" use value n_threads. grain_size minimum amount work thread. value set high enough, less n_threads n_sgd_threads used processing, might give performance improvement overhead thread management context switching outweighing improvement due concurrent processing. left default (1) work spread evenly threads specified. kernel Type kernel function create input probabilities. Can one \"gauss\" (default) \"knn\". \"gauss\" uses usual Gaussian weighted similarities. \"knn\" assigns equal probabilities every edge nearest neighbor graph, zero otherwise, using perplexity nearest neighbors. n_neighbors parameter ignored case. pca set positive integer value, reduce data number columns using PCA. applied distance metric \"hamming\", dimensions data larger number specified (.e. number rows columns must larger value parameter). > 100 columns data frame matrix, reducing number columns way may substantially increase performance nearest neighbor search cost potential decrease accuracy. many t-SNE applications, value 50 recommended, although guarantee appropriate settings. pca_center TRUE, center columns X carrying PCA. binary data, recommended set FALSE. pcg_rand TRUE, use PCG random number generator (O'Neill, 2014) optimization. Otherwise, use faster (probably less statistically good) Tausworthe \"taus88\" generator. default TRUE. parameter superseded rng_type – set, rng_type takes precedence. fast_sgd TRUE, following combination parameters set: pcg_rand = TRUE n_sgd_threads = \"auto\". default FALSE. Setting TRUE speed stochastic optimization phase, give potentially less accurate embedding, exactly reproducible even fixed seed. visualization, fast_sgd = TRUE give perfectly good results. generic dimensionality reduction, safer leave fast_sgd = FALSE. fast_sgd = TRUE, user-supplied values pcg_rand n_sgd_threads, ignored. ret_nn TRUE, addition embedding, also return nearest neighbor data can used input nn_method avoid overhead repeatedly calculating nearest neighbors manipulating unrelated parameters (e.g. min_dist, n_epochs, init). See \"Value\" section names list items. FALSE, just return coordinates. Note nearest neighbors sensitive data scaling, wary reusing nearest neighbor data modifying scale parameter. ret_extra vector indicating extra data return. May contain combination following strings: \"nn\" setting ret_nn = TRUE. \"P\" high dimensional probability matrix. graph     returned sparse symmetric N x N matrix class     dgCMatrix-class, non-zero entry (, j) gives     input probability (similarity affinity) edge connecting     vertex vertex j. Note graph sparsified     removing edges sufficiently low membership strength     sampled probabilistic edge sampling employed     optimization therefore number non-zero elements     matrix dependent n_epochs. interested     fuzzy input graph (e.g. clustering), setting     n_epochs = 0 avoid sparsifying. aware     setting binary_edge_weights = TRUE affect graph (    non-zero edge weights 1). sigma vector bandwidths used calibrate input     Gaussians reproduce target \"perplexity\". tmpdir Temporary directory store nearest neighbor indexes nearest neighbor search. Default tempdir. index written disk n_threads > 1 nn_method = \"annoy\"; otherwise, parameter ignored. verbose TRUE, log details console. batch TRUE, embedding coordinates updated end epoch rather epoch. batch mode, results reproducible fixed random seed even n_sgd_threads > 1, cost slightly higher memory use. may also modify learning_rate increase n_epochs, whether provides speed increase single-threaded optimization likely dataset hardware-dependent. opt_args list optimizer parameters, used batch = TRUE. default optimization method used Adam (Kingma Ba, 2014). method optimization method use. Either \"adam\"   \"sgd\" (stochastic gradient descent). Default: \"adam\". beta1 (Adam ). weighting parameter   exponential moving average first moment estimator. Effectively   momentum parameter. floating point value 0 1.   Higher values can smooth oscillatory updates poorly-conditioned   situations may allow larger learning_rate   specified, high can cause divergence. Default: 0.5. beta2 (Adam ). weighting parameter   exponential moving average uncentered second moment estimator.   floating point value 0 1. Controls degree   adaptivity step-size. Higher values put weight previous   time steps. Default: 0.9. eps (Adam ). Intended small value prevent   division zero, practice can also affect convergence due   interaction beta2. Higher values reduce effect   step-size adaptivity bring behavior closer stochastic gradient   descent momentum. Typical values 1e-8 1e-3. Default:   1e-7. alpha initial learning rate. Default: value   learning_rate parameter. epoch_callback function invoked end every epoch. signature : (epoch, n_epochs, coords), : epoch current epoch number (1   n_epochs). n_epochs Number epochs use optimization   embedded coordinates. coords embedded coordinates end current   epoch, matrix dimensions (N, n_components). pca_method Method carry PCA dimensionality reduction pca parameter specified. Allowed values : \"irlba\". Uses prcomp_irlba   irlba package. \"rsvd\". Uses 5 iterations svdr   irlba package.   likely give much faster potentially less accurate results   using \"irlba\". purposes nearest neighbor   calculation coordinates initialization, loss accuracy   seem matter much. \"bigstatsr\". Uses big_randomSVD   bigstatsr   package. SVD methods used bigstatsr may faster   systems without access efficient linear algebra libraries (e.g.   Windows). Note: bigstatsr dependency   uwot: choose use package PCA, must install   . \"svd\". Uses svd SVD.   likely slow smallest datasets. \"auto\" (default). Uses \"irlba\", unless   50   case \"svd\" used. binary_edge_weights TRUE edge weights input graph treated binary (0/1) rather real valued. affects sampling frequency neighbors strategy used PaCMAP method (Wang co-workers, 2020). Practical (Böhm co-workers, 2020) theoretical (Damrich Hamprecht, 2021) work suggests little effect UMAP's performance. nn_args list containing additional arguments pass nearest neighbor method. nn_method = \"annoy\", can specify \"n_trees\" \"search_k\", override n_trees search_k parameters. nn_method = \"hnsw\", may specify following arguments: M maximum number neighbors keep vertex. Reasonable values 2 100. Higher values give better recall cost memory. Default value 16. ef_construction positive integer specifying size dynamic list used index construction. higher value provide better results cost longer time build index. Default 200. ef positive integer specifying size dynamic list used search. smaller n_neighbors higher number items index. Default 10. nn_method = \"nndescent\", may specify following arguments: n_trees number trees use random projection forest initialize search. larger number give accurate results cost longer computation time. default NULL means number chosen based number observations X. max_candidates number potential neighbors explore per iteration. default, set n_neighbors 60, whichever smaller. larger number give accurate results cost longer computation time. n_iters number iterations run search. larger number give accurate results cost longer computation time. default, chosen based number observations X. may also need modify convergence criterion delta. delta minimum relative change neighbor graph allowed early stopping. value 0 1. smaller value, smaller amount progress iterations allowed. Default value 0.001 means least 0.1 neighbor graph must updated iteration. init initialize nearest neighbor descent. default set \"tree\" uses random project forest. set \"rand\", random selection used. Usually less accurate using RP trees, high-dimensional cases, may little difference quality initialization random initialization lot faster. set \"rand\", n_trees parameter ignored. rng_type type random number generator use optimization. One : \"pcg\". Use PCG random number generator (O'Neill, 2014). \"tausworthe\". Use Tausworthe \"taus88\" generator. \"deterministic\". Use deterministic number generator.  actually random, may provide enough variation negative  sampling give good embedding can provide noticeable speed-. backwards compatibility, default unset choice pcg_rand used (making \"pcg\" effective default).","code":""},{"path":"https://jlmelville.github.io/uwot/reference/lvish.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Dimensionality Reduction with a LargeVis-like method — lvish","text":"matrix optimized coordinates, : ret_nn = TRUE (ret_extra contains \"nn\"),     returns nearest neighbor data list called nn.     contains one list metric calculated, containing     matrix idx integer ids neighbors; matrix     dist distances. nn list (sub-list) can     used input nn_method parameter. ret_extra contains \"P\", returns high     dimensional probability matrix sparse matrix called P,     type dgCMatrix-class. ret_extra contains \"sigma\", returns vector     high dimensional gaussian bandwidths point,     \"dint\" vector estimates intrinsic dimensionality     point, based method given Lee co-workers (2015). returned list contains combined data combination   specifying ret_nn ret_extra.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/lvish.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Dimensionality Reduction with a LargeVis-like method — lvish","text":"lvish differs official LargeVis implementation following: nearest-neighbor index search phase multi-threaded. Matrix input data normalized. n_trees parameter dynamically chosen based   data set size. Nearest neighbor results refined via   neighbor---neighbor method. search_k parameter twice   large default compensate. Gradient values clipped 4.0 rather 5.0. Negative edges generated uniform sampling vertexes rather   degree ^ 0.75. default number samples much reduced. default number   epochs, n_epochs, set 5000, much larger   umap, may need increased depending   dataset. Using init = \"spectral\" can help.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/lvish.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Dimensionality Reduction with a LargeVis-like method — lvish","text":"Belkin, M., & Niyogi, P. (2002). Laplacian eigenmaps spectral techniques embedding clustering. Advances neural information processing systems (pp. 585-591). http://papers.nips.cc/paper/1961-laplacian-eigenmaps--spectral-techniques--embedding--clustering.pdf Böhm, J. N., Berens, P., & Kobak, D. (2020). unifying perspective neighbor embeddings along attraction-repulsion spectrum. arXiv preprint arXiv:2007.08902. https://arxiv.org/abs/2007.08902 Damrich, S., & Hamprecht, F. . (2021). UMAP's true loss function. Advances Neural Information Processing Systems, 34. https://proceedings.neurips.cc/paper/2021/hash/2de5d16682c3c35007e4e92982f1a2ba-Abstract.html Dong, W., Moses, C., & Li, K. (2011, March). Efficient k-nearest neighbor graph construction generic similarity measures. Proceedings 20th international conference World Wide Web (pp. 577-586). ACM. doi:10.1145/1963405.1963487 . Kingma, D. P., & Ba, J. (2014). Adam: method stochastic optimization. arXiv preprint arXiv:1412.6980. https://arxiv.org/abs/1412.6980 Lee, J. ., Peluffo-Ordóñez, D. H., & Verleysen, M. (2015). Multi-scale similarities stochastic neighbour embedding: Reducing dimensionality preserving local global structure. Neurocomputing, 169, 246-261. Malkov, Y. ., & Yashunin, D. . (2018). Efficient robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions pattern analysis machine intelligence, 42(4), 824-836. McInnes, L., Healy, J., & Melville, J. (2018). UMAP: Uniform Manifold Approximation Projection Dimension Reduction arXiv preprint arXiv:1802.03426. https://arxiv.org/abs/1802.03426 O’Neill, M. E. (2014). PCG: family simple fast space-efficient statistically good algorithms random number generation (Report . HMC-CS-2014-0905). Harvey Mudd College. Tang, J., Liu, J., Zhang, M., & Mei, Q. (2016, April). Visualizing large-scale high-dimensional data. Proceedings 25th International Conference World Wide Web (pp. 287-297). International World Wide Web Conferences Steering Committee. https://arxiv.org/abs/1602.00370 Van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-SNE. Journal Machine Learning Research, 9 (2579-2605). https://www.jmlr.org/papers/v9/vandermaaten08a.html Wang, Y., Huang, H., Rudin, C., & Shaposhnik, Y. (2021). Understanding Dimension Reduction Tools Work: Empirical Approach Deciphering t-SNE, UMAP, TriMap, PaCMAP Data Visualization. Journal Machine Learning Research, 22(201), 1-73. https://www.jmlr.org/papers/v22/20-1061.html","code":""},{"path":"https://jlmelville.github.io/uwot/reference/lvish.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Dimensionality Reduction with a LargeVis-like method — lvish","text":"","code":"# Default number of epochs is much larger than for UMAP, assumes random # initialization. Use perplexity rather than n_neighbors to control the size # of the local neighborhood 20 epochs may be too small for a random # initialization iris_lvish <- lvish(iris,   perplexity = 50, learning_rate = 0.5,   init = \"random\", n_epochs = 20 )"},{"path":"https://jlmelville.github.io/uwot/reference/optimize_graph_layout.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimize Graph Layout — optimize_graph_layout","title":"Optimize Graph Layout — optimize_graph_layout","text":"Carry dimensionality reduction input graph, distances low dimensional space attempt reproduce neighbor relations input data. default, cost function used optimize output coordinates use Uniform Manifold Approximation Projection (UMAP) method (McInnes et al., 2018), approach LargeVis (Tang et al., 2016) can also used. function can used produce low dimensional representation graph produced similarity_graph.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/optimize_graph_layout.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimize Graph Layout — optimize_graph_layout","text":"","code":"optimize_graph_layout(   graph,   X = NULL,   n_components = 2,   n_epochs = NULL,   learning_rate = 1,   init = \"spectral\",   init_sdev = NULL,   spread = 1,   min_dist = 0.01,   repulsion_strength = 1,   negative_sample_rate = 5,   a = NULL,   b = NULL,   method = \"umap\",   approx_pow = FALSE,   pcg_rand = TRUE,   fast_sgd = FALSE,   n_sgd_threads = 0,   grain_size = 1,   verbose = getOption(\"verbose\", TRUE),   batch = FALSE,   opt_args = NULL,   epoch_callback = NULL,   pca_method = NULL,   binary_edge_weights = FALSE,   rng_type = NULL )"},{"path":"https://jlmelville.github.io/uwot/reference/optimize_graph_layout.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimize Graph Layout — optimize_graph_layout","text":"graph sparse, symmetric N x N weighted adjacency matrix representing graph. Non-zero entries indicate edge two nodes given edge weight. can varying number non-zero entries row/column. X Optional input data. Used PCA-based initialization. n_components dimension space embed . defaults 2 provide easy visualization, can reasonably set integer value range 2 100. n_epochs Number epochs use optimization   embedded coordinates. default, value set 500   datasets containing 10,000 vertices less, 200 otherwise.   n_epochs = 0, coordinates determined \"init\"   returned. UMAP, default \"none\". learning_rate Initial learning rate used optimization coordinates. init Type initialization coordinates. Options : \"spectral\" Spectral embedding using normalized Laplacian    fuzzy 1-skeleton, Gaussian noise added. \"normlaplacian\". Spectral embedding using normalized    Laplacian fuzzy 1-skeleton, without noise. \"random\". Coordinates assigned using uniform random    distribution -10 10. \"lvrandom\". Coordinates assigned using Gaussian    distribution standard deviation 1e-4, used LargeVis    (Tang et al., 2016) t-SNE. \"laplacian\". Spectral embedding using Laplacian Eigenmap. \"pca\". first two principal components PCA    X X data frame, 2-dimensional classical    MDS X class \"dist\". \"spca\". Like \"pca\", dimension scaled    standard deviation 1e-4, give distribution similar    used t-SNE. alias init = \"pca\", init_sdev =    1e-4. \"agspectral\" \"approximate global\" modification    \"spectral\" edges graph value 1,    sets random number edges (negative_sample_rate edges per    vertex) 0.1, approximate effect non-local affinities. matrix initial coordinates. spectral initializations, (\"spectral\", \"normlaplacian\", \"laplacian\", \"agspectral\"), one connected component identified, spectral initialization attempted. Instead PCA-based initialization attempted. verbose = TRUE number connected components logged console. existence multiple connected components implies global view data attained initialization. Increasing value n_neighbors may help. init_sdev non-NULL, scales dimension initialized coordinates (including user-supplied matrix) standard deviation. default scaling carried , except init = \"spca\", case value 0.0001. Scaling input may help unscaled versions result initial coordinates large inter-point distances outliers. usually results small gradients optimization little progress made layout. Shrinking initial embedding rescaling can help circumstances. Scaling result init = \"pca\" usually recommended init = \"spca\" alias init = \"pca\", init_sdev = 1e-4 spectral initializations scaled versions usually necessary unless using large value n_neighbors (e.g. n_neighbors = 150 higher). compatibility recent versions Python UMAP package, using init = \"spectral\", also set init_sdev = \"range\", range scale columns containing initial data 0-10. set default maintain backwards compatibility previous versions uwot. spread effective scale embedded points. combination min_dist, determines clustered/clumped embedded points . min_dist effective minimum distance embedded points. Smaller values result clustered/clumped embedding nearby points manifold drawn closer together, larger values result even dispersal points. value set relative spread value, determines scale embedded points spread . repulsion_strength Weighting applied negative samples low dimensional embedding optimization. Values higher one result greater weight given negative samples. negative_sample_rate number negative edge/1-simplex samples use per positive edge/1-simplex sample optimizing low dimensional embedding. specific parameters controlling embedding. NULL values set automatically determined min_dist spread. b specific parameters controlling embedding. NULL values set automatically determined min_dist spread. method Cost function optimize. One : \"umap\". UMAP method McInnes co-workers (2018). \"tumap\". UMAP b parameters fixed   1. \"largevis\". LargeVis method Tang co-workers (2016). approx_pow TRUE, use approximation power function UMAP gradient, https://martin.ankerl.com/2012/01/25/optimized-approximative-pow--c--cpp/. pcg_rand TRUE, use PCG random number generator (O'Neill, 2014) optimization. Otherwise, use faster (probably less statistically good) Tausworthe \"taus88\" generator. default TRUE. parameter superseded rng_type – set, rng_type takes precedence. fast_sgd TRUE, following combination parameters set: pcg_rand = TRUE, n_sgd_threads = \"auto\" approx_pow = TRUE. default FALSE. Setting TRUE speed stochastic optimization phase, give potentially less accurate embedding, exactly reproducible even fixed seed. visualization, fast_sgd = TRUE give perfectly good results. generic dimensionality reduction, safer leave fast_sgd = FALSE. fast_sgd = TRUE, user-supplied values pcg_rand, n_sgd_threads, approx_pow ignored. n_sgd_threads Number threads use stochastic gradient descent. set > 1, aware batch = FALSE, results reproducible, even set.seed called fixed seed running. set \"auto\" half number concurrent threads supported system used. grain_size minimum amount work thread. value set high enough, less n_threads n_sgd_threads used processing, might give performance improvement overhead thread management context switching outweighing improvement due concurrent processing. left default (1) work spread evenly threads specified. verbose TRUE, log details console. batch TRUE, embedding coordinates updated end epoch rather epoch. batch mode, results reproducible fixed random seed even n_sgd_threads > 1, cost slightly higher memory use. may also modify learning_rate increase n_epochs, whether provides speed increase single-threaded optimization likely dataset hardware-dependent. opt_args list optimizer parameters, used batch = TRUE. default optimization method used Adam (Kingma Ba, 2014). method optimization method use. Either \"adam\"   \"sgd\" (stochastic gradient descent). Default: \"adam\". beta1 (Adam ). weighting parameter   exponential moving average first moment estimator. Effectively   momentum parameter. floating point value 0 1.   Higher values can smooth oscillatory updates poorly-conditioned   situations may allow larger learning_rate   specified, high can cause divergence. Default: 0.5. beta2 (Adam ). weighting parameter   exponential moving average uncentered second moment estimator.   floating point value 0 1. Controls degree   adaptivity step-size. Higher values put weight previous   time steps. Default: 0.9. eps (Adam ). Intended small value prevent   division zero, practice can also affect convergence due   interaction beta2. Higher values reduce effect   step-size adaptivity bring behavior closer stochastic gradient   descent momentum. Typical values 1e-8 1e-3. Default:   1e-7. alpha initial learning rate. Default: value   learning_rate parameter. epoch_callback function invoked end every epoch. signature : (epoch, n_epochs, coords), : epoch current epoch number (1   n_epochs). n_epochs Number epochs use optimization   embedded coordinates. coords embedded coordinates end current   epoch, matrix dimensions (N, n_components). pca_method Method carry PCA dimensionality reduction pca parameter specified. Allowed values : \"irlba\". Uses prcomp_irlba   irlba package. \"rsvd\". Uses 5 iterations svdr   irlba package.   likely give much faster potentially less accurate results   using \"irlba\". purposes nearest neighbor   calculation coordinates initialization, loss accuracy   seem matter much. \"bigstatsr\". Uses big_randomSVD   bigstatsr   package. SVD methods used bigstatsr may faster   systems without access efficient linear algebra libraries (e.g.   Windows). Note: bigstatsr dependency   uwot: choose use package PCA, must install   . \"svd\". Uses svd SVD.   likely slow smallest datasets. \"auto\" (default). Uses \"irlba\", unless   50   case \"svd\" used. binary_edge_weights TRUE edge weights input graph treated binary (0/1) rather real valued. rng_type type random number generator use optimization. One : \"pcg\". Use PCG random number generator (O'Neill, 2014). \"tausworthe\". Use Tausworthe \"taus88\" generator. \"deterministic\". Use deterministic number generator.  actually random, may provide enough variation negative  sampling give good embedding can provide noticeable speed-. backwards compatibility, default unset choice pcg_rand used (making \"pcg\" effective default).","code":""},{"path":"https://jlmelville.github.io/uwot/reference/optimize_graph_layout.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimize Graph Layout — optimize_graph_layout","text":"matrix optimized coordinates.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/optimize_graph_layout.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimize Graph Layout — optimize_graph_layout","text":"Kingma, D. P., & Ba, J. (2014). Adam: method stochastic optimization. arXiv preprint arXiv:1412.6980. https://arxiv.org/abs/1412.6980 McInnes, L., Healy, J., & Melville, J. (2018). UMAP: Uniform Manifold Approximation Projection Dimension Reduction arXiv preprint arXiv:1802.03426. https://arxiv.org/abs/1802.03426 O’Neill, M. E. (2014). PCG: family simple fast space-efficient statistically good algorithms random number generation (Report . HMC-CS-2014-0905). Harvey Mudd College. Tang, J., Liu, J., Zhang, M., & Mei, Q. (2016, April). Visualizing large-scale high-dimensional data. Proceedings 25th International Conference World Wide Web (pp. 287-297). International World Wide Web Conferences Steering Committee. https://arxiv.org/abs/1602.00370","code":""},{"path":"https://jlmelville.github.io/uwot/reference/optimize_graph_layout.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimize Graph Layout — optimize_graph_layout","text":"","code":"iris30 <- iris[c(1:10, 51:60, 101:110), ]  # return a 30 x 30 sparse matrix with similarity data based on 10 nearest # neighbors per item iris30_sim_graph <- similarity_graph(iris30, n_neighbors = 10) # produce 2D coordinates replicating the neighbor relations in the similarity # graph set.seed(42) iris30_opt <- optimize_graph_layout(iris30_sim_graph, X = iris30)  # the above two steps are the same as: # set.seed(42); iris_umap <- umap(iris30, n_neighbors = 10)"},{"path":"https://jlmelville.github.io/uwot/reference/save_uwot.html","id":null,"dir":"Reference","previous_headings":"","what":"Save or Load a Model — save_uwot","title":"Save or Load a Model — save_uwot","text":"Functions write UMAP model file, restore.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/save_uwot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save or Load a Model — save_uwot","text":"","code":"save_uwot(model, file, unload = FALSE, verbose = FALSE)"},{"path":"https://jlmelville.github.io/uwot/reference/save_uwot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save or Load a Model — save_uwot","text":"model UMAP model create umap. file name file model saved read . unload TRUE, unload nearest neighbor indexes model. model longer valid use umap_transform temporary working directory used model saving deleted. need reload model load_uwot use model. FALSE, model can re-used without reloading, must manually unload NN index finished using want delete temporary working directory. unload manually, use unload_uwot. absolute path working directory found mod_dir item return value. verbose TRUE, log information console.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/save_uwot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save or Load a Model — save_uwot","text":"model one extra item: mod_dir, contains   path working directory. unload = FALSE directory   still exists function returns, can cleaned   unload_uwot. care cleaning   directory, unload = TRUE, can ignore return value.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/reference/save_uwot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Save or Load a Model — save_uwot","text":"","code":"iris_train <- iris[c(1:10, 51:60), ] iris_test <- iris[100:110, ]  # create model model <- umap(iris_train, ret_model = TRUE, n_epochs = 20)  # save without unloading: this leaves behind a temporary working directory model_file <- tempfile(\"iris_umap\") model <- save_uwot(model, file = model_file)  # The model can continue to be used test_embedding <- umap_transform(iris_test, model)  # To manually unload the model from memory when finished and to clean up # the working directory (this doesn't touch your model file) unload_uwot(model)  # At this point, model cannot be used with umap_transform, this would fail: # test_embedding2 <- umap_transform(iris_test, model)  # restore the model: this also creates a temporary working directory model2 <- load_uwot(file = model_file) test_embedding2 <- umap_transform(iris_test, model2)  # Unload and clean up the loaded model temp directory unload_uwot(model2)  # clean up the model file unlink(model_file)  # save with unloading: this deletes the temporary working directory but # doesn't allow the model to be re-used model3 <- umap(iris_train, ret_model = TRUE, n_epochs = 20) model_file3 <- tempfile(\"iris_umap\") model3 <- save_uwot(model3, file = model_file3, unload = TRUE)"},{"path":"https://jlmelville.github.io/uwot/reference/similarity_graph.html","id":null,"dir":"Reference","previous_headings":"","what":"Similarity Graph — similarity_graph","title":"Similarity Graph — similarity_graph","text":"Create graph (sparse symmetric weighted adjacency matrix) representing similarities items data set. dimensionality reduction carried . default, similarities calculated using merged fuzzy simplicial set approach Uniform Manifold Approximation Projection (UMAP) method (McInnes et al., 2018), approach LargeVis (Tang et al., 2016) can also used.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/similarity_graph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Similarity Graph — similarity_graph","text":"","code":"similarity_graph(   X = NULL,   n_neighbors = NULL,   metric = \"euclidean\",   scale = NULL,   set_op_mix_ratio = 1,   local_connectivity = 1,   nn_method = NULL,   n_trees = 50,   search_k = 2 * n_neighbors * n_trees,   perplexity = 50,   method = \"umap\",   y = NULL,   target_n_neighbors = n_neighbors,   target_metric = \"euclidean\",   target_weight = 0.5,   pca = NULL,   pca_center = TRUE,   ret_extra = c(),   n_threads = NULL,   grain_size = 1,   kernel = \"gauss\",   tmpdir = tempdir(),   verbose = getOption(\"verbose\", TRUE),   pca_method = NULL,   binary_edge_weights = FALSE,   nn_args = list() )"},{"path":"https://jlmelville.github.io/uwot/reference/similarity_graph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Similarity Graph — similarity_graph","text":"X Input data. Can data.frame, matrix, dist object sparseMatrix. Matrix data frames contain one observation per row. Data frames non-numeric columns removed, although factor columns used explicitly included via metric (see help metric details). sparse matrix interpreted distance matrix, assumed symmetric, can also pass explicitly upper lower triangular sparse matrix save storage. must least n_neighbors non-zero distances row. implicit explicit zero entries ignored. Set zero distances want keep arbitrarily small non-zero value (e.g. 1e-10). X can also NULL pre-computed nearest neighbor data passed nn_method. n_neighbors size local neighborhood (terms number neighboring sample points) used manifold approximation. Larger values result global views manifold, smaller values result local data preserved. general values range 2 100. metric Type distance metric use find nearest neighbors.  nn_method = \"annoy\" can one : \"euclidean\" (default) \"cosine\" \"manhattan\" \"hamming\" \"correlation\" (distance based Pearson correlation) \"categorical\" (see ) nn_method = \"hnsw\" can one : \"euclidean\" \"cosine\" \"correlation\" rnndescent installed nn_method = \"nndescent\" specified many metrics avaiable, including: \"braycurtis\" \"canberra\" \"chebyshev\" \"dice\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" \"symmetrickl\" \"tsss\" \"yule\" details see package documentation rnndescent. nn_method = \"fnn\", distance metric always \"euclidean\". X data frame matrix, multiple metrics can specified, passing list argument, name item list one metric names . value list item vector giving names integer ids columns included calculation, e.g. metric = list(euclidean = 1:4, manhattan = 5:10). metric calculation results separate fuzzy simplicial set, intersected together produce final set. Metric names can repeated. non-numeric columns removed data frame, safer use column names integer ids. Factor columns can also used specifying metric name \"categorical\". Factor columns treated different numeric columns although multiple factor columns can specified vector, factor column specified processed individually. specify non-factor column, coerced factor. given data block, may override pca pca_center arguments block, providing list one unnamed item containing column names ids, pca pca_center overrides named items, e.g. metric = list(euclidean = 1:4, manhattan = list(5:10, pca_center = FALSE)). exists allow mixed binary real-valued data included PCA applied , centering applied real-valued data (typical apply centering binary data PCA applied). scale Scaling apply X data frame matrix: \"none\" FALSE NULL scaling. \"Z\" \"scale\" TRUE Scale column   zero mean variance 1. \"maxabs\" Center column mean 0, divide   element maximum absolute value entire matrix. \"range\" Range scale entire matrix, smallest   element 0 largest 1. \"colrange\" Scale column range (0,1). method \"umap\", default \"none\". \"largevis\", default \"maxabs\". set_op_mix_ratio Interpolate (fuzzy) union intersection set operation used combine local fuzzy simplicial sets obtain global fuzzy simplicial sets. fuzzy set operations use product t-norm. value parameter 0.0 1.0; value 1.0 use pure fuzzy union, 0.0 use pure fuzzy intersection. Ignored method = \"largevis\" local_connectivity local connectivity required – .e. number nearest neighbors assumed connected local level. higher value connected manifold becomes locally. practice local intrinsic dimension manifold. Ignored method = \"largevis\". nn_method Method finding nearest neighbors. Options : \"fnn\". Use exact nearest neighbors via     FNN package. \"annoy\" Use approximate nearest neighbors via     RcppAnnoy package. \"hnsw\" Use approximate nearest neighbors     Hierarchical Navigable Small World (HNSW) method (Malkov Yashunin,     2018) via     RcppHNSW package.     RcppHNSW dependency package: option     available installed RcppHNSW . Also,     HNSW supports following arguments metric     target_metric: \"euclidean\", \"cosine\"     \"correlation\". \"nndescent\" Use approximate nearest neighbors     Nearest Neighbor Descent method (Dong et al., 2011) via     rnndescent     package. rnndescent dependency package:     option available installed rnndescent     . default, X less 4,096 vertices, exact nearest neighbors found. Otherwise, approximate nearest neighbors used. may also pass pre-calculated nearest neighbor data argument. must one two formats, either list consisting two elements: \"idx\". n_vertices x n_neighbors matrix   containing integer indexes nearest neighbors X.   vertex considered nearest neighbor, .e.   idx[, 1] == 1:n_vertices. \"dist\". n_vertices x n_neighbors matrix   containing distances nearest neighbors. sparse distance matrix type dgCMatrix, dimensions n_vertices x n_vertices. Distances arranged column, .e. non-zero entry row j ith column indicates jth observation X nearest neighbor ith observation distance given value element. n_neighbors parameter ignored using precomputed nearest neighbor data. using sparse distance matrix input, column can contain different number neighbors. n_trees Number trees build constructing nearest neighbor index. trees specified, larger index, better results. search_k, determines accuracy Annoy nearest neighbor search. used nn_method \"annoy\". Sensible values 10 100. search_k Number nodes search neighbor retrieval. larger k, accurate results, longer search takes. n_trees, determines accuracy Annoy nearest neighbor search. used nn_method \"annoy\". perplexity Used method = \"largevis\". Controls size local neighborhood used manifold approximation. value 1 one less number items X. specified, specify value n_neighbors unless know . method generate similarities items. One : \"umap\" UMAP method McInnes et al. (2018). \"largevis\" LargeVis method Tang et al. (2016). y Optional target data add supervised semi-supervised weighting   similarity graph . Can vector, matrix data frame. Use   target_metric parameter specify metrics use, using   syntax metric. Usually either single numeric factor   column used, complex formats possible. following types   allowed: Factor columns length X. NA     allowed observation unknown level, case     UMAP operates form semi-supervised learning. column     treated separately. Numeric data. NA allowed case. Use     parameter target_n_neighbors set number neighbors used     y. unset, n_neighbors used. Unlike factors,     numeric columns grouped one block unless target_metric     specifies otherwise. example, wish columns     b treated separately, specify     target_metric = list(euclidean = \"\", euclidean = \"b\"). Otherwise,     data effectively treated matrix two columns. Nearest neighbor data, consisting list two matrices,     idx dist. represent precalculated nearest     neighbor indices distances, respectively.     format expected precalculated data     nn_method. format assumes underlying data     numeric vector. user-supplied value target_n_neighbors     parameter ignored case, number columns     matrices used value. Multiple nearest neighbor data using     different metrics can supplied passing list lists. Unlike X, factor columns included y automatically used. parameter ignored method = \"largevis\". target_n_neighbors Number nearest neighbors use construct target simplicial set. Default value n_neighbors. Applies y non-NULL numeric.  parameter ignored method = \"largevis\". target_metric metric used measure distance y using supervised dimension reduction. Used y numeric. parameter ignored method = \"largevis\". target_weight Weighting factor data topology target topology. value 0.0 weights entirely data, value 1.0 weights entirely target. default 0.5 balances weighting equally data target. applies y non-NULL. parameter ignored method = \"largevis\". pca set positive integer value, reduce data number columns using PCA. applied distance metric \"hamming\", dimensions data larger number specified (.e. number rows columns must larger value parameter). > 100 columns data frame matrix, reducing number columns way may substantially increase performance nearest neighbor search cost potential decrease accuracy. many t-SNE applications, value 50 recommended, although guarantee appropriate settings. pca_center TRUE, center columns X carrying PCA. binary data, recommended set FALSE. ret_extra vector indicating extra data return. May contain combination following strings: \"nn\" nearest neighbor data can used input     nn_method avoid overhead repeatedly calculating     nearest neighbors manipulating unrelated parameters. See     \"Value\" section names list items. Note nearest     neighbors sensitive data scaling, wary reusing     nearest neighbor data modifying scale parameter. \"sigma\" normalization value observation     dataset constructing smoothed distances     neighbors. gives sense local density     observation high dimensional space: higher values     sigma indicate higher dispersion lower density. n_threads Number threads use. Default half number concurrent threads supported system. nearest neighbor search, applies nn_method = \"annoy\". n_threads > 1, Annoy index temporarily written disk location determined tempfile. grain_size minimum amount work thread. value set high enough, less n_threads used processing, might give performance improvement overhead thread management context switching outweighing improvement due concurrent processing. left default (1) work spread evenly threads specified. kernel Used method = \"largevis\". Type kernel function create input similiarties. Can one \"gauss\" (default) \"knn\". \"gauss\" uses usual Gaussian weighted similarities. \"knn\" assigns equal similiarties. every edge nearest neighbor graph, zero otherwise, using perplexity nearest neighbors. n_neighbors parameter ignored case. tmpdir Temporary directory store nearest neighbor indexes nearest neighbor search. Default tempdir. index written disk n_threads > 1 nn_method = \"annoy\"; otherwise, parameter ignored. verbose TRUE, log details console. pca_method Method carry PCA dimensionality reduction pca parameter specified. Allowed values : \"irlba\". Uses prcomp_irlba   irlba package. \"rsvd\". Uses 5 iterations svdr   irlba package.   likely give much faster potentially less accurate results   using \"irlba\". purposes nearest neighbor   calculation coordinates initialization, loss accuracy   seem matter much. \"bigstatsr\". Uses big_randomSVD   bigstatsr   package. SVD methods used bigstatsr may faster   systems without access efficient linear algebra libraries (e.g.   Windows). Note: bigstatsr dependency   uwot: choose use package PCA, must install   . \"svd\". Uses svd SVD.   likely slow smallest datasets. \"auto\" (default). Uses \"irlba\", unless   50   case \"svd\" used. binary_edge_weights TRUE edge weights returned graph binary (0/1) rather reflecting degree similarity. nn_args list containing additional arguments pass nearest neighbor method. nn_method = \"annoy\", can specify \"n_trees\" \"search_k\", override n_trees search_k parameters. nn_method = \"hnsw\", may specify following arguments: M maximum number neighbors keep vertex. Reasonable values 2 100. Higher values give better recall cost memory. Default value 16. ef_construction positive integer specifying size dynamic list used index construction. higher value provide better results cost longer time build index. Default 200. ef positive integer specifying size dynamic list used search. smaller n_neighbors higher number items index. Default 10. nn_method = \"nndescent\", may specify following arguments: n_trees number trees use random projection forest initialize search. larger number give accurate results cost longer computation time. default NULL means number chosen based number observations X. max_candidates number potential neighbors explore per iteration. default, set n_neighbors 60, whichever smaller. larger number give accurate results cost longer computation time. n_iters number iterations run search. larger number give accurate results cost longer computation time. default, chosen based number observations X. may also need modify convergence criterion delta. delta minimum relative change neighbor graph allowed early stopping. value 0 1. smaller value, smaller amount progress iterations allowed. Default value 0.001 means least 0.1 neighbor graph must updated iteration. init initialize nearest neighbor descent. default set \"tree\" uses random project forest. set \"rand\", random selection used. Usually less accurate using RP trees, high-dimensional cases, may little difference quality initialization random initialization lot faster. set \"rand\", n_trees parameter ignored. pruning_degree_multiplier maximum number edges per node retain search graph, relative n_neighbors. larger value give accurate results cost longer computation time. Default 1.5. parameter affects neighbor search transforming new data umap_transform. epsilon Controls degree back-tracking traversing search graph. Setting 0.0 greedy search back-tracking. larger value give accurate results cost longer computation time. Default 0.1. parameter affects neighbor search transforming new data umap_transform. max_search_fraction Specifies maximum fraction search graph traverse. default, set 1.0, entire graph (.e. items X) may visited. may want set smaller value large dataset (conjunction epsilon) avoid inefficient exhaustive search data X. parameter affects neighbor search transforming new data umap_transform.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/similarity_graph.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Similarity Graph — similarity_graph","text":"sparse symmetrized matrix similarities items   X nn_method contains pre-computed nearest neighbor   data, items nn_method. symmetrization,   may non-zero items column specified value   n_neighbors (pre-computed neighbors nn_method).   ret_extra specified return value list   containing: similarity_graph similarity graph sparse matrix     described . nn (ret_extra contained \"nn\") nearest     neighbor data list called nn. contains one list     metric calculated, containing matrix idx     integer ids neighbors; matrix dist     distances. nn list (sub-list) can used input     nn_method parameter. sigma (ret_extra contains \"sigma\"),     vector calibrated parameters, one item input data,     reflecting local data density item. exact definition     values depends choice method parameter. rho (ret_extra contains \"sigma\"),     vector containing largest distance locally connected neighbors     item input data. exist     method = \"umap\". localr (ret_extra contains \"localr\")     vector estimated local radii, sum \"sigma\"     \"rho\". exist method = \"umap\".","code":""},{"path":"https://jlmelville.github.io/uwot/reference/similarity_graph.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Similarity Graph — similarity_graph","text":"equivalent running umap ret_extra = c(\"fgraph\") parameter, without overhead calculating (returning) optimized low-dimensional coordinates.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/similarity_graph.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Similarity Graph — similarity_graph","text":"Dong, W., Moses, C., & Li, K. (2011, March). Efficient k-nearest neighbor graph construction generic similarity measures. Proceedings 20th international conference World Wide Web (pp. 577-586). ACM. doi:10.1145/1963405.1963487 . Malkov, Y. ., & Yashunin, D. . (2018). Efficient robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions pattern analysis machine intelligence, 42(4), 824-836. McInnes, L., Healy, J., & Melville, J. (2018). UMAP: Uniform Manifold Approximation Projection Dimension Reduction arXiv preprint arXiv:1802.03426. https://arxiv.org/abs/1802.03426 Tang, J., Liu, J., Zhang, M., & Mei, Q. (2016, April). Visualizing large-scale high-dimensional data. Proceedings 25th International Conference World Wide Web (pp. 287-297). International World Wide Web Conferences Steering Committee. https://arxiv.org/abs/1602.00370","code":""},{"path":"https://jlmelville.github.io/uwot/reference/similarity_graph.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Similarity Graph — similarity_graph","text":"","code":"iris30 <- iris[c(1:10, 51:60, 101:110), ]  # return a 30 x 30 sparse matrix with similarity data based on 10 nearest # neighbors per item iris30_sim_graph <- similarity_graph(iris30, n_neighbors = 10)  # Default is to use the UMAP method of calculating similarities, but LargeVis # is also available: for that method, use perplexity instead of n_neighbors # to control neighborhood size. Use ret_extra = \"nn\" to return nearest # neighbor data as well as the similarity graph. Return value is a list # containing similarity_graph' and 'nn' items. iris30_lv_graph <- similarity_graph(iris30,   perplexity = 10,   method = \"largevis\", ret_extra = \"nn\" ) # If you have the neighbor information you don't need the original data iris30_lv_graph_nn <- similarity_graph(   nn_method = iris30_lv_graph$nn,   perplexity = 10, method = \"largevis\" ) all(iris30_lv_graph_nn == iris30_lv_graph$similarity_graph) #> [1] TRUE"},{"path":"https://jlmelville.github.io/uwot/reference/simplicial_set_intersect.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge Similarity Graph by Simplicial Set Intersection — simplicial_set_intersect","title":"Merge Similarity Graph by Simplicial Set Intersection — simplicial_set_intersect","text":"Combine two similarity graphs treating fuzzy topological sets forming intersection.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/simplicial_set_intersect.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge Similarity Graph by Simplicial Set Intersection — simplicial_set_intersect","text":"","code":"simplicial_set_intersect(x, y, weight = 0.5, n_threads = NULL, verbose = FALSE)"},{"path":"https://jlmelville.github.io/uwot/reference/simplicial_set_intersect.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge Similarity Graph by Simplicial Set Intersection — simplicial_set_intersect","text":"x sparse matrix representing first similarity graph intersection operation. y sparse matrix representing second similarity graph intersection operation. weight value 0 - 1, controlling relative influence x y intersection. Default (0.5) gives equal influence. Values smaller 0.5 put weight x. Values greater 0.5 put weight y. n_threads Number threads use resetting local metric. Default half number concurrent threads supported system. verbose TRUE, log progress console.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/simplicial_set_intersect.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge Similarity Graph by Simplicial Set Intersection — simplicial_set_intersect","text":"sparse matrix containing intersection x   y.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/simplicial_set_intersect.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merge Similarity Graph by Simplicial Set Intersection — simplicial_set_intersect","text":"","code":"# Form two different \"views\" of the same data iris30 <- iris[c(1:10, 51:60, 101:110), ] iris_sg12 <- similarity_graph(iris30[, 1:2], n_neighbors = 5) iris_sg34 <- similarity_graph(iris30[, 3:4], n_neighbors = 5)  # Combine the two representations into one iris_combined <- simplicial_set_intersect(iris_sg12, iris_sg34)  # Optimize the layout based on the combined view iris_combined_umap <- optimize_graph_layout(iris_combined, n_epochs = 100)"},{"path":"https://jlmelville.github.io/uwot/reference/simplicial_set_union.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge Similarity Graph by Simplicial Set Union — simplicial_set_union","title":"Merge Similarity Graph by Simplicial Set Union — simplicial_set_union","text":"Combine two similarity graphs treating fuzzy topological sets forming union.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/simplicial_set_union.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge Similarity Graph by Simplicial Set Union — simplicial_set_union","text":"","code":"simplicial_set_union(x, y, n_threads = NULL, verbose = FALSE)"},{"path":"https://jlmelville.github.io/uwot/reference/simplicial_set_union.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge Similarity Graph by Simplicial Set Union — simplicial_set_union","text":"x sparse matrix representing first similarity graph union operation. y sparse matrix representing second similarity graph union operation. n_threads Number threads use resetting local metric. Default half number concurrent threads supported system. verbose TRUE, log progress console.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/simplicial_set_union.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge Similarity Graph by Simplicial Set Union — simplicial_set_union","text":"sparse matrix containing union x y.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/simplicial_set_union.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merge Similarity Graph by Simplicial Set Union — simplicial_set_union","text":"","code":"# Form two different \"views\" of the same data iris30 <- iris[c(1:10, 51:60, 101:110), ] iris_sg12 <- similarity_graph(iris30[, 1:2], n_neighbors = 5) iris_sg34 <- similarity_graph(iris30[, 3:4], n_neighbors = 5)  # Combine the two representations into one iris_combined <- simplicial_set_union(iris_sg12, iris_sg34)  # Optimize the layout based on the combined view iris_combined_umap <- optimize_graph_layout(iris_combined, n_epochs = 100)"},{"path":"https://jlmelville.github.io/uwot/reference/tumap.html","id":null,"dir":"Reference","previous_headings":"","what":"Dimensionality Reduction Using t-Distributed UMAP (t-UMAP) — tumap","title":"Dimensionality Reduction Using t-Distributed UMAP (t-UMAP) — tumap","text":"faster (less flexible) version UMAP (McInnes et al, 2018) gradient. detail UMAP, see umap function.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/tumap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dimensionality Reduction Using t-Distributed UMAP (t-UMAP) — tumap","text":"","code":"tumap(   X,   n_neighbors = 15,   n_components = 2,   metric = \"euclidean\",   n_epochs = NULL,   learning_rate = 1,   scale = FALSE,   init = \"spectral\",   init_sdev = NULL,   set_op_mix_ratio = 1,   local_connectivity = 1,   bandwidth = 1,   repulsion_strength = 1,   negative_sample_rate = 5,   nn_method = NULL,   n_trees = 50,   search_k = 2 * n_neighbors * n_trees,   n_threads = NULL,   n_sgd_threads = 0,   grain_size = 1,   y = NULL,   target_n_neighbors = n_neighbors,   target_metric = \"euclidean\",   target_weight = 0.5,   pca = NULL,   pca_center = TRUE,   pcg_rand = TRUE,   fast_sgd = FALSE,   ret_model = FALSE,   ret_nn = FALSE,   ret_extra = c(),   tmpdir = tempdir(),   verbose = getOption(\"verbose\", TRUE),   batch = FALSE,   opt_args = NULL,   epoch_callback = NULL,   pca_method = NULL,   binary_edge_weights = FALSE,   seed = NULL,   nn_args = list(),   rng_type = NULL )"},{"path":"https://jlmelville.github.io/uwot/reference/tumap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dimensionality Reduction Using t-Distributed UMAP (t-UMAP) — tumap","text":"X Input data. Can data.frame, matrix, dist object sparseMatrix. Matrix data frames contain one observation per row. Data frames non-numeric columns removed, although factor columns used explicitly included via metric (see help metric details). sparse matrix interpreted distance matrix, assumed symmetric, can also pass explicitly upper lower triangular sparse matrix save storage. must least n_neighbors non-zero distances row. implicit explicit zero entries ignored. Set zero distances want keep arbitrarily small non-zero value (e.g. 1e-10). X can also NULL pre-computed nearest neighbor data passed nn_method, init \"spca\" \"pca\". n_neighbors size local neighborhood (terms number neighboring sample points) used manifold approximation. Larger values result global views manifold, smaller values result local data preserved. general values range 2 100. n_components dimension space embed . defaults 2 provide easy visualization, can reasonably set integer value range 2 100. metric Type distance metric use find nearest neighbors.  nn_method = \"annoy\" can one : \"euclidean\" (default) \"cosine\" \"manhattan\" \"hamming\" \"correlation\" (distance based Pearson correlation) \"categorical\" (see ) nn_method = \"hnsw\" can one : \"euclidean\" \"cosine\" \"correlation\" rnndescent installed nn_method = \"nndescent\" specified many metrics avaiable, including: \"braycurtis\" \"canberra\" \"chebyshev\" \"dice\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" \"symmetrickl\" \"tsss\" \"yule\" details see package documentation rnndescent. nn_method = \"fnn\", distance metric always \"euclidean\". X data frame matrix, multiple metrics can specified, passing list argument, name item list one metric names . value list item vector giving names integer ids columns included calculation, e.g. metric = list(euclidean = 1:4, manhattan = 5:10). metric calculation results separate fuzzy simplicial set, intersected together produce final set. Metric names can repeated. non-numeric columns removed data frame, safer use column names integer ids. Factor columns can also used specifying metric name \"categorical\". Factor columns treated different numeric columns although multiple factor columns can specified vector, factor column specified processed individually. specify non-factor column, coerced factor. given data block, may override pca pca_center arguments block, providing list one unnamed item containing column names ids, pca pca_center overrides named items, e.g. metric = list(euclidean = 1:4, manhattan = list(5:10, pca_center = FALSE)). exists allow mixed binary real-valued data included PCA applied , centering applied real-valued data (typical apply centering binary data PCA applied). n_epochs Number epochs use optimization embedded coordinates. default, value set 500 datasets containing 10,000 vertices less, 200 otherwise. n_epochs = 0, coordinates determined \"init\" returned. learning_rate Initial learning rate used optimization coordinates. scale Scaling apply X data frame matrix: \"none\" FALSE NULL scaling. \"Z\" \"scale\" TRUE Scale column   zero mean variance 1. \"maxabs\" Center column mean 0, divide   element maximum absolute value entire matrix. \"range\" Range scale entire matrix, smallest   element 0 largest 1. \"colrange\" Scale column range (0,1). t-UMAP, default \"none\". init Type initialization coordinates. Options : \"spectral\" Spectral embedding using normalized Laplacian    fuzzy 1-skeleton, Gaussian noise added. \"normlaplacian\". Spectral embedding using normalized    Laplacian fuzzy 1-skeleton, without noise. \"random\". Coordinates assigned using uniform random    distribution -10 10. \"lvrandom\". Coordinates assigned using Gaussian    distribution standard deviation 1e-4, used LargeVis    (Tang et al., 2016) t-SNE. \"laplacian\". Spectral embedding using Laplacian Eigenmap    (Belkin Niyogi, 2002). \"pca\". first two principal components PCA    X X data frame, 2-dimensional classical    MDS X class \"dist\". \"spca\". Like \"pca\", dimension scaled    standard deviation 1e-4, give distribution similar    used t-SNE. alias init = \"pca\", init_sdev =    1e-4. \"agspectral\" \"approximate global\" modification    \"spectral\" edges graph value 1,    sets random number edges (negative_sample_rate edges per    vertex) 0.1, approximate effect non-local affinities. matrix initial coordinates. spectral initializations, (\"spectral\", \"normlaplacian\", \"laplacian\", \"agspectral\"), one connected component identified, spectral initialization attempted. Instead PCA-based initialization attempted. verbose = TRUE number connected components logged console. existence multiple connected components implies global view data attained initialization. Increasing value n_neighbors may help. init_sdev non-NULL, scales dimension initialized coordinates (including user-supplied matrix) standard deviation. default scaling carried , except init = \"spca\", case value 0.0001. Scaling input may help unscaled versions result initial coordinates large inter-point distances outliers. usually results small gradients optimization little progress made layout. Shrinking initial embedding rescaling can help circumstances. Scaling result init = \"pca\" usually recommended init = \"spca\" alias init = \"pca\", init_sdev = 1e-4 spectral initializations scaled versions usually necessary unless using large value n_neighbors (e.g. n_neighbors = 150 higher). compatibility recent versions Python UMAP package, using init = \"spectral\", also set init_sdev = \"range\", range scale columns containing initial data 0-10. set default maintain backwards compatibility previous versions uwot. set_op_mix_ratio Interpolate (fuzzy) union intersection set operation used combine local fuzzy simplicial sets obtain global fuzzy simplicial sets. fuzzy set operations use product t-norm. value parameter 0.0 1.0; value 1.0 use pure fuzzy union, 0.0 use pure fuzzy intersection. local_connectivity local connectivity required – .e. number nearest neighbors assumed connected local level. higher value connected manifold becomes locally. practice local intrinsic dimension manifold. bandwidth effective bandwidth kernel view algorithm similar Laplacian Eigenmaps. Larger values induce connectivity global view data, smaller values concentrate locally. repulsion_strength Weighting applied negative samples low dimensional embedding optimization. Values higher one result greater weight given negative samples. negative_sample_rate number negative edge/1-simplex samples use per positive edge/1-simplex sample optimizing low dimensional embedding. nn_method Method finding nearest neighbors. Options : \"fnn\". Use exact nearest neighbors via     FNN package. \"annoy\" Use approximate nearest neighbors via     RcppAnnoy package. \"hnsw\" Use approximate nearest neighbors     Hierarchical Navigable Small World (HNSW) method (Malkov Yashunin,     2018) via     RcppHNSW package.     RcppHNSW dependency package: option     available installed RcppHNSW . Also,     HNSW supports following arguments metric     target_metric: \"euclidean\", \"cosine\"     \"correlation\". \"nndescent\" Use approximate nearest neighbors     Nearest Neighbor Descent method (Dong et al., 2011) via     rnndescent     package. rnndescent dependency package:     option available installed rnndescent     . default, X less 4,096 vertices, exact nearest neighbors found. Otherwise, approximate nearest neighbors used. may also pass pre-calculated nearest neighbor data argument. must one two formats, either list consisting two elements: \"idx\". n_vertices x n_neighbors matrix   containing integer indexes nearest neighbors X.   vertex considered nearest neighbor, .e.   idx[, 1] == 1:n_vertices. \"dist\". n_vertices x n_neighbors matrix   containing distances nearest neighbors. sparse distance matrix type dgCMatrix, dimensions n_vertices x n_vertices. Distances arranged column, .e. non-zero entry row j ith column indicates jth observation X nearest neighbor ith observation distance given value element. n_neighbors parameter ignored using precomputed nearest neighbor data. using sparse distance matrix input, column can contain different number neighbors. n_trees Number trees build constructing nearest neighbor index. trees specified, larger index, better results. search_k, determines accuracy Annoy nearest neighbor search. used nn_method \"annoy\". Sensible values 10 100. search_k Number nodes search neighbor retrieval. larger k, accurate results, longer search takes. n_trees, determines accuracy Annoy nearest neighbor search. used nn_method \"annoy\". n_threads Number threads use (except stochastic gradient descent). Default half number concurrent threads supported system. nearest neighbor search, applies nn_method = \"annoy\". n_threads > 1, Annoy index temporarily written disk location determined tempfile. n_sgd_threads Number threads use stochastic gradient descent. set > 1, aware batch = FALSE, results reproducible, even set.seed called fixed seed running. Set \"auto\" use value n_threads. grain_size minimum amount work thread. value set high enough, less n_threads n_sgd_threads used processing, might give performance improvement overhead thread management context switching outweighing improvement due concurrent processing. left default (1) work spread evenly threads specified. y Optional target data supervised dimension reduction. Can vector, matrix data frame. Use target_metric parameter specify metrics use, using syntax metric. Usually either single numeric factor column used, complex formats possible. following types allowed: Factor columns length X. NA     allowed observation unknown level, case     UMAP operates form semi-supervised learning. column     treated separately. Numeric data. NA allowed case. Use     parameter target_n_neighbors set number neighbors used     y. unset, n_neighbors used. Unlike factors,     numeric columns grouped one block unless target_metric     specifies otherwise. example, wish columns     b treated separately, specify     target_metric = list(euclidean = \"\", euclidean = \"b\"). Otherwise,     data effectively treated matrix two columns. Nearest neighbor data, consisting list two matrices,     idx dist. represent precalculated nearest     neighbor indices distances, respectively.     format expected precalculated data     nn_method. format assumes underlying data     numeric vector. user-supplied value target_n_neighbors     parameter ignored case, number columns     matrices used value. Multiple nearest neighbor data using     different metrics can supplied passing list lists. Unlike X, factor columns included y automatically used. target_n_neighbors Number nearest neighbors use construct target simplicial set. Default value n_neighbors. Applies y non-NULL numeric. target_metric metric used measure distance y using supervised dimension reduction. Used y numeric. target_weight Weighting factor data topology target topology. value 0.0 weights entirely data, value 1.0 weights entirely target. default 0.5 balances weighting equally data target. applies y non-NULL. pca set positive integer value, reduce data number columns using PCA. applied distance metric \"hamming\", dimensions data larger number specified (.e. number rows columns must larger value parameter). > 100 columns data frame matrix, reducing number columns way may substantially increase performance nearest neighbor search cost potential decrease accuracy. many t-SNE applications, value 50 recommended, although guarantee appropriate settings. pca_center TRUE, center columns X carrying PCA. binary data, recommended set FALSE. pcg_rand TRUE, use PCG random number generator (O'Neill, 2014) optimization. Otherwise, use faster (probably less statistically good) Tausworthe \"taus88\" generator. default TRUE. parameter superseded rng_type – set, rng_type takes precedence. fast_sgd TRUE, following combination parameters set: pcg_rand = TRUE n_sgd_threads = \"auto\". default FALSE. Setting TRUE speed stochastic optimization phase, give potentially less accurate embedding, exactly reproducible even fixed seed. visualization, fast_sgd = TRUE give perfectly good results. generic dimensionality reduction, safer leave fast_sgd = FALSE. fast_sgd = TRUE, user-supplied values pcg_rand n_sgd_threads, ignored. ret_model TRUE, return extra data can used add new data existing embedding via umap_transform. embedded coordinates returned list item embedding. FALSE, just return coordinates. parameter can used conjunction ret_nn ret_extra. Note settings incompatible production UMAP model: external neighbor data (passed via list nn_method), factor columns included via metric parameter. latter case, model produced based numeric data. transformation using new data possible, factor columns new data ignored. Note setting ret_model = TRUE forces use approximate nearest neighbors method. small datasets otherwise use exact nearest neighbor calculations, setting ret_model = TRUE means different results may returned small datasets terms returned nearest neighbors (requested) final embedded coordinates, compared ret_model = FALSE, even random number seed fixed. avoid , explicitly set nn_method = \"annoy\" ret_model = FALSE case. ret_nn TRUE, addition embedding, also return nearest neighbor data can used input nn_method avoid overhead repeatedly calculating nearest neighbors manipulating unrelated parameters (e.g. min_dist, n_epochs, init). See \"Value\" section names list items. FALSE, just return coordinates. Note nearest neighbors sensitive data scaling, wary reusing nearest neighbor data modifying scale parameter. parameter can used conjunction ret_model ret_extra. ret_extra vector indicating extra data return. May contain combination following strings: \"model\" setting ret_model = TRUE. \"nn\" setting ret_nn = TRUE. \"fgraph\" high dimensional fuzzy graph (.e. fuzzy     simplicial set merged local views input data). graph     returned sparse symmetric N x N matrix class     dgCMatrix-class, non-zero entry (, j) gives     membership strength edge connecting vertex vertex j.     can considered analogous input probability (similarity     affinity) used t-SNE LargeVis. Note graph     sparsified removing edges sufficiently low membership strength     sampled probabilistic edge sampling     employed optimization therefore number non-zero elements     matrix dependent n_epochs.     interested fuzzy input graph (e.g. clustering), setting     n_epochs = 0 avoid sparsifying. aware     setting binary_edge_weights = TRUE affect graph (    non-zero edge weights 1). \"sigma\" normalization value observation     dataset constructing smoothed distances     neighbors. gives sense local density     observation high dimensional space: higher values     sigma indicate higher dispersion lower density. tmpdir Temporary directory store nearest neighbor indexes nearest neighbor search. Default tempdir. index written disk n_threads > 1 nn_method = \"annoy\"; otherwise, parameter ignored. verbose TRUE, log details console. batch TRUE, embedding coordinates updated end epoch rather epoch. batch mode, results reproducible fixed random seed even n_sgd_threads > 1, cost slightly higher memory use. may also modify learning_rate increase n_epochs, whether provides speed increase single-threaded optimization likely dataset hardware-dependent. opt_args list optimizer parameters, used batch = TRUE. default optimization method used Adam (Kingma Ba, 2014). method optimization method use. Either \"adam\"   \"sgd\" (stochastic gradient descent). Default: \"adam\". beta1 (Adam ). weighting parameter   exponential moving average first moment estimator. Effectively   momentum parameter. floating point value 0 1.   Higher values can smooth oscillatory updates poorly-conditioned   situations may allow larger learning_rate   specified, high can cause divergence. Default: 0.5. beta2 (Adam ). weighting parameter   exponential moving average uncentered second moment estimator.   floating point value 0 1. Controls degree   adaptivity step-size. Higher values put weight previous   time steps. Default: 0.9. eps (Adam ). Intended small value prevent   division zero, practice can also affect convergence due   interaction beta2. Higher values reduce effect   step-size adaptivity bring behavior closer stochastic gradient   descent momentum. Typical values 1e-8 1e-3. Default:   1e-7. alpha initial learning rate. Default: value   learning_rate parameter. epoch_callback function invoked end every epoch. signature : (epoch, n_epochs, coords), : epoch current epoch number (1   n_epochs). n_epochs Number epochs use optimization   embedded coordinates. coords embedded coordinates end current   epoch, matrix dimensions (N, n_components). pca_method Method carry PCA dimensionality reduction pca parameter specified. Allowed values : \"irlba\". Uses prcomp_irlba   irlba package. \"rsvd\". Uses 5 iterations svdr   irlba package.   likely give much faster potentially less accurate results   using \"irlba\". purposes nearest neighbor   calculation coordinates initialization, loss accuracy   seem matter much. \"bigstatsr\". Uses big_randomSVD   bigstatsr   package. SVD methods used bigstatsr may faster   systems without access efficient linear algebra libraries (e.g.   Windows). Note: bigstatsr dependency   uwot: choose use package PCA, must install   . \"svd\". Uses svd SVD.   likely slow smallest datasets. \"auto\" (default). Uses \"irlba\", unless   50   case \"svd\" used. binary_edge_weights TRUE edge weights input graph treated binary (0/1) rather real valued. affects sampling frequency neighbors strategy used PaCMAP method (Wang co-workers, 2020). Practical (Böhm co-workers, 2020) theoretical (Damrich Hamprecht, 2021) work suggests little effect UMAP's performance. seed Integer seed use initialize random number generator state. Combined n_sgd_threads = 1 batch = TRUE, give consistent output across multiple runs given installation. Setting value equivalent calling set.seed, may convenient situations call separate function. default set seed. ret_model = TRUE, seed stored output model used set seed inside umap_transform. nn_args list containing additional arguments pass nearest neighbor method. nn_method = \"annoy\", can specify \"n_trees\" \"search_k\", override n_trees search_k parameters. nn_method = \"hnsw\", may specify following arguments: M maximum number neighbors keep vertex. Reasonable values 2 100. Higher values give better recall cost memory. Default value 16. ef_construction positive integer specifying size dynamic list used index construction. higher value provide better results cost longer time build index. Default 200. ef positive integer specifying size dynamic list used search. smaller n_neighbors higher number items index. Default 10. nn_method = \"nndescent\", may specify following arguments: n_trees number trees use random projection forest initialize search. larger number give accurate results cost longer computation time. default NULL means number chosen based number observations X. max_candidates number potential neighbors explore per iteration. default, set n_neighbors 60, whichever smaller. larger number give accurate results cost longer computation time. n_iters number iterations run search. larger number give accurate results cost longer computation time. default, chosen based number observations X. may also need modify convergence criterion delta. delta minimum relative change neighbor graph allowed early stopping. value 0 1. smaller value, smaller amount progress iterations allowed. Default value 0.001 means least 0.1 neighbor graph must updated iteration. init initialize nearest neighbor descent. default set \"tree\" uses random project forest. set \"rand\", random selection used. Usually less accurate using RP trees, high-dimensional cases, may little difference quality initialization random initialization lot faster. set \"rand\", n_trees parameter ignored. pruning_degree_multiplier maximum number edges per node retain search graph, relative n_neighbors. larger value give accurate results cost longer computation time. Default 1.5. parameter affects neighbor search transforming new data umap_transform. epsilon Controls degree back-tracking traversing search graph. Setting 0.0 greedy search back-tracking. larger value give accurate results cost longer computation time. Default 0.1. parameter affects neighbor search transforming new data umap_transform. max_search_fraction Specifies maximum fraction search graph traverse. default, set 1.0, entire graph (.e. items X) may visited. may want set smaller value large dataset (conjunction epsilon) avoid inefficient exhaustive search data X. parameter affects neighbor search transforming new data umap_transform. nn_method = \"nndescent\", may specify following arguments: n_trees number trees use random projection forest initialize search. larger number give accurate results cost longer computation time. default NULL means number chosen based number observations X. max_candidates number potential neighbors explore per iteration. default, set n_neighbors 60, whichever smaller. larger number give accurate results cost longer computation time. n_iters number iterations run search. larger number give accurate results cost longer computation time. default, chosen based number observations X. may also need modify convergence criterion delta. delta minimum relative change neighbor graph allowed early stopping. value 0 1. smaller value, smaller amount progress iterations allowed. Default value 0.001 means least 0.1 neighbor graph must updated iteration. init initialize nearest neighbor descent. default set \"tree\" uses random project forest. set \"rand\", random selection used. Usually less accurate using RP trees, high-dimensional cases, may little difference quality initialization random initialization lot faster. set \"rand\", n_trees parameter ignored. pruning_degree_multiplier maximum number edges per node retain search graph, relative n_neighbors. larger value give accurate results cost longer computation time. Default 1.5. parameter affects neighbor search transforming new data umap_transform. epsilon Controls degree back-tracking traversing search graph. Setting 0.0 greedy search back-tracking. larger value give accurate results cost longer computation time. Default 0.1. parameter affects neighbor search transforming new data umap_transform. max_search_fraction Specifies maximum fraction search graph traverse. default, set 1.0, entire graph (.e. items X) may visited. may want set smaller value large dataset (conjunction epsilon) avoid inefficient exhaustive search data X. parameter affects neighbor search transforming new data umap_transform. rng_type type random number generator use optimization. One : \"pcg\". Use PCG random number generator (O'Neill, 2014). \"tausworthe\". Use Tausworthe \"taus88\" generator. \"deterministic\". Use deterministic number generator.  actually random, may provide enough variation negative  sampling give good embedding can provide noticeable speed-. backwards compatibility, default unset choice pcg_rand used (making \"pcg\" effective default).","code":""},{"path":"https://jlmelville.github.io/uwot/reference/tumap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Dimensionality Reduction Using t-Distributed UMAP (t-UMAP) — tumap","text":"matrix optimized coordinates, : ret_model = TRUE (ret_extra contains     \"model\"), returns list containing extra information can     used add new data existing embedding via     umap_transform. case, coordinates available     list item embedding. NOTE: contents     model list considered stable part     public API, purposely left undocumented. ret_nn = TRUE (ret_extra contains \"nn\"),     returns nearest neighbor data list called nn.     contains one list metric calculated, containing     matrix idx integer ids neighbors; matrix     dist distances. nn list (sub-list) can     used input nn_method parameter. ret_extra contains \"fgraph\" returns high     dimensional fuzzy graph sparse matrix called fgraph, type     dgCMatrix-class. ret_extra contains \"sigma\", returns vector     smooth knn distance normalization terms observation     \"sigma\" vector \"rho\" containing largest     distance locally connected neighbors observation. ret_extra contains \"localr\", returns vector     estimated local radii, sum \"sigma\" \"rho\". returned list contains combined data combination   specifying ret_model, ret_nn ret_extra.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/tumap.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Dimensionality Reduction Using t-Distributed UMAP (t-UMAP) — tumap","text":"setting UMAP curve parameters b 1, get back Cauchy distribution used t-SNE (van der Maaten Hinton, 2008) LargeVis (Tang et al., 2016). also results substantially simplified gradient expression. can give speed improvement around 50%.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/tumap.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Dimensionality Reduction Using t-Distributed UMAP (t-UMAP) — tumap","text":"Belkin, M., & Niyogi, P. (2002). Laplacian eigenmaps spectral techniques embedding clustering. Advances neural information processing systems (pp. 585-591). http://papers.nips.cc/paper/1961-laplacian-eigenmaps--spectral-techniques--embedding--clustering.pdf Böhm, J. N., Berens, P., & Kobak, D. (2020). unifying perspective neighbor embeddings along attraction-repulsion spectrum. arXiv preprint arXiv:2007.08902. https://arxiv.org/abs/2007.08902 Damrich, S., & Hamprecht, F. . (2021). UMAP's true loss function. Advances Neural Information Processing Systems, 34. https://proceedings.neurips.cc/paper/2021/hash/2de5d16682c3c35007e4e92982f1a2ba-Abstract.html Dong, W., Moses, C., & Li, K. (2011, March). Efficient k-nearest neighbor graph construction generic similarity measures. Proceedings 20th international conference World Wide Web (pp. 577-586). ACM. doi:10.1145/1963405.1963487 . Kingma, D. P., & Ba, J. (2014). Adam: method stochastic optimization. arXiv preprint arXiv:1412.6980. https://arxiv.org/abs/1412.6980 Malkov, Y. ., & Yashunin, D. . (2018). Efficient robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions pattern analysis machine intelligence, 42(4), 824-836. McInnes, L., Healy, J., & Melville, J. (2018). UMAP: Uniform Manifold Approximation Projection Dimension Reduction arXiv preprint arXiv:1802.03426. https://arxiv.org/abs/1802.03426 O’Neill, M. E. (2014). PCG: family simple fast space-efficient statistically good algorithms random number generation (Report . HMC-CS-2014-0905). Harvey Mudd College. Tang, J., Liu, J., Zhang, M., & Mei, Q. (2016, April). Visualizing large-scale high-dimensional data. Proceedings 25th International Conference World Wide Web (pp. 287-297). International World Wide Web Conferences Steering Committee. https://arxiv.org/abs/1602.00370 Van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-SNE. Journal Machine Learning Research, 9 (2579-2605). https://www.jmlr.org/papers/v9/vandermaaten08a.html Wang, Y., Huang, H., Rudin, C., & Shaposhnik, Y. (2021). Understanding Dimension Reduction Tools Work: Empirical Approach Deciphering t-SNE, UMAP, TriMap, PaCMAP Data Visualization. Journal Machine Learning Research, 22(201), 1-73. https://www.jmlr.org/papers/v22/20-1061.html","code":""},{"path":"https://jlmelville.github.io/uwot/reference/tumap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Dimensionality Reduction Using t-Distributed UMAP (t-UMAP) — tumap","text":"","code":"iris_tumap <- tumap(iris, n_neighbors = 50, learning_rate = 0.5)"},{"path":"https://jlmelville.github.io/uwot/reference/umap.html","id":null,"dir":"Reference","previous_headings":"","what":"Dimensionality Reduction with UMAP — umap","title":"Dimensionality Reduction with UMAP — umap","text":"Carry dimensionality reduction dataset using Uniform Manifold Approximation Projection (UMAP) method (McInnes et al., 2018). following help text lifted verbatim Python reference implementation https://github.com/lmcinnes/umap.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/umap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dimensionality Reduction with UMAP — umap","text":"","code":"umap(   X,   n_neighbors = 15,   n_components = 2,   metric = \"euclidean\",   n_epochs = NULL,   learning_rate = 1,   scale = FALSE,   init = \"spectral\",   init_sdev = NULL,   spread = 1,   min_dist = 0.01,   set_op_mix_ratio = 1,   local_connectivity = 1,   bandwidth = 1,   repulsion_strength = 1,   negative_sample_rate = 5,   a = NULL,   b = NULL,   nn_method = NULL,   n_trees = 50,   search_k = 2 * n_neighbors * n_trees,   approx_pow = FALSE,   y = NULL,   target_n_neighbors = n_neighbors,   target_metric = \"euclidean\",   target_weight = 0.5,   pca = NULL,   pca_center = TRUE,   pcg_rand = TRUE,   fast_sgd = FALSE,   ret_model = FALSE,   ret_nn = FALSE,   ret_extra = c(),   n_threads = NULL,   n_sgd_threads = 0,   grain_size = 1,   tmpdir = tempdir(),   verbose = getOption(\"verbose\", TRUE),   batch = FALSE,   opt_args = NULL,   epoch_callback = NULL,   pca_method = NULL,   binary_edge_weights = FALSE,   dens_scale = NULL,   seed = NULL,   nn_args = list(),   rng_type = NULL )"},{"path":"https://jlmelville.github.io/uwot/reference/umap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dimensionality Reduction with UMAP — umap","text":"X Input data. Can data.frame, matrix, dist object sparseMatrix. Matrix data frames contain one observation per row. Data frames non-numeric columns removed, although factor columns used explicitly included via metric (see help metric details). sparse matrix interpreted distance matrix, assumed symmetric, can also pass explicitly upper lower triangular sparse matrix save storage. must least n_neighbors non-zero distances row. implicit explicit zero entries ignored. Set zero distances want keep arbitrarily small non-zero value (e.g. 1e-10). X can also NULL pre-computed nearest neighbor data passed nn_method, init \"spca\" \"pca\". n_neighbors size local neighborhood (terms number neighboring sample points) used manifold approximation. Larger values result global views manifold, smaller values result local data preserved. general values range 2 100. n_components dimension space embed . defaults 2 provide easy visualization, can reasonably set integer value range 2 100. metric Type distance metric use find nearest neighbors.  nn_method = \"annoy\" can one : \"euclidean\" (default) \"cosine\" \"manhattan\" \"hamming\" \"correlation\" (distance based Pearson correlation) \"categorical\" (see ) nn_method = \"hnsw\" can one : \"euclidean\" \"cosine\" \"correlation\" rnndescent installed nn_method = \"nndescent\" specified many metrics avaiable, including: \"braycurtis\" \"canberra\" \"chebyshev\" \"dice\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" \"symmetrickl\" \"tsss\" \"yule\" details see package documentation rnndescent. nn_method = \"fnn\", distance metric always \"euclidean\". X data frame matrix, multiple metrics can specified, passing list argument, name item list one metric names . value list item vector giving names integer ids columns included calculation, e.g. metric = list(euclidean = 1:4, manhattan = 5:10). metric calculation results separate fuzzy simplicial set, intersected together produce final set. Metric names can repeated. non-numeric columns removed data frame, safer use column names integer ids. Factor columns can also used specifying metric name \"categorical\". Factor columns treated different numeric columns although multiple factor columns can specified vector, factor column specified processed individually. specify non-factor column, coerced factor. given data block, may override pca pca_center arguments block, providing list one unnamed item containing column names ids, pca pca_center overrides named items, e.g. metric = list(euclidean = 1:4, manhattan = list(5:10, pca_center = FALSE)). exists allow mixed binary real-valued data included PCA applied , centering applied real-valued data (typical apply centering binary data PCA applied). n_epochs Number epochs use optimization embedded coordinates. default, value set 500 datasets containing 10,000 vertices less, 200 otherwise. n_epochs = 0, coordinates determined \"init\" returned. learning_rate Initial learning rate used optimization coordinates. scale Scaling apply X data frame matrix: \"none\" FALSE NULL scaling. \"Z\" \"scale\" TRUE Scale column   zero mean variance 1. \"maxabs\" Center column mean 0, divide   element maximum absolute value entire matrix. \"range\" Range scale entire matrix, smallest   element 0 largest 1. \"colrange\" Scale column range (0,1). UMAP, default \"none\". init Type initialization coordinates. Options : \"spectral\" Spectral embedding using normalized Laplacian    fuzzy 1-skeleton, Gaussian noise added. \"normlaplacian\". Spectral embedding using normalized    Laplacian fuzzy 1-skeleton, without noise. \"random\". Coordinates assigned using uniform random    distribution -10 10. \"lvrandom\". Coordinates assigned using Gaussian    distribution standard deviation 1e-4, used LargeVis    (Tang et al., 2016) t-SNE. \"laplacian\". Spectral embedding using Laplacian Eigenmap    (Belkin Niyogi, 2002). \"pca\". first two principal components PCA    X X data frame, 2-dimensional classical    MDS X class \"dist\". \"spca\". Like \"pca\", dimension scaled    standard deviation 1e-4, give distribution similar    used t-SNE. alias init = \"pca\", init_sdev =    1e-4. \"agspectral\" \"approximate global\" modification    \"spectral\" edges graph value 1,    sets random number edges (negative_sample_rate edges per    vertex) 0.1, approximate effect non-local affinities. matrix initial coordinates. spectral initializations, (\"spectral\", \"normlaplacian\", \"laplacian\", \"agspectral\"), one connected component identified, spectral initialization attempted. Instead PCA-based initialization attempted. verbose = TRUE number connected components logged console. existence multiple connected components implies global view data attained initialization. Increasing value n_neighbors may help. init_sdev non-NULL, scales dimension initialized coordinates (including user-supplied matrix) standard deviation. default scaling carried , except init = \"spca\", case value 0.0001. Scaling input may help unscaled versions result initial coordinates large inter-point distances outliers. usually results small gradients optimization little progress made layout. Shrinking initial embedding rescaling can help circumstances. Scaling result init = \"pca\" usually recommended init = \"spca\" alias init = \"pca\", init_sdev = 1e-4 spectral initializations scaled versions usually necessary unless using large value n_neighbors (e.g. n_neighbors = 150 higher). compatibility recent versions Python UMAP package, using init = \"spectral\", also set init_sdev = \"range\", range scale columns containing initial data 0-10. set default maintain backwards compatibility previous versions uwot. spread effective scale embedded points. combination min_dist, determines clustered/clumped embedded points . min_dist effective minimum distance embedded points. Smaller values result clustered/clumped embedding nearby points manifold drawn closer together, larger values result even dispersal points. value set relative spread value, determines scale embedded points spread . set_op_mix_ratio Interpolate (fuzzy) union intersection set operation used combine local fuzzy simplicial sets obtain global fuzzy simplicial sets. fuzzy set operations use product t-norm. value parameter 0.0 1.0; value 1.0 use pure fuzzy union, 0.0 use pure fuzzy intersection. local_connectivity local connectivity required – .e. number nearest neighbors assumed connected local level. higher value connected manifold becomes locally. practice local intrinsic dimension manifold. bandwidth effective bandwidth kernel view algorithm similar Laplacian Eigenmaps. Larger values induce connectivity global view data, smaller values concentrate locally. repulsion_strength Weighting applied negative samples low dimensional embedding optimization. Values higher one result greater weight given negative samples. negative_sample_rate number negative edge/1-simplex samples use per positive edge/1-simplex sample optimizing low dimensional embedding. specific parameters controlling embedding. NULL values set automatically determined min_dist spread. b specific parameters controlling embedding. NULL values set automatically determined min_dist spread. nn_method Method finding nearest neighbors. Options : \"fnn\". Use exact nearest neighbors via     FNN package. \"annoy\" Use approximate nearest neighbors via     RcppAnnoy package. \"hnsw\" Use approximate nearest neighbors     Hierarchical Navigable Small World (HNSW) method (Malkov Yashunin,     2018) via     RcppHNSW package.     RcppHNSW dependency package: option     available installed RcppHNSW . Also,     HNSW supports following arguments metric     target_metric: \"euclidean\", \"cosine\"     \"correlation\". \"nndescent\" Use approximate nearest neighbors     Nearest Neighbor Descent method (Dong et al., 2011) via     rnndescent     package. rnndescent dependency package:     option available installed rnndescent     . default, X less 4,096 vertices, exact nearest neighbors found. Otherwise, approximate nearest neighbors used. may also pass pre-calculated nearest neighbor data argument. must one two formats, either list consisting two elements: \"idx\". n_vertices x n_neighbors matrix   containing integer indexes nearest neighbors X.   vertex considered nearest neighbor, .e.   idx[, 1] == 1:n_vertices. \"dist\". n_vertices x n_neighbors matrix   containing distances nearest neighbors. sparse distance matrix type dgCMatrix, dimensions n_vertices x n_vertices. Distances arranged column, .e. non-zero entry row j ith column indicates jth observation X nearest neighbor ith observation distance given value element. n_neighbors parameter ignored using precomputed nearest neighbor data. using sparse distance matrix input, column can contain different number neighbors. n_trees Number trees build constructing nearest neighbor index. trees specified, larger index, better results. search_k, determines accuracy Annoy nearest neighbor search. used nn_method \"annoy\". Sensible values 10 100. search_k Number nodes search neighbor retrieval. larger k, accurate results, longer search takes. n_trees, determines accuracy Annoy nearest neighbor search. used nn_method \"annoy\". approx_pow TRUE, use approximation power function UMAP gradient, https://martin.ankerl.com/2012/01/25/optimized-approximative-pow--c--cpp/. Ignored dens_scale non-NULL. y Optional target data supervised dimension reduction. Can vector, matrix data frame. Use target_metric parameter specify metrics use, using syntax metric. Usually either single numeric factor column used, complex formats possible. following types allowed: Factor columns length X. NA     allowed observation unknown level, case     UMAP operates form semi-supervised learning. column     treated separately. Numeric data. NA allowed case. Use     parameter target_n_neighbors set number neighbors used     y. unset, n_neighbors used. Unlike factors,     numeric columns grouped one block unless target_metric     specifies otherwise. example, wish columns     b treated separately, specify     target_metric = list(euclidean = \"\", euclidean = \"b\"). Otherwise,     data effectively treated matrix two columns. Nearest neighbor data, consisting list two matrices,     idx dist. represent precalculated nearest     neighbor indices distances, respectively.     format expected precalculated data     nn_method. format assumes underlying data     numeric vector. user-supplied value target_n_neighbors     parameter ignored case, number columns     matrices used value. Multiple nearest neighbor data using     different metrics can supplied passing list lists. Unlike X, factor columns included y automatically used. target_n_neighbors Number nearest neighbors use construct target simplicial set. Default value n_neighbors. Applies y non-NULL numeric. target_metric metric used measure distance y using supervised dimension reduction. Used y numeric. target_weight Weighting factor data topology target topology. value 0.0 weights entirely data, value 1.0 weights entirely target. default 0.5 balances weighting equally data target. applies y non-NULL. pca set positive integer value, reduce data number columns using PCA. applied distance metric \"hamming\", dimensions data larger number specified (.e. number rows columns must larger value parameter). > 100 columns data frame matrix, reducing number columns way may substantially increase performance nearest neighbor search cost potential decrease accuracy. many t-SNE applications, value 50 recommended, although guarantee appropriate settings. pca_center TRUE, center columns X carrying PCA. binary data, recommended set FALSE. pcg_rand TRUE, use PCG random number generator (O'Neill, 2014) optimization. Otherwise, use faster (probably less statistically good) Tausworthe \"taus88\" generator. default TRUE. parameter superseded rng_type – set, rng_type takes precedence. fast_sgd TRUE, following combination parameters set: pcg_rand = TRUE, n_sgd_threads = \"auto\" approx_pow = TRUE. default FALSE. Setting TRUE speed stochastic optimization phase, give potentially less accurate embedding, exactly reproducible even fixed seed. visualization, fast_sgd = TRUE give perfectly good results. generic dimensionality reduction, safer leave fast_sgd = FALSE. fast_sgd = TRUE, user-supplied values pcg_rand, n_sgd_threads, approx_pow ignored. ret_model TRUE, return extra data can used add new data existing embedding via umap_transform. embedded coordinates returned list item embedding. FALSE, just return coordinates. parameter can used conjunction ret_nn ret_extra. Note settings incompatible production UMAP model: external neighbor data (passed via list nn_method), factor columns included via metric parameter. latter case, model produced based numeric data. transformation using new data possible, factor columns new data ignored. Note setting ret_model = TRUE forces use approximate nearest neighbors method. small datasets otherwise use exact nearest neighbor calculations, setting ret_model = TRUE means different results may returned small datasets terms returned nearest neighbors (requested) final embedded coordinates, compared ret_model = FALSE, even random number seed fixed. avoid , explicitly set nn_method = \"annoy\" ret_model = FALSE case. ret_nn TRUE, addition embedding, also return nearest neighbor data can used input nn_method avoid overhead repeatedly calculating nearest neighbors manipulating unrelated parameters (e.g. min_dist, n_epochs, init). See \"Value\" section names list items. FALSE, just return coordinates. Note nearest neighbors sensitive data scaling, wary reusing nearest neighbor data modifying scale parameter. parameter can used conjunction ret_model ret_extra. ret_extra vector indicating extra data return. May contain combination following strings: \"model\" setting ret_model = TRUE. \"nn\" setting ret_nn = TRUE. \"fgraph\" high dimensional fuzzy graph (.e. fuzzy     simplicial set merged local views input data). graph     returned sparse symmetric N x N matrix class     dgCMatrix-class, non-zero entry (, j) gives     membership strength edge connecting vertex vertex j.     can considered analogous input probability (similarity     affinity) used t-SNE LargeVis. Note graph     sparsified removing edges sufficiently low membership strength     sampled probabilistic edge sampling     employed optimization therefore number non-zero elements     matrix dependent n_epochs.     interested fuzzy input graph (e.g. clustering), setting     n_epochs = 0 avoid sparsifying.     aware setting `binary_edge_weights = TRUE` affect     graph (non-zero edge weights 1). \"sigma\" normalization value observation     dataset constructing smoothed distances     neighbors. gives sense local density     observation high dimensional space: higher values     sigma indicate higher dispersion lower density. n_threads Number threads use (except stochastic gradient descent). Default half number concurrent threads supported system. nearest neighbor search, applies nn_method = \"annoy\". n_threads > 1, Annoy index temporarily written disk location determined tempfile. n_sgd_threads Number threads use stochastic gradient descent. set > 1, aware batch = FALSE, results reproducible, even set.seed called fixed seed running. Set \"auto\" use value n_threads. grain_size minimum amount work thread. value set high enough, less n_threads n_sgd_threads used processing, might give performance improvement overhead thread management context switching outweighing improvement due concurrent processing. left default (1) work spread evenly threads specified. tmpdir Temporary directory store nearest neighbor indexes nearest neighbor search. Default tempdir. index written disk n_threads > 1 nn_method = \"annoy\"; otherwise, parameter ignored. verbose TRUE, log details console. batch TRUE, embedding coordinates updated end epoch rather epoch. batch mode, results reproducible fixed random seed even n_sgd_threads > 1, cost slightly higher memory use. may also modify learning_rate increase n_epochs, whether provides speed increase single-threaded optimization likely dataset hardware-dependent. opt_args list optimizer parameters, used batch = TRUE. default optimization method used Adam (Kingma Ba, 2014). method optimization method use. Either \"adam\"   \"sgd\" (stochastic gradient descent). Default: \"adam\". beta1 (Adam ). weighting parameter   exponential moving average first moment estimator. Effectively   momentum parameter. floating point value 0 1.   Higher values can smooth oscillatory updates poorly-conditioned   situations may allow larger learning_rate   specified, high can cause divergence. Default: 0.5. beta2 (Adam ). weighting parameter   exponential moving average uncentered second moment estimator.   floating point value 0 1. Controls degree   adaptivity step-size. Higher values put weight previous   time steps. Default: 0.9. eps (Adam ). Intended small value prevent   division zero, practice can also affect convergence due   interaction beta2. Higher values reduce effect   step-size adaptivity bring behavior closer stochastic gradient   descent momentum. Typical values 1e-8 1e-3. Default:   1e-7. alpha initial learning rate. Default: value   learning_rate parameter. epoch_callback function invoked end every epoch. signature : (epoch, n_epochs, coords), : epoch current epoch number (1   n_epochs). n_epochs Number epochs use optimization   embedded coordinates. coords embedded coordinates end current   epoch, matrix dimensions (N, n_components). pca_method Method carry PCA dimensionality reduction pca parameter specified. Allowed values : \"irlba\". Uses prcomp_irlba   irlba package. \"rsvd\". Uses 5 iterations svdr   irlba package.   likely give much faster potentially less accurate results   using \"irlba\". purposes nearest neighbor   calculation coordinates initialization, loss accuracy   seem matter much. \"bigstatsr\". Uses big_randomSVD   bigstatsr   package. SVD methods used bigstatsr may faster   systems without access efficient linear algebra libraries (e.g.   Windows). Note: bigstatsr dependency   uwot: choose use package PCA, must install   . \"svd\". Uses svd SVD.   likely slow smallest datasets. \"auto\" (default). Uses \"irlba\", unless   50   case \"svd\" used. binary_edge_weights TRUE edge weights input graph treated binary (0/1) rather real valued. affects sampling frequency neighbors strategy used PaCMAP method (Wang co-workers, 2020). Practical (Böhm co-workers, 2020) theoretical (Damrich Hamprecht, 2021) work suggests little effect UMAP's performance. dens_scale value 0 1. > 0 output attempts preserve relative local density around observation. uses approximation densMAP method (Narayan co-workers, 2021). larger value dens_scale, greater range output densities used map input densities. option ignored using multiple metric blocks. seed Integer seed use initialize random number generator state. Combined n_sgd_threads = 1 batch = TRUE, give consistent output across multiple runs given installation. Setting value equivalent calling set.seed, may convenient situations call separate function. default set seed. ret_model = TRUE, seed stored output model used set seed inside umap_transform. nn_args list containing additional arguments pass nearest neighbor method. nn_method = \"annoy\", can specify \"n_trees\" \"search_k\", override n_trees search_k parameters. nn_method = \"hnsw\", may specify following arguments: M maximum number neighbors keep vertex. Reasonable values 2 100. Higher values give better recall cost memory. Default value 16. ef_construction positive integer specifying size dynamic list used index construction. higher value provide better results cost longer time build index. Default 200. ef positive integer specifying size dynamic list used search. smaller n_neighbors higher number items index. Default 10. nn_method = \"nndescent\", may specify following arguments: n_trees number trees use random projection forest initialize search. larger number give accurate results cost longer computation time. default NULL means number chosen based number observations X. max_candidates number potential neighbors explore per iteration. default, set n_neighbors 60, whichever smaller. larger number give accurate results cost longer computation time. n_iters number iterations run search. larger number give accurate results cost longer computation time. default, chosen based number observations X. may also need modify convergence criterion delta. delta minimum relative change neighbor graph allowed early stopping. value 0 1. smaller value, smaller amount progress iterations allowed. Default value 0.001 means least 0.1 neighbor graph must updated iteration. init initialize nearest neighbor descent. default set \"tree\" uses random project forest. set \"rand\", random selection used. Usually less accurate using RP trees, high-dimensional cases, may little difference quality initialization random initialization lot faster. set \"rand\", n_trees parameter ignored. pruning_degree_multiplier maximum number edges per node retain search graph, relative n_neighbors. larger value give accurate results cost longer computation time. Default 1.5. parameter affects neighbor search transforming new data umap_transform. epsilon Controls degree back-tracking traversing search graph. Setting 0.0 greedy search back-tracking. larger value give accurate results cost longer computation time. Default 0.1. parameter affects neighbor search transforming new data umap_transform. max_search_fraction Specifies maximum fraction search graph traverse. default, set 1.0, entire graph (.e. items X) may visited. may want set smaller value large dataset (conjunction epsilon) avoid inefficient exhaustive search data X. parameter affects neighbor search transforming new data umap_transform. rng_type type random number generator use optimization. One : \"pcg\". Use PCG random number generator (O'Neill, 2014). \"tausworthe\". Use Tausworthe \"taus88\" generator. \"deterministic\". Use deterministic number generator.  actually random, may provide enough variation negative  sampling give good embedding can provide noticeable speed-. backwards compatibility, default unset choice pcg_rand used (making \"pcg\" effective default).","code":""},{"path":"https://jlmelville.github.io/uwot/reference/umap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Dimensionality Reduction with UMAP — umap","text":"matrix optimized coordinates, : ret_model = TRUE (ret_extra contains     \"model\"), returns list containing extra information can     used add new data existing embedding via     umap_transform. case, coordinates available     list item embedding. NOTE: contents     model list considered stable part     public API, purposely left undocumented. ret_nn = TRUE (ret_extra contains \"nn\"),     returns nearest neighbor data list called nn.     contains one list metric calculated, containing     matrix idx integer ids neighbors; matrix     dist distances. nn list (sub-list) can     used input nn_method parameter. ret_extra contains \"fgraph\", returns high     dimensional fuzzy graph sparse matrix called fgraph, type     dgCMatrix-class. ret_extra contains \"sigma\", returns vector     smooth knn distance normalization terms observation     \"sigma\" vector \"rho\" containing largest     distance locally connected neighbors observation. ret_extra contains \"localr\", returns vector     estimated local radii, sum \"sigma\" \"rho\". returned list contains combined data combination   specifying ret_model, ret_nn ret_extra.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/umap.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Dimensionality Reduction with UMAP — umap","text":"Belkin, M., & Niyogi, P. (2002). Laplacian eigenmaps spectral techniques embedding clustering. Advances neural information processing systems (pp. 585-591). http://papers.nips.cc/paper/1961-laplacian-eigenmaps--spectral-techniques--embedding--clustering.pdf Böhm, J. N., Berens, P., & Kobak, D. (2020). unifying perspective neighbor embeddings along attraction-repulsion spectrum. arXiv preprint arXiv:2007.08902. https://arxiv.org/abs/2007.08902 Damrich, S., & Hamprecht, F. . (2021). UMAP's true loss function. Advances Neural Information Processing Systems, 34. https://proceedings.neurips.cc/paper/2021/hash/2de5d16682c3c35007e4e92982f1a2ba-Abstract.html Dong, W., Moses, C., & Li, K. (2011, March). Efficient k-nearest neighbor graph construction generic similarity measures. Proceedings 20th international conference World Wide Web (pp. 577-586). ACM. doi:10.1145/1963405.1963487 . Kingma, D. P., & Ba, J. (2014). Adam: method stochastic optimization. arXiv preprint arXiv:1412.6980. https://arxiv.org/abs/1412.6980 Malkov, Y. ., & Yashunin, D. . (2018). Efficient robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions pattern analysis machine intelligence, 42(4), 824-836. McInnes, L., Healy, J., & Melville, J. (2018). UMAP: Uniform Manifold Approximation Projection Dimension Reduction arXiv preprint arXiv:1802.03426. https://arxiv.org/abs/1802.03426 Narayan, ., Berger, B., & Cho, H. (2021). Assessing single-cell transcriptomic variability density-preserving data visualization. Nature biotechnology, 39(6), 765-774. doi:10.1038/s41587-020-00801-7 O’Neill, M. E. (2014). PCG: family simple fast space-efficient statistically good algorithms random number generation (Report . HMC-CS-2014-0905). Harvey Mudd College. Tang, J., Liu, J., Zhang, M., & Mei, Q. (2016, April). Visualizing large-scale high-dimensional data. Proceedings 25th International Conference World Wide Web (pp. 287-297). International World Wide Web Conferences Steering Committee. https://arxiv.org/abs/1602.00370 Van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-SNE. Journal Machine Learning Research, 9 (2579-2605). https://www.jmlr.org/papers/v9/vandermaaten08a.html Wang, Y., Huang, H., Rudin, C., & Shaposhnik, Y. (2021). Understanding Dimension Reduction Tools Work: Empirical Approach Deciphering t-SNE, UMAP, TriMap, PaCMAP Data Visualization. Journal Machine Learning Research, 22(201), 1-73. https://www.jmlr.org/papers/v22/20-1061.html","code":""},{"path":"https://jlmelville.github.io/uwot/reference/umap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Dimensionality Reduction with UMAP — umap","text":"","code":"iris30 <- iris[c(1:10, 51:60, 101:110), ]  # Non-numeric columns are automatically removed so you can pass data frames # directly in a lot of cases without pre-processing iris_umap <- umap(iris30, n_neighbors = 5, learning_rate = 0.5, init = \"random\", n_epochs = 20)  # Faster approximation to the gradient and return nearest neighbors iris_umap <- umap(iris30, n_neighbors = 5, approx_pow = TRUE, ret_nn = TRUE, n_epochs = 20)  # Can specify min_dist and spread parameters to control separation and size # of clusters and reuse nearest neighbors for efficiency nn <- iris_umap$nn iris_umap <- umap(iris30, n_neighbors = 5, min_dist = 1, spread = 5, nn_method = nn, n_epochs = 20)  # Supervised dimension reduction using the 'Species' factor column iris_sumap <- umap(iris30,   n_neighbors = 5, min_dist = 0.001, y = iris30$Species,   target_weight = 0.5, n_epochs = 20 )  # Calculate Petal and Sepal neighbors separately (uses intersection of the resulting sets): iris_umap <- umap(iris30, metric = list(   \"euclidean\" = c(\"Sepal.Length\", \"Sepal.Width\"),   \"euclidean\" = c(\"Petal.Length\", \"Petal.Width\") ))"},{"path":"https://jlmelville.github.io/uwot/reference/umap2.html","id":null,"dir":"Reference","previous_headings":"","what":"Dimensionality Reduction with UMAP — umap2","title":"Dimensionality Reduction with UMAP — umap2","text":"Carry dimensionality reduction dataset using Uniform Manifold Approximation Projection (UMAP) method (McInnes et al., 2018).","code":""},{"path":"https://jlmelville.github.io/uwot/reference/umap2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dimensionality Reduction with UMAP — umap2","text":"","code":"umap2(   X,   n_neighbors = 15,   n_components = 2,   metric = \"euclidean\",   n_epochs = NULL,   learning_rate = 1,   scale = FALSE,   init = \"spectral\",   init_sdev = \"range\",   spread = 1,   min_dist = 0.1,   set_op_mix_ratio = 1,   local_connectivity = 1,   bandwidth = 1,   repulsion_strength = 1,   negative_sample_rate = 5,   a = NULL,   b = NULL,   nn_method = NULL,   n_trees = 50,   search_k = 2 * n_neighbors * n_trees,   approx_pow = FALSE,   y = NULL,   target_n_neighbors = n_neighbors,   target_metric = \"euclidean\",   target_weight = 0.5,   pca = NULL,   pca_center = TRUE,   pcg_rand = TRUE,   fast_sgd = FALSE,   ret_model = FALSE,   ret_nn = FALSE,   ret_extra = c(),   n_threads = NULL,   n_sgd_threads = 0,   grain_size = 1,   tmpdir = tempdir(),   verbose = getOption(\"verbose\", TRUE),   batch = TRUE,   opt_args = NULL,   epoch_callback = NULL,   pca_method = NULL,   binary_edge_weights = FALSE,   dens_scale = NULL,   seed = NULL,   nn_args = list(),   rng_type = NULL )"},{"path":"https://jlmelville.github.io/uwot/reference/umap2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dimensionality Reduction with UMAP — umap2","text":"X Input data. Can data.frame, matrix, dist object sparseMatrix. Matrix data frames contain one observation per row. Data frames non-numeric columns removed, although factor columns used explicitly included via metric (see help metric details). Sparse matrices must dgCMatrix format, must also install rnndescent set nn_method = \"nndescent\" X can also NULL pre-computed nearest neighbor data passed nn_method, init \"spca\" \"pca\". n_neighbors size local neighborhood (terms number neighboring sample points) used manifold approximation. Larger values result global views manifold, smaller values result local data preserved. general values range 2 100. n_components dimension space embed . defaults 2 provide easy visualization, can reasonably set integer value range 2 100. metric Type distance metric use find nearest neighbors.  nn_method = \"annoy\" can one : \"euclidean\" (default) \"cosine\" \"manhattan\" \"hamming\" \"correlation\" (distance based Pearson correlation) \"categorical\" (see ) nn_method = \"hnsw\" can one : \"euclidean\" \"cosine\" \"correlation\" rnndescent installed nn_method = \"nndescent\" specified many metrics avaiable, including: \"braycurtis\" \"canberra\" \"chebyshev\" \"dice\" \"hamming\" \"hellinger\" \"jaccard\" \"jensenshannon\" \"kulsinski\" \"rogerstanimoto\" \"russellrao\" \"sokalmichener\" \"sokalsneath\" \"spearmanr\" \"symmetrickl\" \"tsss\" \"yule\" details see package documentation rnndescent. nn_method = \"fnn\", distance metric always \"euclidean\". X data frame matrix, multiple metrics can specified, passing list argument, name item list one metric names . value list item vector giving names integer ids columns included calculation, e.g. metric = list(euclidean = 1:4, manhattan = 5:10). metric calculation results separate fuzzy simplicial set, intersected together produce final set. Metric names can repeated. non-numeric columns removed data frame, safer use column names integer ids. Factor columns can also used specifying metric name \"categorical\". Factor columns treated different numeric columns although multiple factor columns can specified vector, factor column specified processed individually. specify non-factor column, coerced factor. given data block, may override pca pca_center arguments block, providing list one unnamed item containing column names ids, pca pca_center overrides named items, e.g. metric = list(euclidean = 1:4, manhattan = list(5:10, pca_center = FALSE)). exists allow mixed binary real-valued data included PCA applied , centering applied real-valued data (typical apply centering binary data PCA applied). n_epochs Number epochs use optimization embedded coordinates. default, value set 500 datasets containing 10,000 vertices less, 200 otherwise. n_epochs = 0, coordinates determined \"init\" returned. learning_rate Initial learning rate used optimization coordinates. scale Scaling apply X data frame matrix: \"none\" FALSE NULL scaling. \"Z\" \"scale\" TRUE Scale column   zero mean variance 1. \"maxabs\" Center column mean 0, divide   element maximum absolute value entire matrix. \"range\" Range scale entire matrix, smallest   element 0 largest 1. \"colrange\" Scale column range (0,1). UMAP, default \"none\". init Type initialization coordinates. Options : \"spectral\" Spectral embedding using normalized Laplacian    fuzzy 1-skeleton, Gaussian noise added. \"normlaplacian\". Spectral embedding using normalized    Laplacian fuzzy 1-skeleton, without noise. \"random\". Coordinates assigned using uniform random    distribution -10 10. \"lvrandom\". Coordinates assigned using Gaussian    distribution standard deviation 1e-4, used LargeVis    (Tang et al., 2016) t-SNE. \"laplacian\". Spectral embedding using Laplacian Eigenmap    (Belkin Niyogi, 2002). \"pca\". first two principal components PCA    X X data frame, 2-dimensional classical    MDS X class \"dist\". \"spca\". Like \"pca\", dimension scaled    standard deviation 1e-4, give distribution similar    used t-SNE. alias init = \"pca\", init_sdev =    1e-4. \"agspectral\" \"approximate global\" modification    \"spectral\" edges graph value 1,    sets random number edges (negative_sample_rate edges per    vertex) 0.1, approximate effect non-local affinities. matrix initial coordinates. spectral initializations, (\"spectral\", \"normlaplacian\", \"laplacian\", \"agspectral\"), one connected component identified, spectral initialization attempted. Instead PCA-based initialization attempted. verbose = TRUE number connected components logged console. existence multiple connected components implies global view data attained initialization. Increasing value n_neighbors may help. init_sdev non-NULL, scales dimension initialized coordinates (including user-supplied matrix) standard deviation. default, (init_sdev = \"range\"), column initial coordinates range scaled 0-10. Scaling input may help unscaled versions result initial coordinates large inter-point distances outliers. usually results small gradients optimization little progress made layout. Shrinking initial embedding rescaling can help circumstances. Scaling result init = \"pca\" usually recommended init = \"spca\" alias init = \"pca\", init_sdev = 1e-4 spectral initializations scaled versions usually necessary unless using large value n_neighbors (e.g. n_neighbors = 150 higher). spread effective scale embedded points. combination min_dist, determines clustered/clumped embedded points . min_dist effective minimum distance embedded points. Smaller values result clustered/clumped embedding nearby points manifold drawn closer together, larger values result even dispersal points. value set relative spread value, determines scale embedded points spread . set_op_mix_ratio Interpolate (fuzzy) union intersection set operation used combine local fuzzy simplicial sets obtain global fuzzy simplicial sets. fuzzy set operations use product t-norm. value parameter 0.0 1.0; value 1.0 use pure fuzzy union, 0.0 use pure fuzzy intersection. local_connectivity local connectivity required – .e. number nearest neighbors assumed connected local level. higher value connected manifold becomes locally. practice local intrinsic dimension manifold. bandwidth effective bandwidth kernel view algorithm similar Laplacian Eigenmaps. Larger values induce connectivity global view data, smaller values concentrate locally. repulsion_strength Weighting applied negative samples low dimensional embedding optimization. Values higher one result greater weight given negative samples. negative_sample_rate number negative edge/1-simplex samples use per positive edge/1-simplex sample optimizing low dimensional embedding. specific parameters controlling embedding. NULL values set automatically determined min_dist spread. b specific parameters controlling embedding. NULL values set automatically determined min_dist spread. nn_method Method finding nearest neighbors. Options : \"fnn\". Use exact nearest neighbors via     FNN package. \"annoy\" Use approximate nearest neighbors via     RcppAnnoy package. \"hnsw\" Use approximate nearest neighbors     Hierarchical Navigable Small World (HNSW) method (Malkov Yashunin,     2018) via     RcppHNSW package.     RcppHNSW dependency package: option     available installed RcppHNSW . Also,     HNSW supports following arguments metric     target_metric: \"euclidean\", \"cosine\"     \"correlation\". \"nndescent\" Use approximate nearest neighbors     Nearest Neighbor Descent method (Dong et al., 2011) via     rnndescent     package. rnndescent dependency package:     option available installed rnndescent     . default, X less 4,096 vertices, exact nearest neighbors found. Otherwise, approximate nearest neighbors used. may also pass pre-calculated nearest neighbor data argument. must one two formats, either list consisting two elements: \"idx\". n_vertices x n_neighbors matrix   containing integer indexes nearest neighbors X.   vertex considered nearest neighbor, .e.   idx[, 1] == 1:n_vertices. \"dist\". n_vertices x n_neighbors matrix   containing distances nearest neighbors. sparse distance matrix type dgCMatrix, dimensions n_vertices x n_vertices. Distances arranged column, .e. non-zero entry row j ith column indicates jth observation X nearest neighbor ith observation distance given value element. n_neighbors parameter ignored using precomputed nearest neighbor data. using sparse distance matrix input, column can contain different number neighbors. n_trees Number trees build constructing nearest neighbor index. trees specified, larger index, better results. search_k, determines accuracy Annoy nearest neighbor search. used nn_method \"annoy\". Sensible values 10 100. search_k Number nodes search neighbor retrieval. larger k, accurate results, longer search takes. n_trees, determines accuracy Annoy nearest neighbor search. used nn_method \"annoy\". approx_pow TRUE, use approximation power function UMAP gradient, https://martin.ankerl.com/2012/01/25/optimized-approximative-pow--c--cpp/. Ignored dens_scale non-NULL. y Optional target data supervised dimension reduction. Can vector, matrix data frame. Use target_metric parameter specify metrics use, using syntax metric. Usually either single numeric factor column used, complex formats possible. following types allowed: Factor columns length X. NA     allowed observation unknown level, case     UMAP operates form semi-supervised learning. column     treated separately. Numeric data. NA allowed case. Use     parameter target_n_neighbors set number neighbors used     y. unset, n_neighbors used. Unlike factors,     numeric columns grouped one block unless target_metric     specifies otherwise. example, wish columns     b treated separately, specify     target_metric = list(euclidean = \"\", euclidean = \"b\"). Otherwise,     data effectively treated matrix two columns. Nearest neighbor data, consisting list two matrices,     idx dist. represent precalculated nearest     neighbor indices distances, respectively.     format expected precalculated data     nn_method. format assumes underlying data     numeric vector. user-supplied value target_n_neighbors     parameter ignored case, number columns     matrices used value. Multiple nearest neighbor data using     different metrics can supplied passing list lists. Unlike X, factor columns included y automatically used. target_n_neighbors Number nearest neighbors use construct target simplicial set. Default value n_neighbors. Applies y non-NULL numeric. target_metric metric used measure distance y using supervised dimension reduction. Used y numeric. target_weight Weighting factor data topology target topology. value 0.0 weights entirely data, value 1.0 weights entirely target. default 0.5 balances weighting equally data target. applies y non-NULL. pca set positive integer value, reduce data number columns using PCA. applied distance metric \"hamming\", dimensions data larger number specified (.e. number rows columns must larger value parameter). > 100 columns data frame matrix, reducing number columns way may substantially increase performance nearest neighbor search cost potential decrease accuracy. many t-SNE applications, value 50 recommended, although guarantee appropriate settings. pca_center TRUE, center columns X carrying PCA. binary data, recommended set FALSE. pcg_rand TRUE, use PCG random number generator (O'Neill, 2014) optimization. Otherwise, use faster (probably less statistically good) Tausworthe \"taus88\" generator. default TRUE. parameter superseded rng_type – set, rng_type takes precedence. fast_sgd TRUE, following combination parameters set: pcg_rand = TRUE, n_sgd_threads = \"auto\" approx_pow = TRUE. default FALSE. Setting TRUE speed stochastic optimization phase, give potentially less accurate embedding, exactly reproducible even fixed seed. visualization, fast_sgd = TRUE give perfectly good results. generic dimensionality reduction, safer leave fast_sgd = FALSE. fast_sgd = TRUE, user-supplied values pcg_rand, n_sgd_threads, approx_pow ignored. ret_model TRUE, return extra data can used add new data existing embedding via umap_transform. embedded coordinates returned list item embedding. FALSE, just return coordinates. parameter can used conjunction ret_nn ret_extra. Note settings incompatible production UMAP model: external neighbor data (passed via list nn_method), factor columns included via metric parameter. latter case, model produced based numeric data. transformation using new data possible, factor columns new data ignored. Note setting ret_model = TRUE forces use approximate nearest neighbors method. small datasets otherwise use exact nearest neighbor calculations, setting ret_model = TRUE means different results may returned small datasets terms returned nearest neighbors (requested) final embedded coordinates, compared ret_model = FALSE, even random number seed fixed. avoid , explicitly set nn_method = \"annoy\" ret_model = FALSE case. ret_nn TRUE, addition embedding, also return nearest neighbor data can used input nn_method avoid overhead repeatedly calculating nearest neighbors manipulating unrelated parameters (e.g. min_dist, n_epochs, init). See \"Value\" section names list items. FALSE, just return coordinates. Note nearest neighbors sensitive data scaling, wary reusing nearest neighbor data modifying scale parameter. parameter can used conjunction ret_model ret_extra. ret_extra vector indicating extra data return. May contain combination following strings: \"model\" setting ret_model = TRUE. \"nn\" setting ret_nn = TRUE. \"fgraph\" high dimensional fuzzy graph (.e. fuzzy     simplicial set merged local views input data). graph     returned sparse symmetric N x N matrix class     dgCMatrix-class, non-zero entry (, j) gives     membership strength edge connecting vertex vertex j.     can considered analogous input probability (similarity     affinity) used t-SNE LargeVis. Note graph     sparsified removing edges sufficiently low membership strength     sampled probabilistic edge sampling     employed optimization therefore number non-zero elements     matrix dependent n_epochs.     interested fuzzy input graph (e.g. clustering), setting     n_epochs = 0 avoid sparsifying.     aware setting `binary_edge_weights = TRUE` affect     graph (non-zero edge weights 1). \"sigma\" normalization value observation     dataset constructing smoothed distances     neighbors. gives sense local density     observation high dimensional space: higher values     sigma indicate higher dispersion lower density. n_threads Number threads use (except stochastic gradient descent). Default half number concurrent threads supported system. nearest neighbor search, applies nn_method = \"annoy\". n_threads > 1, Annoy index temporarily written disk location determined tempfile. n_sgd_threads Number threads use stochastic gradient descent. set > 1, aware batch = FALSE, results reproducible, even set.seed called fixed seed running. Set \"auto\" use value n_threads. Default use one thread, unless batch = TRUE case \"auto\" used. grain_size minimum amount work thread. value set high enough, less n_threads n_sgd_threads used processing, might give performance improvement overhead thread management context switching outweighing improvement due concurrent processing. left default (1) work spread evenly threads specified. tmpdir Temporary directory store nearest neighbor indexes nearest neighbor search. Default tempdir. index written disk n_threads > 1 nn_method = \"annoy\"; otherwise, parameter ignored. verbose TRUE, log details console. batch TRUE, embedding coordinates updated end epoch rather epoch. batch mode, results reproducible fixed random seed even n_sgd_threads > 1, cost slightly higher memory use. may also modify learning_rate increase n_epochs, whether provides speed increase single-threaded optimization likely dataset hardware-dependent. opt_args list optimizer parameters, used batch = TRUE. default optimization method used Adam (Kingma Ba, 2014). method optimization method use. Either \"adam\"   \"sgd\" (stochastic gradient descent). Default: \"adam\". beta1 (Adam ). weighting parameter   exponential moving average first moment estimator. Effectively   momentum parameter. floating point value 0 1.   Higher values can smooth oscillatory updates poorly-conditioned   situations may allow larger learning_rate   specified, high can cause divergence. Default: 0.5. beta2 (Adam ). weighting parameter   exponential moving average uncentered second moment estimator.   floating point value 0 1. Controls degree   adaptivity step-size. Higher values put weight previous   time steps. Default: 0.9. eps (Adam ). Intended small value prevent   division zero, practice can also affect convergence due   interaction beta2. Higher values reduce effect   step-size adaptivity bring behavior closer stochastic gradient   descent momentum. Typical values 1e-8 1e-3. Default:   1e-7. alpha initial learning rate. Default: value   learning_rate parameter. epoch_callback function invoked end every epoch. signature : (epoch, n_epochs, coords), : epoch current epoch number (1   n_epochs). n_epochs Number epochs use optimization   embedded coordinates. coords embedded coordinates end current   epoch, matrix dimensions (N, n_components). pca_method Method carry PCA dimensionality reduction pca parameter specified. Allowed values : \"irlba\". Uses prcomp_irlba   irlba package. \"rsvd\". Uses 5 iterations svdr   irlba package.   likely give much faster potentially less accurate results   using \"irlba\". purposes nearest neighbor   calculation coordinates initialization, loss accuracy   seem matter much. \"bigstatsr\". Uses big_randomSVD   bigstatsr   package. SVD methods used bigstatsr may faster   systems without access efficient linear algebra libraries (e.g.   Windows). Note: bigstatsr dependency   uwot: choose use package PCA, must install   . \"svd\". Uses svd SVD.   likely slow smallest datasets. \"auto\" (default). Uses \"irlba\", unless   50   case \"svd\" used. binary_edge_weights TRUE edge weights input graph treated binary (0/1) rather real valued. affects sampling frequency neighbors strategy used PaCMAP method (Wang co-workers, 2020). Practical (Böhm co-workers, 2020) theoretical (Damrich Hamprecht, 2021) work suggests little effect UMAP's performance. dens_scale value 0 1. > 0 output attempts preserve relative local density around observation. uses approximation densMAP method (Narayan co-workers, 2021). larger value dens_scale, greater range output densities used map input densities. option ignored using multiple metric blocks. seed Integer seed use initialize random number generator state. Combined n_sgd_threads = 1 batch = TRUE, give consistent output across multiple runs given installation. Setting value equivalent calling set.seed, may convenient situations call separate function. default set seed. ret_model = TRUE, seed stored output model used set seed inside umap_transform. nn_args list containing additional arguments pass nearest neighbor method. nn_method = \"annoy\", can specify \"n_trees\" \"search_k\", override n_trees search_k parameters. nn_method = \"hnsw\", may specify following arguments: M maximum number neighbors keep vertex. Reasonable values 2 100. Higher values give better recall cost memory. Default value 16. ef_construction positive integer specifying size dynamic list used index construction. higher value provide better results cost longer time build index. Default 200. ef positive integer specifying size dynamic list used search. smaller n_neighbors higher number items index. Default 10. nn_method = \"nndescent\", may specify following arguments: n_trees number trees use random projection forest initialize search. larger number give accurate results cost longer computation time. default NULL means number chosen based number observations X. max_candidates number potential neighbors explore per iteration. default, set n_neighbors 60, whichever smaller. larger number give accurate results cost longer computation time. n_iters number iterations run search. larger number give accurate results cost longer computation time. default, chosen based number observations X. may also need modify convergence criterion delta. delta minimum relative change neighbor graph allowed early stopping. value 0 1. smaller value, smaller amount progress iterations allowed. Default value 0.001 means least 0.1 neighbor graph must updated iteration. init initialize nearest neighbor descent. default set \"tree\" uses random project forest. set \"rand\", random selection used. Usually less accurate using RP trees, high-dimensional cases, may little difference quality initialization random initialization lot faster. set \"rand\", n_trees parameter ignored. pruning_degree_multiplier maximum number edges per node retain search graph, relative n_neighbors. larger value give accurate results cost longer computation time. Default 1.5. parameter affects neighbor search transforming new data umap_transform. epsilon Controls degree back-tracking traversing search graph. Setting 0.0 greedy search back-tracking. larger value give accurate results cost longer computation time. Default 0.1. parameter affects neighbor search transforming new data umap_transform. max_search_fraction Specifies maximum fraction search graph traverse. default, set 1.0, entire graph (.e. items X) may visited. may want set smaller value large dataset (conjunction epsilon) avoid inefficient exhaustive search data X. parameter affects neighbor search transforming new data umap_transform. rng_type type random number generator use optimization. One : \"pcg\". Use PCG random number generator (O'Neill, 2014). \"tausworthe\". Use Tausworthe \"taus88\" generator. \"deterministic\". Use deterministic number generator.  actually random, may provide enough variation negative  sampling give good embedding can provide noticeable speed-. backwards compatibility, default unset choice pcg_rand used (making \"pcg\" effective default).","code":""},{"path":"https://jlmelville.github.io/uwot/reference/umap2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Dimensionality Reduction with UMAP — umap2","text":"matrix optimized coordinates, : ret_model = TRUE (ret_extra contains     \"model\"), returns list containing extra information can     used add new data existing embedding via     umap_transform. case, coordinates available     list item embedding. NOTE: contents     model list considered stable part     public API, purposely left undocumented. ret_nn = TRUE (ret_extra contains \"nn\"),     returns nearest neighbor data list called nn.     contains one list metric calculated, containing     matrix idx integer ids neighbors; matrix     dist distances. nn list (sub-list) can     used input nn_method parameter. ret_extra contains \"fgraph\", returns high     dimensional fuzzy graph sparse matrix called fgraph, type     dgCMatrix-class. ret_extra contains \"sigma\", returns vector     smooth knn distance normalization terms observation     \"sigma\" vector \"rho\" containing largest     distance locally connected neighbors observation. ret_extra contains \"localr\", returns vector     estimated local radii, sum \"sigma\" \"rho\". returned list contains combined data combination   specifying ret_model, ret_nn ret_extra.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/umap2.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Dimensionality Reduction with UMAP — umap2","text":"function behaves like umap except updated defaults make behave like Python implementation added umap without breaking backwards compatibility. addition: RcppHNSW   installed, used preference Annoy compatible metric   requested. RcppHNSW present,  rnndescent   installed, used preference Annoy compatible metric   requested. batch = TRUE default n_sgd_threads set  value n_threads. input data X sparse matrix, interpreted  similarly dense matrix dataframe, distance matrix.  requires rnndescent package installed.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/umap2.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Dimensionality Reduction with UMAP — umap2","text":"Belkin, M., & Niyogi, P. (2002). Laplacian eigenmaps spectral techniques embedding clustering. Advances neural information processing systems (pp. 585-591). http://papers.nips.cc/paper/1961-laplacian-eigenmaps--spectral-techniques--embedding--clustering.pdf Böhm, J. N., Berens, P., & Kobak, D. (2020). unifying perspective neighbor embeddings along attraction-repulsion spectrum. arXiv preprint arXiv:2007.08902. https://arxiv.org/abs/2007.08902 Damrich, S., & Hamprecht, F. . (2021). UMAP's true loss function. Advances Neural Information Processing Systems, 34. https://proceedings.neurips.cc/paper/2021/hash/2de5d16682c3c35007e4e92982f1a2ba-Abstract.html Dong, W., Moses, C., & Li, K. (2011, March). Efficient k-nearest neighbor graph construction generic similarity measures. Proceedings 20th international conference World Wide Web (pp. 577-586). ACM. doi:10.1145/1963405.1963487 . Kingma, D. P., & Ba, J. (2014). Adam: method stochastic optimization. arXiv preprint arXiv:1412.6980. https://arxiv.org/abs/1412.6980 Malkov, Y. ., & Yashunin, D. . (2018). Efficient robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions pattern analysis machine intelligence, 42(4), 824-836. McInnes, L., Healy, J., & Melville, J. (2018). UMAP: Uniform Manifold Approximation Projection Dimension Reduction arXiv preprint arXiv:1802.03426. https://arxiv.org/abs/1802.03426 Narayan, ., Berger, B., & Cho, H. (2021). Assessing single-cell transcriptomic variability density-preserving data visualization. Nature biotechnology, 39(6), 765-774. doi:10.1038/s41587-020-00801-7 O’Neill, M. E. (2014). PCG: family simple fast space-efficient statistically good algorithms random number generation (Report . HMC-CS-2014-0905). Harvey Mudd College. Tang, J., Liu, J., Zhang, M., & Mei, Q. (2016, April). Visualizing large-scale high-dimensional data. Proceedings 25th International Conference World Wide Web (pp. 287-297). International World Wide Web Conferences Steering Committee. https://arxiv.org/abs/1602.00370 Van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-SNE. Journal Machine Learning Research, 9 (2579-2605). https://www.jmlr.org/papers/v9/vandermaaten08a.html Wang, Y., Huang, H., Rudin, C., & Shaposhnik, Y. (2021). Understanding Dimension Reduction Tools Work: Empirical Approach Deciphering t-SNE, UMAP, TriMap, PaCMAP Data Visualization. Journal Machine Learning Research, 22(201), 1-73. https://www.jmlr.org/papers/v22/20-1061.html","code":""},{"path":"https://jlmelville.github.io/uwot/reference/umap2.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Dimensionality Reduction with UMAP — umap2","text":"","code":"iris30 <- iris[c(1:10, 51:60, 101:110), ] iris_umap <- umap2(iris30, n_neighbors = 5)"},{"path":"https://jlmelville.github.io/uwot/reference/umap_transform.html","id":null,"dir":"Reference","previous_headings":"","what":"Add New Points to an Existing Embedding — umap_transform","title":"Add New Points to an Existing Embedding — umap_transform","text":"Carry embedding new data using existing embedding. Requires using result calling umap tumap ret_model = TRUE.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/umap_transform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add New Points to an Existing Embedding — umap_transform","text":"","code":"umap_transform(   X = NULL,   model = NULL,   nn_method = NULL,   init_weighted = TRUE,   search_k = NULL,   tmpdir = tempdir(),   n_epochs = NULL,   n_threads = NULL,   n_sgd_threads = 0,   grain_size = 1,   verbose = FALSE,   init = \"weighted\",   batch = NULL,   learning_rate = NULL,   opt_args = NULL,   epoch_callback = NULL,   ret_extra = NULL,   seed = NULL )"},{"path":"https://jlmelville.github.io/uwot/reference/umap_transform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add New Points to an Existing Embedding — umap_transform","text":"X new data transformed, either matrix data frame. Must columns order input data used generate model. model Data associated existing embedding. nn_method Optional pre-calculated nearest neighbor data. two supported formats. first list consisting two elements: \"idx\". n_vertices x n_neighbors matrix   n_vertices number observations X. contents   matrix integer indexes data used generate   model, n_neighbors-nearest neighbors   data transformed. \"dist\". n_vertices x n_neighbors matrix   containing distances nearest neighbors. second supported format sparse distance matrix type dgCMatrix, dimensions n_model_vertices x n_vertices. n_model_vertices number observations original data generated model. Distances arranged column, .e. non-zero entry row j ith column indicates jth observation original data used generate model nearest neighbor ith observation new data, distance given value element. format, different number neighbors allowed observation, .e. column can contain different number non-zero values. Multiple nearest neighbor data (e.g. two different pre-calculated metrics) can passed passing list containing nearest neighbor data lists items. init_weighted TRUE, initialize embedded coordinates X using weighted average coordinates nearest neighbors original embedding model, weights used edge weights UMAP smoothed knn distances. Otherwise, use un-weighted average. parameter deprecated removed version 1.0 package. Use init parameter replacement, replacing init_weighted = TRUE init = \"weighted\" init_weighted = FALSE init = \"average\". search_k Number nodes search neighbor retrieval. larger k, accurate results, longer search takes. Default value used building model used. tmpdir Temporary directory store nearest neighbor indexes nearest neighbor search. Default tempdir. index written disk n_threads > 1; otherwise, parameter ignored. n_epochs Number epochs use optimization embedded coordinates. value 30 - 100 reasonable trade speed thoroughness. default, value set one third number epochs used build model. n_threads Number threads use, (except stochastic gradient descent). Default half number concurrent threads supported system. n_sgd_threads Number threads use stochastic gradient descent. set > 1, aware batch = FALSE, results reproducible, even set.seed called fixed seed running. Set \"auto\" use value n_threads. grain_size Minimum batch size multithreading. number items process thread falls number, threads used. Used conjunction n_threads n_sgd_threads. verbose TRUE, log details console. init initialize transformed coordinates. One : \"weighted\" (default). Use weighted average     coordinates nearest neighbors original embedding     model, weights used edge weights UMAP     smoothed knn distances. Equivalent init_weighted = TRUE. \"average\". Use mean average coordinates     nearest neighbors original embedding model.     Equivalent init_weighted = FALSE. matrix user-specified input coordinates, must     dimensions (nrow(X), ncol(model$embedding)). parameter used preference init_weighted. batch TRUE, embedding coordinates updated end epoch rather epoch. batch mode, results reproducible fixed random seed even n_sgd_threads > 1, cost slightly higher memory use. may also modify learning_rate increase n_epochs, whether provides speed increase single-threaded optimization likely dataset hardware-dependent. NULL, transform use value provided model, available. Default: FALSE. learning_rate Initial learning rate used optimization coordinates. overrides value associated model. left unspecified circumstances. opt_args list optimizer parameters, used batch = TRUE. default optimization method used Adam (Kingma Ba, 2014). method optimization method use. Either \"adam\"   \"sgd\" (stochastic gradient descent). Default: \"adam\". beta1 (Adam ). weighting parameter   exponential moving average first moment estimator. Effectively   momentum parameter. floating point value 0 1.   Higher values can smooth oscillatory updates poorly-conditioned   situations may allow larger learning_rate   specified, high can cause divergence. Default: 0.5. beta2 (Adam ). weighting parameter   exponential moving average uncentered second moment estimator.   floating point value 0 1. Controls degree   adaptivity step-size. Higher values put weight previous   time steps. Default: 0.9. eps (Adam ). Intended small value prevent   division zero, practice can also affect convergence due   interaction beta2. Higher values reduce effect   step-size adaptivity bring behavior closer stochastic gradient   descent momentum. Typical values 1e-8 1e-3. Default:   1e-7. alpha initial learning rate. Default: value   learning_rate parameter. NULL, transform use value provided model, available. epoch_callback function invoked end every epoch. signature : (epoch, n_epochs, coords, fixed_coords), : epoch current epoch number (1   n_epochs). n_epochs Number epochs use optimization   embedded coordinates. coords embedded coordinates end current   epoch, matrix dimensions (N, n_components). fixed_coords originally embedded coordinates   model. fixed change. matrix dimensions   (Nmodel, n_components) Nmodel number   observations original data. ret_extra vector indicating extra data return. May contain combination following strings: \"fgraph\" high dimensional fuzzy graph (.e. fuzzy     simplicial set merged local views input data). graph     returned sparse matrix class dgCMatrix-class     dimensions NX x Nmodel, NX number     items data transform X, NModel     number items data used build UMAP model.     non-zero entry (, j) gives membership strength edge     connecting vertex representing ith item X     jth item data used build model. Note     graph sparsified removing edges sufficiently low     membership strength sampled probabilistic     edge sampling employed optimization therefore number     non-zero elements matrix dependent n_epochs.     interested fuzzy input graph (e.g. clustering),     setting n_epochs = 0 avoid sparsifying. \"nn\" nearest neighbor graph X respect    observations model. graph returned    list two items: idx matrix indices, many rows    items X many columns nearest    neighbors computed (value determined model).    indices rows data used build    model, necessarily much use unless    access data. second item, dist matrix    equivalent distances, dimensions idx. seed Integer seed use initialize random number generator state. Combined n_sgd_threads = 1 batch = TRUE, give consistent output across multiple runs given installation. Setting value equivalent calling set.seed, may convenient situations call separate function. default set seed, case function uses behavior specified supplied model: model specifies seed, model seed used seed random number generator, results still consistent (n_sgd_threads = 1). want force seed set, even set model, set seed = FALSE.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/umap_transform.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add New Points to an Existing Embedding — umap_transform","text":"matrix coordinates X transformed space   model, ret_extra specified, list   containing: embedding matrix optimized coordinates. ret_extra contains \"fgraph\", item     name containing high-dimensional fuzzy graph sparse matrix,     type dgCMatrix-class. ret_extra contains \"sigma\", returns vector     smooth knn distance normalization terms observation     \"sigma\" vector \"rho\" containing largest     distance locally connected neighbors observation. ret_extra contains \"localr\", item     name containing vector estimated local radii, sum     \"sigma\" \"rho\". ret_extra contains \"nn\", item name     containing nearest neighbors item X (respect     items created model).","code":""},{"path":"https://jlmelville.github.io/uwot/reference/umap_transform.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add New Points to an Existing Embedding — umap_transform","text":"Note settings incompatible production UMAP model via umap: external neighbor data (passed via list argument nn_method parameter), factor columns included UMAP calculation via metric parameter. latter case, model produced based numeric data. transformation possible, factor columns new data ignored.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/umap_transform.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add New Points to an Existing Embedding — umap_transform","text":"","code":"iris_train <- iris[1:100, ] iris_test <- iris[101:150, ]  # You must set ret_model = TRUE to return extra data needed iris_train_umap <- umap(iris_train, ret_model = TRUE) iris_test_umap <- umap_transform(iris_test, iris_train_umap)"},{"path":"https://jlmelville.github.io/uwot/reference/unload_uwot.html","id":null,"dir":"Reference","previous_headings":"","what":"Unload a Model — unload_uwot","title":"Unload a Model — unload_uwot","text":"Unloads UMAP model. prevents model used umap_transform, allows temporary working directory associated saving loading model removed.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/unload_uwot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Unload a Model — unload_uwot","text":"","code":"unload_uwot(model, cleanup = TRUE, verbose = FALSE)"},{"path":"https://jlmelville.github.io/uwot/reference/unload_uwot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Unload a Model — unload_uwot","text":"model UMAP model create umap. cleanup TRUE, attempt delete temporary working directory used either save load model. verbose TRUE, log information console.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/reference/unload_uwot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Unload a Model — unload_uwot","text":"","code":"iris_train <- iris[c(1:10, 51:60), ] iris_test <- iris[100:110, ]  # create model model <- umap(iris_train, ret_model = TRUE, n_epochs = 20)  # save without unloading: this leaves behind a temporary working directory model_file <- tempfile(\"iris_umap\") model <- save_uwot(model, file = model_file)  # The model can continue to be used test_embedding <- umap_transform(iris_test, model)  # To manually unload the model from memory when finished and to clean up # the working directory (this doesn't touch your model file) unload_uwot(model)  # At this point, model cannot be used with umap_transform, this would fail: # test_embedding2 <- umap_transform(iris_test, model)  # restore the model: this also creates a temporary working directory model2 <- load_uwot(file = model_file) test_embedding2 <- umap_transform(iris_test, model2)  # Unload and clean up the loaded model temp directory unload_uwot(model2)  # clean up the model file unlink(model_file)  # save with unloading: this deletes the temporary working directory but # doesn't allow the model to be re-used model3 <- umap(iris_train, ret_model = TRUE, n_epochs = 20) model_file3 <- tempfile(\"iris_umap\") model3 <- save_uwot(model3, file = model_file3, unload = TRUE)"},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-development-version","dir":"Changelog","previous_headings":"","what":"New features:","title":"uwot (development version)","text":"New parameter: rng_type. used favor boolean pcg_rand parameter, although pcg_rand still work backwards compatibility. New negative sampling option: set rng_type = \"deterministic\" use deterministic sampling vertices optimization phase. give qualitatively similar results using real PRNG, advantage faster giving reproducible output. feature inspired comment Leland McInnes Reddit.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-development-version","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot (development version)","text":"Setting num_threads directly umap2 result number SGD threads updated value batch = TRUE, . Despite assertions contrary version 0.2.1, umap_transform continued return fuzzy graph transposed form. Thank PedroMilanezAlmeida reopening issue (https://github.com/jlmelville/uwot/issues/118). Relative paths used save model. Thank Wouter van der Bijl bug report (https://github.com/jlmelville/uwot/issues/131) suggested fix. repulsion_strength silently ignored used tumap umap2 = 1, b = 1. Ignoring setting purpose, documented anywhere. repulsion_strength now compatible settings.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-022","dir":"Changelog","previous_headings":"","what":"uwot 0.2.2","title":"uwot 0.2.2","text":"CRAN release: 2024-04-21","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-2-2","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.2.2","text":"RSpectra now required dependency (). required dependency version 0.1.12, became optional (irlba used place). However, problems interactions current version irlba ABI change Matrix package means ’s hard downstream packages users build uwot without re-installing Matrix irlba source, may option people. Also causing CRAN check error. changed tests, examples vignettes use RSpectra explicitly, test irlba code-paths necessary. See https://github.com/jlmelville/uwot/issues/115 links therein details.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-021","dir":"Changelog","previous_headings":"","what":"uwot 0.2.1","title":"uwot 0.2.1","text":"CRAN release: 2024-04-15","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-2-1","dir":"Changelog","previous_headings":"","what":"New features:","title":"uwot 0.2.1","text":"HNSW approximate nearest neighbor search algorithm now supported via RcppHNSW package. Set nn_method = \"hnsw\" use . behavior method can controlled new nn_args parameter, list may contain M, ef_construction ef. See hnswlib library’s ALGO_PARAMS documentation details parameters. Although typically faster Annoy (given accuracy), aware supported metric values \"euclidean\", \"cosine\" \"correlation\". Finally, RcppHNSW suggested package, requirement, need install (e.g. via install.packages(\"RcppHNSW\")). Also see article HNSW uwot documentation. nearest neighbor descent approximate nearest neighbor search algorithm now supported via rnndescent package. Set nn_method = \"nndescent\" use . behavior method can controlled new nn_args parameter. many supported metrics possible parameters can set nn_args, please see article nearest neighbor descent uwot documentation, also rnndescent package’s documentation details. rnndescent suggested package, requirement, need install (e.g. via install.packages(\"rnndescent\")). New function: umap2, acts like umap modified defaults, reflecting experience UMAP correcting small mistakes. See umap2 article details.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-2-1","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.2.1","text":"init_sdev = \"range\" caused error user-supplied init matrix. Transforming new data correlation metric actually using cosine metric saved reloaded model. Thank Holly Hall report helpful detective work (https://github.com/jlmelville/uwot/issues/117). umap_transform fail new data transformed scaled:center scaled:scale attributes set (e.g. applying scale function). asked umap_transform return fuzzy graph ( ret_extra = c(\"fgraph\")), transposed batch = TRUE, n_epochs = 0. Thank PedroMilanezAlmeida reporting (https://github.com/jlmelville/uwot/issues/118). Setting n_sgd_threads = \"auto\" umap_transform caused crash. warning emitted due specific enough dist class meant may particularly affecting Seurat users. Thank AndiMunteanu reporting (suggesting solution) (https://github.com/jlmelville/uwot/issues/121).","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-0116","dir":"Changelog","previous_headings":"","what":"uwot 0.1.16","title":"uwot 0.1.16","text":"CRAN release: 2023-06-29","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-16","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.16","text":"small change header file required fully support next version RcppAnnoy. Thank Dirk Eddelbuettel PR (https://github.com/jlmelville/uwot/issues/112).","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-0115","dir":"Changelog","previous_headings":"","what":"uwot 0.1.15","title":"uwot 0.1.15","text":"CRAN release: 2023-06-26","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-1-15","dir":"Changelog","previous_headings":"","what":"New features:","title":"uwot 0.1.15","text":"New function: optimize_graph_layout. Use produce optimized output coordinates reflect input similarity graph (produced similarity_graph function. similarity_graph followed optimize_graph_layout running umap, purpose functions allow flexibility decoupling generating nearest neighbor graph optimizing low-dimensional approximation . Based request user Chengwei94 (https://github.com/jlmelville/uwot/issues/98). New functions: simplicial_set_union simplicial_set_intersect. allow combination different fuzzy graph representations dataset single fuzzy graph using UMAP simplicial set operations. Based request Python UMAP issues tracker user Dhar xion. New parameter umap_transform: ret_extra. works like equivalent parameter umap, character vector specifying extra information like returned addition embedding, case list returned embedding member containing optimized coordinates. Supported values \"fgraph\", \"nn\", \"sigma\" \"localr\". Based request user PedroMilanezAlmeida (https://github.com/jlmelville/uwot/issues/104). New parameter umap, tumap umap_transform: seed. equivalent calling set.seed internally, hence help reproducibility. chosen seed exported ret_model = TRUE umap_transform use seed present, need specify umap_transform want change seed. default behavior remains modify random number state. Based request SuhasSrinivasan (https://github.com/jlmelville/uwot/issues/110).","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-15","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.15","text":"new setting init_sdev: set init_sdev = \"range\" initial coordinates range-scaled column takes values 0-10. pre-processing added Python UMAP package point uwot began development probably always used default init = \"spectral\" setting. However, set default maintain backwards compatibility older versions uwot. ret_extra = c(\"sigma\") now supported lvish. Gaussian bandwidths returned sigma vector. addition, vector intrinsic dimensionalities estimated point using analytical expression finite difference method given Lee co-workers returned dint vector. min_dist spread parameters now returned model umap run ret_model = TRUE. just documentation purposes, values used directly model umap_transform. parameters b set directly invoking umap, min_dist spread set NULL returned model. feature added response question kjiang18 (https://github.com/jlmelville/uwot/issues/95). new checks NA values input data added. Also warning emitted n_components seems set high. n_components greater n_neighbors umap_transform crash R session. Thank ChVav reporting (https://github.com/jlmelville/uwot/issues/102). Using umap_transform model dens_scale set cause segmentation fault, destroying session. Even didn’t give entirely artifactual “ring” structure. Thank FemkeSmit reporting providing assistance diagnosing underlying cause (https://github.com/jlmelville/uwot/issues/103). set binary_edge_weights = TRUE, setting exported ret_model = TRUE, therefore respected umap_transform. now fixed, need regenerate models used binary edge weights. rdoc init param said multiple disconnected components, spectral initialization attempt merge multiple sub-graphs. true: actually, spectral initialization abandoned favor PCA. documentation updated reflect true state affairs. idea thinking . load_model save_model didn’t work Windows 7 due version tar handles drive letters. Thank mytarmail report (https://github.com/jlmelville/uwot/issues/109). Warn initial coordinates large scale (standard deviation > 10.0), can lead small gradients poor optimization. Thank SuhasSrinivasan report (https://github.com/jlmelville/uwot/issues/110). change accommodate forthcoming version RcppAnnoy. Thank Dirk Eddelbuettel PR (https://github.com/jlmelville/uwot/issues/111).","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-0114","dir":"Changelog","previous_headings":"","what":"uwot 0.1.14","title":"uwot 0.1.14","text":"CRAN release: 2022-08-22","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-1-14","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.1.14","text":"New function: similarity_graph. interested high-dimensional graph/fuzzy simplicial set representation input data, don’t care low dimensional approximation, similarity_graph function offers similar API umap, neither initialization optimization low-dimensional coordinates performed. return value returned results list fgraph member provided ret_extra = c(\"fgraph\"). Compared getting result via running umap, function bit convenient use, makes intention clearer discarding embedding, saves small amount time. t-SNE/LargeVis similarity graph can returned setting method = \"largevis\".","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-14","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.14","text":"model generated without using pre-generated nearest neighbors, couldn’t use umap_transform pre-generated nearest neighbors (also error message completely useless). Thank AustinHartman reporting (https://github.com/jlmelville/uwot/issues/97).","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-0113","dir":"Changelog","previous_headings":"","what":"uwot 0.1.13","title":"uwot 0.1.13","text":"CRAN release: 2022-08-16 resubmission 0.1.12 internal function (fuzzy_simplicial_set) refactored behave like previous versions. change breaking behavior CRAN package bbknnR.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-1-12","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.1.12","text":"New parameter: dens_weight. set value 0 1, attempt made include relative local densities input data output coordinates. approximation densMAP method. large value dens_weight use larger range output densities reflect input data. data spread , reduce value dens_weight. information see documentation uwot repo. New parameter: binary_edge_weights. set TRUE, instead smoothed knn distances, non-zero edge weights value 1. PaCMAP works practical theoretical reasons believe won’t big effect UMAP can try . \"sigma\": return value contain sigma entry, vector smooth knn distance scaling normalization factors, one observation input data. small value indicates high density points local neighborhood observation. lvish equivalent bandwidths calculated input perplexity returned. also, vector rho exported, distance nearest neighbor number neighbors specified local_connectivity. applies umap tumap. \"localr\": exports vector local radii, sum sigma rho used scale output coordinates dens_weight set. Even using dens_weight, visualizing output coordinates using color scale based value localr can reveal regions input data different densities. functions umap tumap : new data type precomputed nearest neighbor data passed nn_method parameter: may use sparse distance matrix format dgCMatrix dimensions N x N N number observations input data. Distances arranged column, .e. non-zero entry row j ith column indicates jth observation input data nearest neighbor ith observation distance given value element. Note different format sparse distance matrix can passed input X: notably, matrix assumed symmetric. Unlike input formats, may different number neighbors observation (must least one neighbor defined per observation). umap_transform can also take sparse distance matrix nn_method parameter precomputed nearest neighbor data used generate initial model. format nn_method umap. distances arranged columns, expected dimensions sparse matrix N_model x N_new N_model number observations original data N_new number observations data transformed.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-12","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.12","text":"Models couldn’t re-saved loading. Thank ilyakorsunsky reporting (https://github.com/jlmelville/uwot/issues/88). RSpectra now ‘Suggests’, rather ‘Imports’. RSpectra installed, used automatically previous versions required (spectral initialization). Otherwise, irlba used. two-dimensional output, unlikely notice much difference speed accuracy real-world data. highly-structured simulation datasets (e.g. spectral initialization 1D line) RSpectra give much better, faster initializations, typical use cases envisaged package. embedding higher dimensions (e.g. n_components = 100 higher), RSpectra recommended likely -perform irlba even installed good linear algebra library. init = \"laplacian\" returned wrong coordinates slightly subtle issue around order eigenvectors using random walk transition matrix rather normalized graph laplacians. init_sdev parameter ignored init parameter user-supplied matrix. Now input scaled. Matrix input converted data frame pre-processing, causing R allocate memory disinclined ever give even function exited. unnecessary manipulation now avoided. behavior bandwidth parameter changed give results like current version (0.5.2) Python UMAP implementation. likely breaking change non-default settings bandwidth, parameter actually exposed Python UMAP public API , road deprecation uwot don’t recommend change . Transforming data multiple blocks give error number rows new data equal number number rows original data.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-0111","dir":"Changelog","previous_headings":"","what":"uwot 0.1.11","title":"uwot 0.1.11","text":"CRAN release: 2021-12-02","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-1-11","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.1.11","text":"New parameter: batch. TRUE, results reproducible n_sgd_threads > 1 (long use set.seed). price paid optimization slightly less efficient (coordinates updated quickly hence gradients staler longer), highly recommended set n_epochs = 500 higher. Thank Aaron Lun came way implement feature, also wrote entire C++ implementation UMAP (https://github.com/jlmelville/uwot/issues/83). New parameter: opt_args. default optimization method batch = TRUE Adam. can control parameters passing opt_args list. Adam momentum-based method requires extra storage previous gradient data. avoid extra memory overhead can also use opt_args = list(method = \"sgd\") use stochastic gradient descent method like used batch = FALSE. New parameter: epoch_callback. may now pass function invoked end epoch. Mainly useful producing image state embedding different points optimization. another feature taken umappp. \"irlba\" uses irlba::irlba calculate truncated SVD. routine deems trying extract 50% singular vectors, see warning effect logged console. \"rsvd\", uses irlba::svdr truncated SVD. method uses small number iterations give accuracy/speed trade-similar scikit-learn TruncatedSVD method. can much faster using \"irlba\" potentially cost accuracy. However, purposes dimensionality reduction input nearest neighbor search, doesn’t seem matter much. \"bigstatsr\", uses bigstatsr package used. Note: dependency uwot. want use bigstatsr, must install . platforms without easy access fast linear algebra libraries (e.g. Windows), using bigstatsr may give speed PCA calculations. \"svd\", uses base::svd. Warning: likely slow datasets exists fallback small datasets \"irlba\" method print warning. \"auto\" (default) uses \"irlba\" calculate truncated SVD, unless attempting extract 50% singular vectors, case \"svd\" used.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-11","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.11","text":"row names provided input data (nearest neighbor data, initialization data ’s matrix), used name rows output embedding (https://github.com/jlmelville/uwot/issues/81), also nearest neighbor data set ret_nn = TRUE. names exist one input data parameters listed , inconsistent, guarantees made names used. Thank jwijffels reporting . umap_transform, learning rate now -scaled factor 4, consistent Python implementation UMAP. need old behavior back, use (newly added) learning_rate parameter umap_transform set explicitly. used default value umap creating model, correct setting umap_transform learning_rate = 1.0. Setting nn_method = \"annoy\" verbose = TRUE lead error datasets fewer 50 items . Using multiple pre-computed nearest neighbors blocks now supported umap_transform (incorrectly documented work). Documentation around pre-calculated nearest neighbor data umap_transform wrong ways: now corrected indicate neighbor data item test data, neighbors distances refer items training data (.e. data used build model). n_neighbors parameter now correctly ignored model generation pre-calculated nearest neighbor data provided. Documentation incorrectly said grain_size didn’t anything.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-0110","dir":"Changelog","previous_headings":"","what":"uwot 0.1.10","title":"uwot 0.1.10","text":"CRAN release: 2020-12-15 release mainly allow internal changes keep compatibility RcppAnnoy, used nearest neighbor calculations.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-10","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.10","text":"Passing data missing values now raise error early. Missing data factor columns intended supervised UMAP still ok. Thank David McGaughey tweeting issue. documentation return value umap tumap now note contents model list subject change intended part uwot public API. recommend relying structure model, especially package intended appear CRAN Bioconductor, breakages delay future releases uwot CRAN.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-019","dir":"Changelog","previous_headings":"","what":"uwot 0.1.9","title":"uwot 0.1.9","text":"CRAN release: 2020-11-15","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-1-9","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.1.9","text":"New metric: metric = \"correlation\" distance based Pearson correlation (https://github.com/jlmelville/uwot/issues/22). Supporting required change internals nearest neighbor data stored. Backwards compatibility models generated previous versions using ret_model = TRUE preserved.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-9","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.9","text":"New parameter, nn_method, umap_transform: pass list containing pre-computed nearest neighbor data (identical used umap function). pass anything X parameter case. extends functionality transforming new points case nearest neighbor data original data new data can calculated external uwot. Thanks Yuhan Hao contributing PR (https://github.com/jlmelville/uwot/issues/63 https://github.com/jlmelville/uwot/issues/64). New parameter, init, umap_transform: provides variety options initializing output coordinates, analogously parameter umap function (without many options currently). intended replace init_weighted, considered deprecated, won’t removed uwot 1.0 (whenever ). Instead init_weighted = TRUE, use init = \"weighted\"; replace init_weighted = FALSE init = \"average\". Additionally, can pass matrix init act initial coordinates. Also umap_transform: previously, setting n_epochs = 0 ignored: least one iteration optimization applied. Now, n_epochs = 0 respected, return initialized coordinates without optimization. Minor performance improvement single-threaded nearest neighbor search verbose = TRUE: progress bar calculations taking detectable amount time now fixed. small data sets (< 50 items) progress bar longer appear building index. Passing sparse distance matrix input now supports upper/lower triangular matrix storage rather wasting storage using explicitly symmetric sparse matrix. Minor license change: uwot used licensed GPL-3 ; now GPL-3 later.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-018","dir":"Changelog","previous_headings":"","what":"uwot 0.1.8","title":"uwot 0.1.8","text":"CRAN release: 2020-03-16","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-8","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.8","text":"default n_threads now NULL provide bit protection changing dependencies. parallel code now uses standard C++11 implementation threading rather tinythread++. grain_size parameter undeprecated. version deprecated never made CRAN, unlikely affected many people.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-7","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.7","text":"uwot longer trigger undefined behavior sanitizers, due temporary replacement RcppParallel package code “borrowed” package using tinythread++ rather tbb (https://github.com/jlmelville/uwot/issues/52). sanitizer improvements nearest neighbor search code due upstream efforts erikbern eddelbuettel (https://github.com/jlmelville/uwot/issues/50). grain_size parameter now ignored remains avoid breaking backwards compatibility .","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-1-6","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.1.6","text":"New parameter, ret_extra, vector can contain combination : \"model\" (ret_model = TRUE), \"nn\" (ret_nn = TRUE) fgraph (see ). New return value data: ret_extra vector contains \"fgraph\", returned list contain fgraph item representing fuzzy simplicial input graph sparse N x N matrix. lvish, use \"P\" instead \"fgraph” (https://github.com/jlmelville/uwot/issues/47). Note sparsifying step edges low membership removed prospect edge sampled optimization. controlled n_epochs: smaller value, sparsifying occur. interested fuzzy graph embedded coordinates, set n_epochs = 0. New function: unload_uwot, unload Annoy nearest neighbor indices model. prevents model used umap_transform, allows temporary working directory created save_uwot load_uwot deleted. Previously, load_uwot save_uwot attempting delete temporary working directories used, always silently fail Annoy making use files directories. attempt made reduce variability results due different compiler C++ library versions different machines. Visually results unchanged cases, breaking change terms numerical output. best chance obtaining floating point determinism across machines use init = \"spca\", fixed values b (rather allowing calculated setting min_dist spread) approx_pow = TRUE. Using tumap method init = \"spca\" probably robust approach.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-6","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.6","text":"New behavior n_epochs = 0. used behave like (n_epochs = NULL) gave default number epochs (dependent number vertices dataset). Now usefully carries calculations except optimization, returned coordinates specified init parameter, easy way access e.g. spectral PCA initialization coordinates. want input fuzzy graph (ret_extra vector contains \"fgraph\"), also prevent graph edges low membership removed. still get old default epochs behavior setting n_epochs = NULL negative value. save_uwot load_uwot updated verbose parameter ’s easier see temporary files created. save_uwot new parameter, unload, set TRUE delete working directory , cost unloading model, .e. can’t used umap_transform reload load_uwot. save_uwot now returns saved model extra field, mod_dir, points location temporary working directory, now assign result calling save_uwot model saved, e.g. model <- save_uwot(model, \"my_model_file\"). field intended use unload_uwot. load_uwot also returns model mod_dir item use unload_uwot. save_uwot load_uwot correctly handling relative paths. previous bug fix load_uwot uwot 0.1.4 work newer versions RcppAnnoy (https://github.com/jlmelville/uwot/issues/31) failed typical case single metric nearest neighbor search using available columns, giving error message along lines : Error: index size <size> multiple vector size <size>. now fixed, required changes save_uwot load_uwot, existing saved models must regenerated. Thank reporter OuNao.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-015","dir":"Changelog","previous_headings":"","what":"uwot 0.1.5","title":"uwot 0.1.5","text":"CRAN release: 2019-12-04","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-5","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.5","text":"R API accessed inside multi-threaded code seed (non-R) random number generators. Probably causing users downstream projects (seurat monocle) experience strange RcppParallel-related crashes. Thanks aldojongejan reporting (https://github.com/jlmelville/uwot/issues/39). Passing floating point value smaller one n_threads caused crash. particularly insidious running system one default thread available default n_threads becomes 0.5. Now n_threads (n_sgd_threads) rounded nearest integer. Initialization supervised UMAP now faster (https://github.com/jlmelville/uwot/issues/34). Contributed Aaron Lun.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-014","dir":"Changelog","previous_headings":"","what":"uwot 0.1.4","title":"uwot 0.1.4","text":"CRAN release: 2019-09-23","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-4","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.4","text":"Fixed incorrect loading Annoy indexes compatible newer versions RcppAnnoy (https://github.com/jlmelville/uwot/issues/31). thanks Dirk Eddelbuettel Erik Bernhardsson aid identifying problem. Fix ERROR: already InterruptableProgressMonitor instance defined. verbose = TRUE, , b curve parameters now logged.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-013","dir":"Changelog","previous_headings":"","what":"uwot 0.1.3","title":"uwot 0.1.3","text":"CRAN release: 2019-04-07","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-3","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.3","text":"Fixed issue session crash Annoy nearest neighbor search unable find k neighbors item.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"known-issue-0-1-3","dir":"Changelog","previous_headings":"","what":"Known issue","title":"uwot 0.1.3","text":"Even fix bug mentioned , nearest neighbor index file larger 2GB size, Annoy may able read data back . occur large high-dimensional datasets. nearest neighbor search fail conditions. work-around set n_threads = 0, index written disk re-loaded circumstances, cost longer search time. Alternatively, set pca parameter reduce dimensionality lower n_trees, reduce size index disk. However, either may lower accuracy nearest neighbor results.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-012","dir":"Changelog","previous_headings":"","what":"uwot 0.1.2","title":"uwot 0.1.2","text":"CRAN release: 2019-04-06 Initial CRAN release.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-1-2","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.1.2","text":"New parameter, tmpdir, allows user specify temporary directory nearest neighbor indexes written Annoy nearest neighbor search. default base::tempdir(). used n_threads > 1 nn_method = \"annoy\".","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-2","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.2","text":"Fixed issue lvish --one error calculating input probabilities. Added safe-guard lvish prevent gaussian precision, beta, becoming overly large binary search fails perplexity calibration. lvish perplexity calibration uses log-sum-exp trick avoid numeric underflow beta becomes large.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-0-0-9010","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.0.0.9010 (31 March 2019)","text":"New parameter: pcg_rand. TRUE (default), random number generator PCG family used stochastic optimization phase. old PRNG, direct translation implementation Tausworthe “taus88” PRNG used Python version UMAP, can obtained setting pcg_rand = FALSE. new PRNG slower, likely superior statistical randomness. change behavior break backwards compatibility: now get slightly different results even seed. New parameter: fast_sgd. TRUE, following combination parameters set: n_sgd_threads = \"auto\", pcg_rand = FALSE approx_pow = TRUE. result substantially faster optimization phase, cost slightly less accurate results exactly repeatable. fast_sgd = FALSE default interested visualization, fast_sgd gives perfectly good results. generic dimensionality reduction reproducibility, keep fast_sgd = FALSE. New parameter: init_sdev specifies large standard deviation column initial coordinates . scale input coordinates (including user-provided matrix coordinates). init = \"spca\" can now thought alias init = \"pca\", init_sdev = 1e-4. may aggressive scaling datasets. typical UMAP spectral initializations tend result standard deviations around 2 5, might appropriate cases. spectral initialization detects multiple components affinity graph falls back scaled PCA, uses init_sdev = 1. result adding init_sdev, init options sspectral, slaplacian snormlaplacian removed (weren’t around long anyway). can get behavior e.g. init = \"spectral\", init_sdev = 1e-4. init = \"spca\" sticking around use lot.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-0-9010","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.0.0.9010 (31 March 2019)","text":"Spectral initialization (default) sometimes generating coordinates large range, due erroneous scale factor failed account negative coordinate values. give rise embeddings noticeable outliers distant main clusters. Also spectral initialization, amount noise added standard deviation order magnitude large compared Python implementation (probably didn’t make difference though). requesting spectral initialization, multiple disconnected components present, fall back init = \"spca\". Removed dependency C++ <random> header. breaks backwards compatibility even set pcg_rand = FALSE. metric = \"cosine\" results incorrectly using unmodified Annoy angular distance. Numeric matrix columns can specified target categorical metric (fixes https://github.com/jlmelville/uwot/issues/20).","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-0009009-1-january-2019","dir":"Changelog","previous_headings":"","what":"uwot 0.0.0.9009 (1 January 2019)","title":"uwot 0.0.0.9009 (1 January 2019)","text":"Data now stored column-wise optimization, result increase performance larger values n_components (e.g. approximately 50% faster optimization time MNIST n_components = 50). New parameter: pca_center, controls whether center data applying PCA. typical set FALSE applying PCA binary data (although note can’t use setting metric = \"hamming\") PCA now used metric \"manhattan\" \"cosine\". ’s still applied using \"hamming\" (data still needs binary format, real-valued). using mixed datatypes, may override pca pca_center parameter values given data block using list value metric, column ids/names unnamed item overriding values named items, e.g. instead manhattan = 1:100, use manhattan = list(1:100, pca_center = FALSE) turn PCA centering just block. functionality exists mainly case mixed binary real-valued data want apply PCA data types. ’s normal apply centering real-valued data binary data.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-0-9009","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.0.0.9009 (1 January 2019)","text":"Fixed bug affected umap_transform, negative sampling size test data (training data). performance improvements (around 10% faster optimization stage MNIST). verbose = TRUE, log Annoy recall accuracy, may help tune values n_trees search_k.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-0-0-9008","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.0.0.9008 (December 23 2018)","text":"New parameter: n_sgd_threads, controls number threads used stochastic gradient descent. default now single-threaded result reproducible results using set.seed. get back old, less consistent, faster settings, set n_sgd_threads = \"auto\". alpha now learning_rate. gamma now repulsion_strength. Default spectral initialization now looks disconnected components initializes separately (also applies laplacian normlaplacian). New init options: sspectral, snormlaplacian slaplacian. like spectral, normlaplacian, laplacian respectively, scaled dimension standard deviation 1e-4. like difference pca spca options.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-0-9008","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.0.0.9008 (December 23 2018)","text":"Hamming distance support (actually using Euclidean distance). Smooth knn/perplexity calibration results small dependency number threads used. Anomalously long spectral initialization times now reduced. Internal changes fixes thanks code review Aaron Lun.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-0-0-9007","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.0.0.9007 (December 9 2018)","text":"New parameter pca: set positive integer reduce matrix data frames number columns using PCA. works metric = \"euclidean\". > 100 columns, can substantially improve speed nearest neighbor search. t-SNE implementations often set value 50.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-0-9007","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.0.0.9007 (December 9 2018)","text":"Laplacian Eigenmap initialization convergence failure now correctly detected. C++ code -writing data passed R function argument.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-0-0-9006","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.0.0.9006 (December 5 2018)","text":"Highly experimental mixed data type support metric: instead specifying single metric name (e.g. metric = \"euclidean\"), can pass list, name item metric use value vector names columns use metric, e.g. metric = list(\"euclidean\" = c(\"A1\", \"A2\"), \"cosine\" = c(\"B1\", \"B2\", \"B3\")) treats columns A1 A2 one block, using Euclidean distance find nearest neighbors, whereas B1, B2 B3 treated second block, using cosine distance. Factor columns can also used metric, using metric name categorical. y may now data frame matrix multiple target data available. New parameter target_metric, specify distance metric use numerical y. capabilities metric. Multiple external nearest neighbor data sources now supported. Instead passing list two matrices, pass list lists, one external metric. details mixed data types can found https://github.com/jlmelville/uwot#mixed-data-types. Compatibility older versions RcppParallel (contributed sirusb). scale = \"Z\" Z-scale column input (synonym scale = TRUE scale = \"scale\"). New scaling option, scale = \"colrange\" scale columns range (0, 1).","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-0-0-9005","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.0.0.9005 (November 4 2018)","text":"Hamming distance now supported, due upgrade RcppAnnoy 0.0.11.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-0-0-9004","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.0.0.9004 (October 21 2018)","text":"supervised UMAP numeric y, may pass nearest neighbor data directly, format supported X-related nearest neighbor data. may useful don’t want use Euclidean distances y data, missing data (way assign nearest neighbors cases, obviously). See Nearest Neighbor Data Format section details.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-0-0-9003","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.0.0.9003 (September 22 2018)","text":"New parameter ret_nn: TRUE returns nearest neighbor matrices nn list: indices item idx distances item dist. Embedded coordinates embedding. ret_nn ret_model can TRUE, cause compatibility issues supervised embeddings. nn_method can now take precomputed nearest neighbor data. Must list two matrices: idx, containing integer indexes, dist containing distances. coincidence, format return ret_nn.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-0-9003","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.0.0.9003 (September 22 2018)","text":"Embedding n_components = 1 broken (https://github.com/jlmelville/uwot/issues/6) User-supplied matrices init parameter modified, defiance basic R pass--copy semantics.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-0-9002","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.0.0.9002 (August 14 2018)","text":"metric = \"cosine\" working n_threads greater 0 (https://github.com/jlmelville/uwot/issues/5)","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-0-0-9001","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.0.0.9001","text":"August 5 2018. can now use existing embedding add new points via umap_transform. See example section . August 1 2018. Numerical vectors now supported supervised dimension reduction. July 31 2018. () initial support supervised dimension reduction: categorical data moment. Pass factor vector (use NA unknown labels) y parameter edges bad (unknown) labels -weighted, hopefully leading better separation classes. works remarkably well Fashion MNIST dataset. July 22 2018. can now use cosine Manhattan distances Annoy nearest neighbor search, via metric = \"cosine\" metric = \"manhattan\", respectively. Hamming distance supported RcppAnnoy doesn’t yet support .","code":""}]
