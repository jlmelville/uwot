[{"path":"https://jlmelville.github.io/uwot/articles/abparams.html","id":"theory","dir":"Articles","previous_headings":"","what":"Theory","title":"Fine-tuning UMAP Visualizations","text":"output weight two points \\(\\) \\(j\\) given : \\[ w_{ij} = 1 / \\left(1 + ad_{ij}^{2b}\\right) \\] \\(d_{ij}\\) Euclidean distance embedding two points. drop \\(ij\\) subscript clarity’s sake. \\(\\) \\(b\\) two hyper-parameters. Usually determined non-linear least squares fit based exponential decay curve parameterized min_dist spread: \\[ w = \\exp\\left[-\\max \\left(0, d - \\rho \\right) / \\sigma \\right] \\] min_dist \\(\\rho\\) spread \\(\\sigma\\). used symbols make obvious equation form UMAP’s weighting function edge weights input space. reminder, presence max operation shifting distances \\(\\rho\\) enforce local connectivity constraint: always edge weight 1 point nearest neighbor. spread determines x-value range y-value decays zero, set spread multiplied 3. ’s R code works plots results using Python UMAP defaults spread = 1, min_dist = 0.1: title indicates, curve leads default parameters \\(= 1.577\\), \\(b = 0.895\\). uwot uses slightly different default min_dist = 0.001, leads \\(= 1.93, b = 0.79\\). don’t know used different default min_dist – probably made mistake. likely change later version uwot, doesn’t make much difference results, fortunately. Setting \\(= 1, b = 1\\) gives Cauchy distribution used t-SNE (tumap function uwot), corresponds roughly spread = 1.12 min_dist = 0.23: putting values back curve-fitting routine give back \\(= 0.99\\), \\(b = 1.01\\). Close enough. uwot default results orange Cauchy results blue overlaid UMAP defaults (green, previous plot): still leaves open question changing spread min_dist, b affects output UMAP. current version UMAP docs doesn’t mention spread treats min_dist adjustable parameter, can varied 0 close 1. follows ’ll take look min_dist spread, use trusty MNIST digits dataset investigate effect.","code":"spread <- 1 min_dist <- 0.1  # define the exponential xv <- seq(   from = 0,   to = spread * 3,   length.out = 300 ) yv <- exp(-(pmax(0, xv - min_dist)) / spread)  # Fit the a,b curve to the exponential params <- stats::nls(yv ~ 1 / (1 + a * xv^(2.0 * b)),   start = list(a = 1, b = 1) )$m$getPars() a <- params[\"a\"] b <- params[\"b\"]  # Plot the results title <-   paste0(     \"exp curve spread = \",     spread,     \", min_dist = \",     min_dist   ) sub <-   paste0(\"UMAP fit (green) a = \", formatC(a), \" b = \", formatC(b)) plot(   xv,   yv,   xlab = \"d\",   ylab = \"w\",   type = \"l\",   main = title,   lwd = 2 )  graphics::mtext(sub) lines(xv, 1 / (1 + a * xv^(2.0 * b)), col = \"#1B9E77FF\", lwd = 2)"},{"path":"https://jlmelville.github.io/uwot/articles/abparams.html","id":"min_dist-and-spread","dir":"Articles","previous_headings":"","what":"min_dist and spread","title":"Fine-tuning UMAP Visualizations","text":"Let’s look min_dist spread first. First, ’ll show results changing spread, keeping min_dist = 0.1 ’ll look changing min_dist, fixing spread = 1. explanation exponential curve suggests allowing min_dist exceed value spread give increasingly odd results.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/abparams.html","id":"spread","dir":"Articles","previous_headings":"min_dist and spread","what":"spread","title":"Fine-tuning UMAP Visualizations","text":"plots , value spread min_dist given title, along values b give rise . value spread increases 0.1 10 go left right top bottom. top-left result, spread = min_dist = 0.1 gives clear indication want spread larger min_dist. , values spread 0.5-3 much happens. spread gets 5 , clusters start overlap . ’s table summarizing b change spread varied. low values spread, starts getting large. min_dist? , min_dist increases 0.0001 2, spread = 1, suspect highest values also show eccentric results.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/abparams.html","id":"min_dist","dir":"Articles","previous_headings":"min_dist and spread","what":"min_dist","title":"Fine-tuning UMAP Visualizations","text":", range min_dist values, 0.0001 0.1 much happens plot. value, clusters begin expand. final shapes diffuse. table shows b change min_dist increases. value min_dist larger 0.5, values drop quickly b increases rapidly. terms differences spread min_dist, min_dist seems obviously increase size clusters, whereas increasing spread keeps shape boundary clusters bit better. spread can therefore used control inter-cluster distances extent, min_dist controls size clusters.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/abparams.html","id":"a-and-b","dir":"Articles","previous_headings":"","what":"a and b","title":"Fine-tuning UMAP Visualizations","text":"Rather change spread min_dist, can supply values b directly. Perhaps can also interpreted. , ’ll look changing value separately, leaving value UMAP defaults (= 1.58 b = 0.90). used range values b resulted changing min_dist spread previous section’s results set range values look . Wang co-workers recommend b > 0.5 good loss function dimensionality reduction (see Proposition 1 2 PaCMAP paper).","code":""},{"path":"https://jlmelville.github.io/uwot/articles/abparams.html","id":"a","dir":"Articles","previous_headings":"a and b","what":"a","title":"Fine-tuning UMAP Visualizations","text":"seemed wider range values b, looked values = 0.0001 = 100. seems control spread clusters range. Low values certainly result diffuse round cloud. = 10, suspect running numerical issues taking large power small positive value. Values 0.1 10 seem reasonable, higher values leading smaller clusters.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/abparams.html","id":"b","dir":"Articles","previous_headings":"a and b","what":"b","title":"Fine-tuning UMAP Visualizations","text":"b definitely seems smaller range useful values compared , looked values b = 0.1 b = 2.5. b seems work like heavy-tail parameter sometimes used t-SNE: low values increase distances clusters, relative size, also reveal sub-clusters appear one structure plots. high values, space clusters reduced, can still see borders clusters, unlike happens low . don’t see big difference using b directly, sticking min_dist spread prefer b , reminds approach used ABSNE (PDF), although don’t claim equivalence b parameters UMAP \\(\\alpha\\) \\(\\beta\\) parameters work.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/abparams.html","id":"examples","dir":"Articles","previous_headings":"","what":"Examples","title":"Fine-tuning UMAP Visualizations","text":"MNIST useful seeing effect changing b, can visualized well default parameters. two examples show can improve default visualizations, using knowledge b roughly represent terms cluster size separation.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/abparams.html","id":"example-1-tasic2018","dir":"Articles","previous_headings":"Examples","what":"Example 1: tasic2018","title":"Fine-tuning UMAP Visualizations","text":"transcriptomics dataset tasic2018 good example default UMAP parameters sub-optimal. default UMAP result top left image, series results based fiddling b response . eventually fumble way setting lower higher b provides better visualization (opinion), can see lower right.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/abparams.html","id":"example-2-coil-20","dir":"Articles","previous_headings":"Examples","what":"Example 2: COIL-20","title":"Fine-tuning UMAP Visualizations","text":"’s another example arguably default UMAP results spread clusters bit much. three alternatives based controlling b.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/abparams.html","id":"my-recommendation","dir":"Articles","previous_headings":"","what":"My recommendation","title":"Fine-tuning UMAP Visualizations","text":"Hopefully enough convince embedding parameters can profitably twiddled random way give visualizations improve default settings. min_dist spread, modifying min_dist 0 1, suggested UMAP docs seems fruitful parameters meddle . personally prefer use b directly. find good values b, can start = 1 b = 1, gives t-SNE-like output function, can use tumap function generate initial plot much faster. also recommend PCA dimensionality reduction outside uwot using ret_nn = TRUE first plot, can re-use nearest neighbors data subsequent runs umap. substantial speed makes repeating runs different values b lot tolerable. ’s example workflow:","code":"# PCA to 50 dimensions first mnist_pca <- irlba::prcomp_irlba(mnist, n = 50, retx = TRUE, center = center,                              scale = FALSE)$x  # t-UMAP is equivalent to a = 1, b = 1 # remember to get the nearest neighbor data back too mnist_a1b1 <- tumap(mnist_pca, ret_nn = TRUE)  # find a mnist_a1.5b1 <- umap(mnist_pca, nn_method = mnist_a1b1$nn, a = 1.5, b = 1) mnist_a0.5b1 <- umap(mnist_pca, nn_method = mnist_a1b1$nn, a = 0.5, b = 1)  # find b based on whichever value of a you prefer mnist_a0.5b1.2 <- umap(mnist_pca, nn_method = mnist_a1b1$nn, a = 0.5, b = 1.2)"},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/fast-sgd.html","id":"norb","dir":"Articles","previous_headings":"","what":"norb","title":"Fast SGD settings","text":"distribution clusters look little bit different, think ’s within variation one expect stochastic nature optimization. bolster point, two runs UMAP NORB dataset fast_sgd = FALSE different seeds: think didn’t know one four images generated fast settiings, ’d hard pressed pick line . variation images likely due default n_neighbors low capture global structure. ’s plots n_neighbors = 150, n_epochs = 500 account increased number edges need sampling: Apart one blue loops open fast_sgd = TRUE result, probably fixable longer optimization, global arrangement two plots pretty similar.","code":""},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/fast-sgd.html","id":"recommendations","dir":"Articles","previous_headings":"","what":"Recommendations","title":"Fast SGD settings","text":"results, ’d say fast_sgd = TRUE settings give results effectively indistinguishable slower settings, want save bit time, seems harm using . actual time savings ’ll see depend long nearest neighbor search takes, initial PCA carry input (examples) can take fair amount run time. example NORB, PCA takes 5 minutes six---half minute total run time, ’s lot time saved. MNIST Fashion, can effectively halve run time (six threads, anyway). reproducibility important , using multiple threads optimization question, although ’s gives biggest speed increase. However, still consider setting approx_pow = TRUE, pcg_random = FALSE. MNIST-sized datasets (mnist, fashion, kuzushiji) saw reasonable speed around 25%. datasets PCA nearest neighbor search dominates, gains smaller: 10-15% speedup tasic2018 macosko2015, 5% norb. need set n_epochs higher, time savings increase.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"using-rcpphnsw","dir":"Articles","previous_headings":"","what":"Using RcppHNSW","title":"Using HNSW for nearest neighbors","text":"Now install RcppHNSW load : Time build index using training data. default settings hnsw_build perfectly good MNIST, ’ll go . recommend using many threads can stage. use 6 threads:","code":"install.packages(\"RcppHNSW\") library(RcppHNSW) mnist_index <- hnsw_build(mnist_train_data, n_threads = 6)"},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"mnist-training-data-k-nearest-neighbors","dir":"Articles","previous_headings":"Using RcppHNSW","what":"MNIST training data k-nearest neighbors","title":"Using HNSW for nearest neighbors","text":"Now query index training data find k-nearest neighbors training data. Note case built index data querying . index building, can get good performance default settings, ’s recommended use many threads can. searching substantially faster index building, though. extra parameter need specify number neighbors want. uwot, default number neighbors 15, shall use k parameter: separate article uwot’s nearest neighbor format good news output format RcppHNSW already right format uwot (coincidence: created maintain RcppHNSW package). Nonetheless, let’s take look output, list two matrices: first matrix, idx, contains indices nearest neighbors point training data. second matrix, dist, contains distances nearest neighbors. Let’s take look dimensions matrices: 60,000 rows, one point training data, 15 columns, one nearest neighbor. Let’s take look first rows columns matrix: k-nearest neighbors dataset, ’s often convention nearest neighbor item item , nearest neighbor first item index 1, distance zero. nearest neighbor packages follow convention, uwot , ’re good.","code":"mnist_train_knn <-   hnsw_search(     mnist_train_data,     mnist_index,     k = 15,     n_threads = 6   ) names(mnist_train_knn) [1] \"idx\"  \"dist\" lapply(mnist_train_knn, `dim`) $idx [1] 60000    15  $dist [1] 60000    15 mnist_train_knn$idx[1:3, 1:3] [,1]  [,2]  [,3] [1,]    1 32249  8729 [2,]    2   640 51122 [3,]    3 54198 46129 mnist_train_knn$dist[1:3, 1:3] [,1]     [,2]     [,3] [1,]    0 1561.472 1591.601 [2,]    0 1020.647 1100.529 [3,]    0 1377.631 1541.127"},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"mnist-test-set-query-neighbors","dir":"Articles","previous_headings":"Using RcppHNSW","what":"MNIST test set query neighbors","title":"Using HNSW for nearest neighbors","text":"also need test set neighbors, let’s now. going build index test set, instead query test set item training set index: output format training set neighbors, surprises : first indices distances test set: Remember queried test set training set, test set indices don’t get chance neighbors . Consequently, also nearest neighbor distances zero.","code":"mnist_test_query_neighbors <-   hnsw_search(     mnist_test_data,     mnist_index,     k = 15,     n_threads = 6,     verbose = TRUE   ) lapply(mnist_test_query_neighbors, `dim`) $idx [1] 10000    15  $dist [1] 10000    15 mnist_test_query_neighbors$idx[1:3, 1:3] [,1]  [,2]  [,3] [1,] 53844 38621 16187 [2,] 28883 49161 24613 [3,] 58742 46513 15225 mnist_test_query_neighbors$dist[1:3, 1:3] [,1]      [,2]      [,3] [1,]  676.5841  793.9868  862.6766 [2,] 1162.9316 1211.8445 1285.9285 [3,]  321.6629  332.4635  341.0484"},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"using-hnsw-k-nearest-neighbors-with-umap","dir":"Articles","previous_headings":"","what":"Using HNSW k-nearest neighbors with UMAP","title":"Using HNSW for nearest neighbors","text":"now ready use HNSW k-nearest neighbors UMAP. time fire uwot.","code":"library(uwot)"},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"umap-on-training-data-with-hnsw-knn","dir":"Articles","previous_headings":"Using HNSW k-nearest neighbors with UMAP","what":"UMAP on training data with HNSW knn","title":"Using HNSW for nearest neighbors","text":"UMAP works nearest neighbor graph, pass pre-computed nearest neighbor data, don’t actually need give data. : set X = NULL. pass nearest neighbor data nn_method parameter. ’m also going set parameters : verbose = TRUE – just like know ’s going . n_sgd_threads = 6 number threads use optimization step. batch = TRUE alternative optimization method gives bit repeatable results setting n_sgd_threads greater one. n_epochs = 500 number epochs use optimization step. find batch = TRUE, need use larger number epochs usual default (MNIST-sized datasets n_epochs = 200). ret_model = TRUE parameter tells uwot return UMAP model can transform new data later. Nothing exciting output, except notice last line output reminding us want transform new data better generated neighbors data . , ’re ok.","code":"mnist_train_umap <-   umap(     X = NULL,     nn_method = mnist_train_knn,     batch = TRUE,     n_epochs = 500,     n_sgd_threads = 6,     ret_model = TRUE,     verbose = TRUE   ) UMAP embedding parameters a = 1.896 b = 0.8006 Commencing smooth kNN distance calibration using 6 threads with target n_neighbors = 15 Initializing from normalized Laplacian + noise (using irlba) Commencing optimization for 500 epochs, with 1239236 positive edges using 6 threads Using method 'umap' Optimizing with Adam alpha = 1 beta1 = 0.5 beta2 = 0.9 eps = 1e-07 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----| **************************************************| Optimization finished Note: model requested with precomputed neighbors. For transforming new data, distance data must be provided separately"},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"transforming-the-test-data","dir":"Articles","previous_headings":"Using HNSW k-nearest neighbors with UMAP","what":"Transforming the test data","title":"Using HNSW for nearest neighbors","text":"Let us now transform test data using UMAP model just created. far fewer nobs twiddle transforming new data mainly baked UMAP model. , pass X = NULL don’t need original test set data, pass test set nearest neighbor data nn_method parameter. also need pass UMAP model just created.","code":"mnist_test_umap <-   umap_transform(     X = NULL,     model = mnist_train_umap,     nn_method = mnist_test_query_neighbors,     n_sgd_threads = 6,     verbose = TRUE   ) Read 10000 rows Processing block 1 of 1 Commencing smooth kNN distance calibration using 6 threads with target n_neighbors = 15 Initializing by weighted average of neighbor coordinates using 6 threads Commencing optimization for 167 epochs, with 150000 positive edges using 6 threads Using method 'umap' Optimizing with Adam alpha = 1 beta1 = 0.5 beta2 = 0.9 eps = 1e-07 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----| **************************************************| Finished"},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"plotting-the-results","dir":"Articles","previous_headings":"Using HNSW k-nearest neighbors with UMAP","what":"Plotting the results","title":"Using HNSW for nearest neighbors","text":"Let’s plot embeddings make sure HNSW giving us useful results. use ggplot2, Polychrome package create set distinct colors digit. gives palette 10 colors following recipe bit like Python glasbey package: Now can plot training test data, two separate plots.  training set embedding looks like typical UMAP--MNIST result, test clusters right place relative training set. can celebrate victory successful use HNSW.","code":"install.packages(c(\"ggplot2\", \"Polychrome\")) library(ggplot2) library(Polychrome) palette <- as.vector(Polychrome::createPalette(   length(levels(mnist$Label)) + 2,   seedcolors = c(\"#ffffff\", \"#000000\"),   range = c(10, 90) )[-(1:2)]) ggplot(   data.frame(mnist_train_umap$embedding, Digit = mnist_train$Label),   aes(x = X1, y = X2, color = Digit) ) +   geom_point(alpha = 0.5, size = 1.0) +   scale_color_manual(values = palette) +   theme_minimal() +   labs(     title = \"MNIST training set UMAP\",     x = \"\",     y = \"\",     color = \"Digit\"   ) +   theme(legend.position = \"right\") +   guides(color = guide_legend(override.aes = list(size = 5, alpha = 1))) ggplot(   data.frame(mnist_test_umap, Digit = mnist_test$Label),   aes(x = X1, y = X2, color = Digit) ) +   geom_point(alpha = 0.5, size = 1.0) +   scale_color_manual(values = palette) +   theme_minimal() +   labs(     title = \"MNIST test set UMAP\",     x = \"\",     y = \"\",     color = \"Digit\"   ) +   theme(legend.position = \"right\") +   guides(color = guide_legend(override.aes = list(size = 5, alpha = 1)))"},{"path":"https://jlmelville.github.io/uwot/articles/hnsw-umap.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"Using HNSW for nearest neighbors","text":"summarize, use RcppHNSW UMAP following: build index data hnsw_build. query index hnsw_search obtain k-nearest neighbors. run UMAP umap setting passing neighbor data nn_method. transform new data, thing following additions modifications: use hnsw_search new data index created first step get neighbors new data (respect training data). run UMAP, set ret_model = TRUE get UMAP model back. use umap_transform UMAP model pass new data’s neighbors nn_method. also don’t need keep original data around neighbors, can set X = NULL running umap umap_transform. data can still useful initialization. example want use PCA-based initialization need keep original data around. ’s necessary default settings umap. plan integrate RcppHNSW tightly uwot future release, now hopefully enough get started.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"spectral-vs-spca","dir":"Articles","previous_headings":"","what":"Spectral vs SPCA","title":"Initialization","text":"examples using either scaled PCA initialization spectral approach. first row, left-hand image result using init = \"spca\", right-hand image init = \"agspectral\". second row, left-hand image init = \"spectral\". right-hand image init = \"laplacian\". datasets don’t second row images, settings used , generate one component graph, therefore init = \"spectral\" init = \"laplacian\" fall back results seen init = \"spca\".","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"spca-init_sdev","dir":"Articles","previous_headings":"","what":"SPCA init_sdev","title":"Initialization","text":"mentioned earlier, init = \"spca\" settings scales PCA initialization standard deviation 1e-4, recommended Kobak Berens. value taken t-SNE initialization, seems arbitrary, least history quite successful side. MNIST, one clusters broken , wasn’t seen spectral initialization. repeated embedding different seed (results shown) saw split cluster, probably wasn’t due bad luck random number generator. standard deviation input blame? Although setting works well t-SNE, normalization involved method may mean sort gradients seen distances quite different used UMAP. test , results using larger standard deviation. Results left use: right:","code":"iris_umap <- umap(iris, init = \"spca\", init_sdev = 0.01) iris_umap <- umap(iris, init = \"spca\", init_sdev = 1)"},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"macosko2015-1","dir":"Articles","previous_headings":"SPCA init_sdev","what":"macosko2015","title":"Initialization","text":"Results much better larger value init_sdev. typical standard deviation spectral initialization datasets range 1-5, using init_sdev = 1, seems like good place start.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"random-initialization-rand","dir":"Articles","previous_headings":"","what":"Random Initialization: rand","title":"Initialization","text":"two results using random initialization, can get sense much variation can expect getting lucky initialization.","code":"# left-hand plots used this seed set.seed(1337) # right-hand plots used this seed # set.seed(42) iris_umap <- umap(iris, init = \"rand\")"},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"macosko2015-2","dir":"Articles","previous_headings":"Random Initialization: rand","what":"macosko2015","title":"Initialization","text":"Broken-clusters observed, although prevalence increased. tried repeating embeddings init_sdev = 1, didn’t help.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"random-initialization-lvrand","dir":"Articles","previous_headings":"","what":"Random Initialization: lvrand","title":"Initialization","text":"result using random initialization t-SNE/LargeVis style, use Gaussian distribution much smaller initial range coordinate values. make much difference?","code":"# left-hand plots used this seed set.seed(1337) # right-hand plots used this seed # set.seed(42) iris_umap <- umap(iris, init = \"lvrand\")"},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"macosko2015-3","dir":"Articles","previous_headings":"Random Initialization: lvrand","what":"macosko2015","title":"Initialization","text":", doesn’t make huge difference. MNIST still issue split clusters. increasing init_sdev value doesn’t really help. help increasing n_neighbors parameter. n_neighbors = 150, MNIST clusters correctly reconstituted.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/init.html","id":"recommendations","dir":"Articles","previous_headings":"","what":"Recommendations","title":"Initialization","text":"default spectral initialization good job, fallback scaled PCA also fine, long set init_sdev = 1. uwot version 0.0.0.9010, default initialization settings. far can see, init = \"agspectral\" settings also work well. ’s much difference init = \"spectral\" results spectral initialization succeeds, added advantage shouldn’t ever fall back scaled PCA approach. means might give relevant initialization PCA (e.g. supervised UMAP). Laplacian Eigenmap initialization gives results similar spectral initialization. spectral initialization causing problems (e.g. suffering numeric issues cause take long), probably look agspectral settings Laplacian Eigenmaps, likely converge easily doesn’t need single-component graph work. said, agspectral approach entirely invention, whereas Laplacian Eigenmaps published technique, may confidence latter (don’t blame ). random initializations probably least good choice. default settings, run sizable risk splitting clusters. can ameliorated increasing n_neighbors parameter, increases run time methods don’t seem sensitive issues.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"densmap","dir":"Articles","previous_headings":"","what":"densMAP","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"densMAP variation UMAP attempts preserve differences density original data adding new component cost function. available Python UMAP implementation setting densmap=True. See also excellent tutorial. request add densMAP uwot since start 2021. document describes approximate version densMAP available uwot may good enough. first, bit densMAP. densMAP can seen adding regularization UMAP cost function “local radius” observation input output space calculated Pearson correlation two optimized. radius observation \\(\\), \\(R_i\\) calculated : \\[ R_i = \\frac{1}{\\sum_i P_{ij}} \\sum_i d_{ij}^2 P_{ij} \\] \\(d_{ij}\\) distance point \\(\\) \\(j\\) \\(P_{ij}\\) symmetrized edge weight \\(\\) \\(j\\), .e. radius edge-weighted average squared distances \\(\\) neighbors. matrix symmetrized, \\(\\) may different number neighbors n_neighbors parameter user specifies. get feel densMAP , results Python UMAP (.e. uwot output ), without densmap=True. recommended current UMAP README, also set n_neighbors=30, except subset_clusters, ’ll explain get . Everything else left defaults. images : Top left: UMAP results colored typical label dataset. cryptic numbers subtitle feel free ignore. Top right: UMAP results point colored log input local radius, using ColorBrewer Spectral palette. Red means small radius, blue means large radius. show areas densMAP likely contract expand, respectively. Middle left: densMAP results colored typical label dataset. Middle right: densMAP results colored local radius. Bottom: plot log input local radii vs log output radii Pearson correlation . radii returned fit_transform method densmap=True. 1000 points, used smoothScatter plot results. First two datasets simple simulation sets based Use t-SNE Effectively, specifically sections 2 (“Cluster sizes t-SNE plot mean nothing”) 6 (“topology, may need one plot”).","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"two_different_clusters","dir":"Articles","previous_headings":"densMAP","what":"two_different_clusters","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"dataset consists two 50D Gaussians 5,000 points . One clusters standard deviation 10x . used snedata generate data (via two_different_clusters_data function). Like t-SNE, default UMAP shows clusters size. densMAP shows one clusters smaller. good start.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"subset_clusters","dir":"Articles","previous_headings":"densMAP","what":"subset_clusters","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"dataset features two 50D gaussians 5,000 points , time overlap entirely. larger cluster standard deviation 50, smaller standard deviation 1. dataset used n_neighbors=150, much larger n_neighbors=30 setting used plots. consistent plots generated uwot. uwot precalculated exact nearest neighbors datasets dataset, using exact 30 nearest neighbors results smaller cluster embedded edge larger cluster. Presumably 30 nearest neighbors doesn’t result enough edges two clusters properly situate smaller cluster. approximate nearest neighbors routine used UMAP results edges sufficiently far away neighbors create neighborhood reflects global structure better. didn’t investigate thoroughly, sounds plausible . UMAP considers two clusters size. densMAP, ring-shaped structure found high perplexities t-SNE shows . least small cluster inside big one. Note re-run n_neighbors=30 results noticeably changed.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"s1k","dir":"Articles","previous_headings":"densMAP","what":"s1k","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"synthetic dataset like use: 1000 points fuzzy 10D simplex. can’t say like densMAP done . one point flung far away rest plot. can see coloring large local radius. perhaps indicates local radius calculation can affected edge effects.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"swissroll","dir":"Articles","previous_headings":"densMAP","what":"swissroll","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"swiss roll dataset devising. densMAP noticeably better job ripping swiss roll UMAP . expect local density fairly uniform across 2D manifold don’t know ’s priori reason thought densMAP better job .","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"scurvehole","dir":"Articles","previous_headings":"densMAP","what":"scurvehole","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"3D S-curve hole data set, used validate PaCMAP method (see also github repo).","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"iris","dir":"Articles","previous_headings":"densMAP","what":"iris","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"Ronald Fisher’s iris dataset.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"spenguins","dir":"Articles","previous_headings":"densMAP","what":"spenguins","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"Palmer Penguins. s spenguins stands scaled, filtered entries missing values Z-scaled inputs.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"mammoth","dir":"Articles","previous_headings":"densMAP","what":"mammoth","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"3D point cloud mammoth Smithsonian, Understanding UMAP, based work originally done Max Noichl.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"oli","dir":"Articles","previous_headings":"densMAP","what":"oli","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"Olivetti Faces images.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"frey","dir":"Articles","previous_headings":"densMAP","what":"frey","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"Frey Faces images.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"isofaces","dir":"Articles","previous_headings":"densMAP","what":"isofaces","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"faces dataset used Isomap (processed via gist).","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"mnist","dir":"Articles","previous_headings":"densMAP","what":"mnist","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"MNIST digits images. orange cluster 1 digits smaller radius exceptionally obvious coloring UMAP plot radius. noticeably shrinks densMAP goes work .","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"fashion","dir":"Articles","previous_headings":"densMAP","what":"fashion","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"Fashion MNIST images. Similarly MNIST digits, orange cluster (time images pants) noticeably smaller clusters.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"kuzushiji","dir":"Articles","previous_headings":"densMAP","what":"kuzushiji","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"Kuzushiji MNIST images.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"norb","dir":"Articles","previous_headings":"densMAP","what":"norb","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"Small NORB images. can see largest blue/purple ring-like structures broken UMAP, densMAP preserves . fact, densMAP plot one nicer UMAP layouts small NORB datasets ’ve seen.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"coil20","dir":"Articles","previous_headings":"densMAP","what":"coil20","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"COIL-20 images.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"coil100","dir":"Articles","previous_headings":"densMAP","what":"coil100","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"COIL-100 images.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"cifar10","dir":"Articles","previous_headings":"densMAP","what":"cifar10","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"CIFAR-10 images.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"macosko2015","dir":"Articles","previous_headings":"densMAP","what":"macosko2015","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"macosko2015 RNAseq data. dataset (RNA sequence dataset) isn’t lot fun visualize vanilla UMAP, densMAP improve matters.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"tasic2018","dir":"Articles","previous_headings":"densMAP","what":"tasic2018","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"RNAseq data Allen Brain Atlas originally reported Tasic co-workers. Another RNAseq dataset, densMAP interesting effect cluster sizes, clusters low density, effect squashing center plot.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"lamanno2020","dir":"Articles","previous_headings":"densMAP","what":"lamanno2020","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"Another RNASeq data, found via Picasso example notebook. think publication reference https://doi.org/10.1038/s41586-021-03775-x (published biorXiv 2020).","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"ng20","dir":"Articles","previous_headings":"densMAP","what":"ng20","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"20 Newsgroups dataset, manually converted dense 3000D PCA result: see gist. densMAP noticeably hard time mapping local radii 2D output. take away : densMAP good job maintaining correlation input output radii. time fails 20 Newsgroups dataset. good correlation doesn’t necessarily mean helpful static visualization. results clusters highly dispersed may reflect true relationship densities makes hard actually see cluster, rest data gets crushed middle plot. ’s necessarily problem interactive plot can pan zoom. can change regularization value dens_lambda. didn’t try reason doubt satisfying value can found datasets didn’t like output default settings. don’t currently want implement densMAP involves adding lots extra code, much C++ lazy.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"approximating-densmap","dir":"Articles","previous_headings":"","what":"Approximating densMAP","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"’s pitch approximation densMAP, named Lightweight Estimate Preservation Local Density (Leopold). input radii, part smooth k-nearest neighbor distances routine already calculate \\(\\rho\\) distance nearest neighbor item dataset (subject local_connectivity constraint). Additionally, also calculate \\(\\sigma\\) normalization factor used neighbors distance greater \\(\\rho\\). must units distance, larger distances neighbors, larger \\(\\sigma\\) gets. propose following definition local radius: \\[ R_i = \\rho + \\sigma \\] biggest difference definition local input radii haven’t bothered square distances (doesn’t make much difference leopold) values \\(\\sigma\\) \\(\\rho\\) calculated symmetrization UMAP input edge weights. values missing influence observations outside initial k-nearest neighborhood graph. output radii, know looking output weight function parameters, increasing parameter makes clusters UMAP plot shrink, vice versa. let’s use measure output density, .e. inverse local radius. every point radius, value given weight points \\(\\) \\(j\\) geometric mean two radii: \\[ w_{ij} = 1 / \\left(1 + \\frac{d_{ij}^2}{\\sqrt{r_i r_j}} \\right) \\] \\(r_i\\) scaled version input radius \\(R_i\\) suitable lower dimensional space. similar sort scaling used Zelnik-Manor Perona “self-tuning” spectral clustering, also recommended local scaling reduce hubness high dimensional space Schnitzer co-workers. scaling input output space sophisticated: observed typical values \\(b\\) used UMAP, usable range \\(\\) values 0.01 (diffuse clusters) 100 (tight clusters), just map inverse input local radii range. final adjustment good introduce parameter controls much range used output thus big disparity input radii reflected output. end, adjustable parameter leopold: dens_scale parameter. Set 1 uses available range . Set 0 get plain old UMAP back, fixed global value specified user (via spread min_dist parameters). Intermediate values get intermediate results. mind scaling radii straightforward, looking description maybe ’s . procedure : Define minimum log density \\(\\times 10^{-2s}\\) \\(\\) UMAP parameter output \\(s\\) den_scale parameter. Define maximum log density \\(\\times 10^{2s}\\). Range scale \\(\\log \\left(1 / R_i \\right)\\) minimum maximum log density. Call range scaled log density \\(\\Delta_i\\) \\(a_i = \\sqrt{ \\exp \\left( \\Delta_i \\right) }\\). leopold output weight function \\(w_{ij} = 1 / \\left( 1 + a_i a_j d_{ij}^2 \\right)\\).","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"the-advantage-of-leopold","dir":"Articles","previous_headings":"Approximating densMAP","what":"The Advantage of leopold","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"main advantage : implement don’t write much new code: \\(\\rho\\) \\(\\sigma\\) already calculated implementing per-observation value output function requires creating new version umap gradient routine expects vector values rather scalar. lot less work implementing densMAP.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"the-disadvantage-of-leopold","dir":"Articles","previous_headings":"Approximating densMAP","what":"The Disadvantage of leopold","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"’s bit approximate. really want technique makes dense clusters get smaller diffuse clusters get bigger. exact changes seem less important long vaguely sensible. don’t claim good sense expect see blob data intrinsic dimensionality \\(d\\) radius \\(r\\) embedded 2D versus one intrinsic dimensionality \\(d + 5\\) radius \\(r / 2\\). Even accurately embedded, even look sensible let alone helpful?","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"results","dir":"Articles","previous_headings":"","what":"Results","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"plots using leopold. used exact nearest neighbors results nearest neighbor calculations usually slowest part UMAP. following deviations default parameters used: min_dist = 0.1 closer Python UMAP results. consistency densMAP, n_neighbors = 30 except subset_clusters uses n_neighbors = 150. Also consistent densMAP, extra 200 epochs top usual default n_epochs used. plots : top left: output running leopold, Top left: leopold results colored typical label dataset. Top right: UMAP results point colored log input local radius, using ColorBrewer Spectral palette. Red means small radius, blue means large radius. show areas densMAP likely contract expand, respectively. Bottom left: plot log input local radii vs log output radii Pearson correlation . 1000 points, used smoothScatter plot results. Bottom right: plot log densmap input local radii leopold equivalent (\\(\\rho + \\sigma\\)). number title Pearson correlation. ’s particular reason two measures local radius , show similar trend. lot datasets, won’t anything say unless interesting contrast similarity densMAP.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"two_different_clusters-1","dir":"Articles","previous_headings":"Results","what":"two_different_clusters","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"leopold able produce two different sizes cluster, although size disparity much larger densMAP. due bimodal nature standard deviations dataset. One cluster gets one end radii scale (highly disperse) second cluster gets (highly compact). Turning dens_scale something like 0.2 works well data set. doesn’t really matter artificial dataset. long clusters right relative size happy.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"subset_clusters-1","dir":"Articles","previous_headings":"Results","what":"subset_clusters","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"shows interesting difference densMAP: densMAP, outer yellow ring uniform density, whereas leopold noticeable density gradient towards center. leave decide good bad thing.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"s1k-1","dir":"Articles","previous_headings":"Results","what":"s1k","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"point got pushed far away rest data densMAP plot far away leopold.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"swissroll-1","dir":"Articles","previous_headings":"Results","what":"swissroll","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"one scurvehole seem pretty comparable densMAP results.","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"mammoth-1","dir":"Articles","previous_headings":"Results","what":"mammoth","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"compare densMAP, extremities much diffuse leopold.","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"mnist-1","dir":"Articles","previous_headings":"Results","what":"mnist","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"main thing hoped see orange ‘1’ cluster shrink. . yellow ‘2’ cluster also noticeably grown. Changes subtle clusters, apart ‘1’ cluster densities quite similar. ’m happy .","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"fashion-1","dir":"Articles","previous_headings":"Results","what":"fashion","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"leopold behaves densMAP-like way shrinking orange cluster. clusters densMAP expands also larger , diffuse densMAP makes . personally prefer leopold layout, biased probably just ignore .","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"norb-1","dir":"Articles","previous_headings":"Results","what":"norb","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"layout fine, maybe bit neater UMAP one. still like densMAP version better.","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"macosko2015-1","dir":"Articles","previous_headings":"Results","what":"macosko2015","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"think layout leopold provides legible densMAP equivalent.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"tasic2018-1","dir":"Articles","previous_headings":"Results","what":"tasic2018","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"Another example leopold produces less compressed result, diffuse clusters (e.g. clusters bottom plot) expanded.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"ng20-1","dir":"Articles","previous_headings":"Results","what":"ng20","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"layout look super-great? . can leopold reproduce input radii output space, seems fact worse job densMAP. suffers less outliers densMAP something.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/leopold.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"Leopold: Lightweight Estimate of Preservation of Local Density","text":"probably clear don’t think ’s anything wrong densMAP. just currently lazy make changes uwot need implemented. Hence leopold’s creation. Neither leopold’s definition local radii results exactly map densMAP. don’t go looking reproduce densMAP plots exactly . think leopold results look fine. dense clusters get smaller diffuse clusters get larger, although latter case extent densMAP. However, isn’t big disadvantage perspective. densMAP better job terms correlation input output radii. Qualitatively, densMAP leopold results quite similar. correlations leopold definition local radii densMAP version pretty high real datasets. venture suggest encode quite similar information. want use leopold uwot: Set dens_scale parameter value 0 (exactly like UMAP) 1 (use full range useful values). Using leopold doesn’t seem require extra epochs don’t need worry run normal UMAP number epochs applying leopold. seem benefit larger value n_neighbors. re-run results n_neighbors = 15 default n_epochs noticed big effect compared results , showing already way many images page. can transform new data models generated dens_scale set. UMAP transforms new data, adjusted local connectivity used affects \\(\\sigma\\) \\(\\rho\\) estimated values \\(R_i\\) precise like, effect seems quite small. Even don’t use local radii generate UMAP layout, coloring results log localr quite informative parts embedding expect shrink expand. conservative use leopold run UMAP normally, visualize results coloring localr obvious clusters lighter darker hue, run dens_scale = 1 current coordinates couple hundred epochs. ’s use iris:","code":"iris_leopold <- umap(iris, dens_scale = 1) # and if you want the local radii values iris_leopold <- umap(iris, dens_scale = 1, ret_extra = c(\"localr\"))"},{"path":"https://jlmelville.github.io/uwot/articles/lvish.html","id":"a-largevis-ish-method","dir":"Articles","previous_headings":"","what":"A LargeVis-ish method","title":"lvish","text":"LargeVis method (see also github), many respects bridge t-SNE UMAP. ’s sufficiently close UMAP uwot also offers LargeVis-like method, lvish: Although lvish like real LargeVis terms input weights, output weight function gradient, give results resemble real thing, note : Like real LargeVis, matrix input data normalized centering column entire matrix scaled dividing maximum absolute value. differs umap, scaling carried . Scaling can controlled scale parameter. Nearest neighbor results refined via neighbor expansion method. search_k parameter twice large Annoy’s default compensate. nearest neighbor index parameter, n_trees, dynamically chosen based data set size. LargeVis, ranges 10 (N < 100,000) 100 (N > 5,000,000). lvish default 50 cover datasets N = 5,000,000, combined default search_k, seems suitable datasets ’ve looked . Negative edges generated uniform sampling vertexes rather degree ^ 0.75. default number epochs dataset-dependent, generate number edge samples used default settings reference LargeVis implementation. normally results substantially longer run time umap. may able get away fewer epochs, using UMAP initialization init = \"spectral\", rather default Gaussian random initialization (init = \"lvrand\") can help. left-hand image result running official LargeVis implementation MNIST. image right running lvish default settings (apart setting n_threads = 8). Given initialized different random configurations, ’s reason believe identical, look pretty similar: default number neighbors 3 times perplexity, default perplexity = 50, nearest neighbor search needs find 150 nearest neighbors per data point, order magnitude larger UMAP defaults. leads less sparse input graph hence edges sample. Combined increased number epochs, expect lvish slower umap: default single-threaded settings, took 20 minutes embed MNIST data circumstances described “Performance” section. n_threads = 4, took 7 minutes. addition, storing extra edges requires lot memory umap defaults: R session increased around 3.2 GB, versus 1 GB umap. alternative usual Gaussian input weight function, can use k-nearest neighbor graph , setting kernel = \"knn\". give edge neighbors uniform weight equal 1/perplexity, leads row’s probability distribution target perplexity. matrix symmetrized usual way. advantage number neighbors reduced perplexity (indeed, n_neighbors parameter ignored setting), leads less memory usage faster runtime. can also get away setting perplexity much lower value usual kernel (e.g. perplexity = 15) get closer UMAP’s performance. use default LargeVis random initialization, still need epochs UMAP, can still expect see big improvement. Something like following works MNIST:","code":"# perplexity, init and n_epoch values shown are the defaults # use perplexity instead of n_neighbors to control local neighborhood size mnist_lv <- lvish(mnist, perplexity = 50, init = \"lvrand\", n_epochs = 5000,                   verbose = TRUE) # Make hilarious Lembas bread joke mnist_lv <- lvish(mnist, kernel = \"knn\", perplexity = 15, n_epochs = 1500,                   init = \"lvrand\", verbose = TRUE)"},{"path":"https://jlmelville.github.io/uwot/articles/lvish.html","id":"some-more-results","dir":"Articles","previous_headings":"","what":"Some More Results","title":"lvish","text":"details datasets, compare output UMAP t-SNE, see UMAP examples gallery.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/lvish.html","id":"gaussian-perplexity","dir":"Articles","previous_headings":"Some More Results","what":"Gaussian Perplexity","title":"lvish","text":"mentioned , default lvish uses Gaussian similarity function determine perplexities, just like t-SNE. results given . two images per dataset. left-hand image uses perplexity 15, similar sort settings UMAP uses. right-hand image perplexity 50, LargeVis default. non-default settings use pca = 100, reduces input dimensionality 100. Note default lvish uses random initialization much larger number epochs match LargeVis defaults. makes optimization take lot longer UMAP. LargeVis uses multiple threads optimization phase, lvish , ensure reproducibility results fixed random seed. get multi-threaded performance like LargeVis, add option, n_sgd_threads = \"auto\", e.g.: also suggest fix number epochs smaller value initially see provides adequate visualization.","code":"iris_lv15 <- lvish(iris, pca = 100, perplexity = 15) iris_lv50 <- lvish(iris, pca = 100, perplexity = 50) iris_lv15 <- lvish(iris, pca = 100, perplexity = 15, n_sgd_threads = \"auto\") iris_lv15 <- lvish(iris, pca = 100, perplexity = 15, n_sgd_threads = \"auto\", n_epochs = 500)"},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/lvish.html","id":"macosko2015","dir":"Articles","previous_headings":"Some More Results > Gaussian Perplexity","what":"macosko2015","title":"lvish","text":"Default initialization lvish, LargeVis t-SNE, random distribution. t-SNE, can see one issue sometimes clusters get split another cluster unable re-merge. MNIST easiest example image see . general, ’s huge difference effect increasing perplexity, larger datasets. smaller datasets ’s apparent resulting clusters tend spread larger perplexity values. norb (small NORB) dataset shows obvious difference, perplexity = 15 results clearly low, break structures apparent perplexity = 50. similar effect seen using UMAP, don’t think due random initialization lvish case. contributing factor likely initial PCA dimensionality reduction 100 dimensions aggressive NORB reduces nearest neighbor accuracy, recovered higher perplexities (requires finding near neighbors). hand, ’s hard see ’s going coil20 especially coil100 results. see going static images , apparent , contrast norb results, perplexity = 50 results high , loop structure clusters gets broken . coil100 coil20 results show issue using LargeVis (UMAP) isn’t normally problem t-SNE: extra repulsion cost function can often spread data quite far apart compared cluster sizes. t-SNE opposite problem clusters expanding large circular form makes discerning clusters harder datasets get larger, single static plot, find t-SNE results easier examine. UMAP lvish, may resort interactive means examining data, using embed_plotly function vizier. alternative lvish modify repulsion_strength parameter (referred gamma LargeVis). default value, 7 taken LargeVis paper seems chosen empiricially. results coil20 perplexity = 50 repulsion reduced repulsion_strength = 0.7 left image, repulsion_strength = 0.07 right: helps bit, limits: blue cluster right remains outlier, reducing repulsion_strength far causes loops shrink, can seen outlying blue black clusters right-hand plot.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/lvish.html","id":"knn-perplexity","dir":"Articles","previous_headings":"Some More Results","what":"KNN Perplexity","title":"lvish","text":"alternative using Gaussian perplexities, use k-nearest neighbor graph directly, involves setting similarity \\(\\) \\(j\\) 1 \\(\\) k-nearest neighbors \\(j\\), 0 otherwise. usual t-SNE procedure symmetrizing (normalization step) carried . t-SNE implementations use kNN-derived perplexities, e.g. majorization-minimization approach Yang co-workers. advantage using kNN kernel get sparser set edges, due lvish using formula determining number iterations required, results shorter run time. results kNN perplexities used setting kernel = \"knn\" comparison, top row images use settings previous section Gaussian perplexities, bottom row show results using kNN kernel. run time embedding given image. used version 0.1.3 uwot CRAN.","code":"iris_lv15k <- lvish(iris, pca = 100, perplexity = 15, kernel = \"knn\") iris_lv50k <- lvish(iris, pca = 100, perplexity = 50, kernel = \"knn\")"},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/lvish.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"lvish","text":"smaller datasets, kNN kernel gives noticeably different results Gaussian perplexity, particularly perplexity = 50. iris, s1k oli, trend seems results expanded kNN kernel. frey, coil20 coil100, clusters separated. larger datasets, difference behavior less pronounced, although macosko2015 results show larger cluster separation well. good new cases, run times noticeably reduced. larger datasets, using kernel = \"knn\" seems ok choice reducing runtime lvish. also seems may want use smaller value perplexity Gaussian perplexity, reduces runtime. smaller datasets, results mixed. seems smaller perplexity case definitely preferred.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/metric-learning.html","id":"visualization","dir":"Articles","previous_headings":"","what":"Visualization","title":"Metric Learning with UMAP","text":"produce plots , used vizier package, can installed using: ’ll show commands produce plots displayed.","code":"devtools::install_github(\"jlmelville/vizier\")"},{"path":"https://jlmelville.github.io/uwot/articles/metric-learning.html","id":"supervised-learning","dir":"Articles","previous_headings":"","what":"Supervised Learning","title":"Metric Learning with UMAP","text":"’ll compare supervised result standard run UMAP: supervised learning, provide suitable vector labels y argument umap (tumap): Let’s take look results, unsupervised embedding left, supervised version right: Clearly, supervised UMAP done much better job separating classes, although also retained relative location clusters pretty well, .","code":"set.seed(1337) fashion_umap <- umap(fashion) set.seed(1337) fashion_sumap <- umap(fashion, y = fashion$Description) vizier::embed_plot(fashion_umap, fashion, cex = 0.5, title = \"Fashion UMAP\", alpha_scale = 0.075) vizier::embed_plot(fashion_sumap, fashion, cex = 0.5, title = \"Fashion Supervised UMAP\", alpha_scale = 0.075)"},{"path":"https://jlmelville.github.io/uwot/articles/metric-learning.html","id":"metric-learning","dir":"Articles","previous_headings":"","what":"Metric Learning","title":"Metric Learning with UMAP","text":"’s also possible use existing embedding embed new points. Fashion MNIST comes suggested split training (first 60,000 images) test (remaining 10,000 images) sets, ’ll use : Training proceeds running UMAP normally, need return just embedded coordinates. return enough information embed new data, need set ret_model flag run umap. return list. embedded coordinates can found embedding item.","code":"fashion_train <- head(fashion, 60000) fashion_test <- tail(fashion, 10000)"},{"path":"https://jlmelville.github.io/uwot/articles/metric-learning.html","id":"training","dir":"Articles","previous_headings":"Metric Learning","what":"Training","title":"Metric Learning with UMAP","text":"training, shall continue use standard UMAP: supervised UMAP: results shouldn’t different full-dataset embeddings, let’s take look anyway: Everything looks order . standard UMAP training plot flipped along y-axis compared full dataset, doesn’t matter.","code":"set.seed(1337) fashion_umap_train <- umap(fashion_train, ret_model = TRUE) set.seed(1337) fashion_sumap_train <- umap(fashion_train, ret_model = TRUE, y = fashion_train$Description) vizier::embed_plot(fashion_umap_train$embedding, fashion_train, cex = 0.5, title = \"Fashion Train UMAP\", alpha_scale = 0.075) vizier::embed_plot(fashion_sumap_train$embedding, fashion_train, cex = 0.5, title = \"Fashion Train Supervised UMAP\", alpha_scale = 0.075)"},{"path":"https://jlmelville.github.io/uwot/articles/metric-learning.html","id":"embedding-new-data","dir":"Articles","previous_headings":"Metric Learning","what":"Embedding New Data","title":"Metric Learning with UMAP","text":"embed new data, use umap_transform function. Pass new data trained UMAP model. ’s difference using standard UMAP model: supervised UMAP model: results: test data results obviously embedded similar way training data. particular interest test results supervised model, clusters stay well separated compared unsupervised results, although misclassifications shirts, t-shirts, coats pullover classes (green, blue red clusters right supervised UMAP plot).","code":"set.seed(1337) fashion_umap_test <- umap_transform(fashion_test, fashion_umap_train) set.seed(1337) fashion_sumap_test <- umap_transform(fashion_test, fashion_sumap_train) vizier::embed_plot(fashion_umap_test, fashion_test, cex = 0.5, title = \"Fashion Test UMAP\", alpha_scale = 0.075) vizier::embed_plot(fashion_sumap_test, fashion_test, cex = 0.5, title = \"Fashion Test Supervised UMAP\", alpha_scale = 0.075)"},{"path":"https://jlmelville.github.io/uwot/articles/metric-learning.html","id":"accuracy-results","dir":"Articles","previous_headings":"Metric Learning","what":"Accuracy Results","title":"Metric Learning with UMAP","text":"quantify improvement, can look accuracy predicting test set labels using embedded coordinates k-nearest neighbor classifier. variety ways can imagine using information model, two obvious ones use label nearest neighbor, (1NN) take vote using n_neighbors (case, 15) nearest neighbors (15NN). standard UMAP, 1NN accuracy 71%, 15NN accuracy 77%. Using supervised UMAP, accuracies improve 83% 84%, respectively. quantitatively, supervised UMAP big help correctly classifying test data. put numbers perspective, can carry similar calculations using input data directly. , 1NN accuracy 85% 15NN accuracy 84%. Possibly, lack improvement going 1 15 neighbors indicates different value n_neighbors parameter improve embedding, haven’t pursued . rate, ’s clear Fashion MNIST images embed well two dimensions, although supervised UMAP gets impressively close matching high dimensional results. Maybe supervised UMAP can even better suitable choice target_weight n_components top fiddling n_neighbors. Fashion MNIST website contains page shows accuracy using 129 scikit-learn methods, 15NN supervised UMAP accuracy puts us top 60, isn’t bad, considering hyperparameter search look 1NN 15NN. However, although highest accuracy reported page 89.7%, deep learning results achieve 90-97%.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/metric-learning.html","id":"supervised-umap-numerical-y","dir":"Articles","previous_headings":"","what":"Supervised UMAP: Numerical Y","title":"Metric Learning with UMAP","text":"’s example using supervised UMAP numerical target vector. shall use diamonds dataset comes ggplot2 package, similar size MNIST. 10 variables associated diamond: five numeric values related geometry diamonds (table, x, y, z depth), three factors measure quality diamond (cut, color clarity), price dollars. price seems like perfect candidate sort thing ’d want target vector, leaving nine variables used dimensionality reduction. uwot’s implementation UMAP uses numeric columns can find calculations, avoid including price non-supervised part UMAP, let’s create new data frame, initially geometric data: depth column related x, y z (albeit non-linearly) ’m going include . Additionally, factors cut, color clarity ordinal variables, .e. categories can ordered, can convert numeric scale include well: now dataset 53,940 rows 8 columns. 360 duplicates, doesn’t seem affect results particularly. Now, ’m saying trickiest dataset extract meaning . First, let’s look standard unsupervised results. starters, ’s plot first two principal components, using irlba package: different columns different units meaning, set scale. = TRUE equalize variances. color scheme “Spectral” palette ColorBrewer: red indicates low price blue high price. Despite majority dataset clumped together plot due outliers can’t really see, progression prices low high already pretty well captured two components. Anyway, let’s see UMAP . Like PCA, columns scaled equal variance (scale = TRUE): bad. high price diamonds clumped together little clusters middle plot. occasion, prefer layout ’s initialized PCA results, though: maintains global structure PCA result. Rather separately create PCA, can also use init = \"pca\" get results (uwot uses irlba internally , ’s loss speed). Onto supervised result. Results particularly affected choice initialization, simplicity ’ll just use standard spectral initialization: expected, embedding now even well-organized along price diamonds. visible gap lowest price diamonds (right) rest embedding. increase n_epochs parameter allow optimization proceed, gap increases substantially, making plot harder read. Adjusting n_epochs parameter, along target_n_neighbors target_weight parameters may required strike right balance. time writing, ’m aware many examples supervised UMAP numeric vector (fact none except thing just wrote) provide lot sage wisdom matter.","code":"library(ggplot2) ?diamonds dia <- diamonds[, c(\"carat\", \"x\", \"y\", \"z\", \"table\")] dia$cut <- as.numeric(diamonds$cut) dia$color <- as.numeric(diamonds$color) dia$clarity <- as.numeric(diamonds$clarity) dia_pca <- irlba::prcomp_irlba(dia, n = 2, scale. = TRUE) vizier::embed_plot(dia_pca$x, diamonds$price, title = \"Diamonds PCA\", color_scheme = \"RColorBrewer::Spectral\", alpha_scale = 0.1, cex = 0.5, pc_axes = TRUE) dia_umap <- umap(dia, scale = TRUE, verbose = TRUE) vizier::embed_plot(dia_umap, diamonds$price, title = \"Diamonds UMAP\", color_scheme = \"RColorBrewer::Spectral\", alpha_scale = 0.1, cex = 0.5, pc_axes = TRUE) dia_umap_from_pca <- umap(dia, scale = TRUE, verbose = TRUE, init = dia_pca$x) vizier::embed_plot(dia_umap_from_pca, diamonds$price, title = \"Diamonds UMAP (PCA init)\", color_scheme = \"RColorBrewer::Spectral\", alpha_scale = 0.1, cex = 0.5, pc_axes = TRUE) dia_sumap <- umap(dia, scale = TRUE, verbose = TRUE, y = diamonds$price) vizier::embed_plot(dia_sumap, diamonds$price, title = \"Diamonds Supervised UMAP\", color_scheme = \"RColorBrewer::Spectral\", alpha_scale = 0.1, cex = 0.5, pc_axes = TRUE)"},{"path":"https://jlmelville.github.io/uwot/articles/mixed-data-types.html","id":"indexing","dir":"Articles","previous_headings":"","what":"Indexing","title":"Mixed Data Types","text":"iris example shows, using column names can verbose. Integer indexing supported, equivalent using integer indexing columns iris : internally, uwot strips non-numeric columns data, use Z-scaling (.e. specify scale = \"Z\"), zero variance columns also removed. likely change index columns. really want use numeric column indexes, strongly advise using scale argument re-arranging data frame necessary non-numeric columns come numeric columns.","code":"metric = list(\"euclidean\" = 3:4, \"euclidean\" = 1:2)"},{"path":"https://jlmelville.github.io/uwot/articles/mixed-data-types.html","id":"categorical-columns","dir":"Articles","previous_headings":"","what":"Categorical columns","title":"Mixed Data Types","text":"supervised UMAP allows factor column used. may now also specify factor columns X data. Use special metric name \"categorical\". example, use Species factor standard UMAP iris along usual four numeric columns, use: Factor columns treated differently numeric columns: always treated separately, one column time. two factor columns, cat1, cat2, like included UMAP, write: convenience, can also write: doesn’t combine cat1 cat2 one block, just saves typing. way categorical data intersected simplicial set, X metric specifies categorical entries. must specify least one standard Annoy metrics numeric data. iris, following error: Specifying numeric columns required: Factor columns explicitly included metric still removed usual. Categorical data appear model returned ret_model = TRUE affect project data used umap_transform. can still use UMAP model project new data, factor columns new data ignored (effectively working like supervised UMAP).","code":"metric = list(\"euclidean\" = 1:4, \"categorical\" = \"Species\") metric = list(\"categorical\" = \"cat1\", \"categorical\" = \"cat2\", ...) metric = list(\"categorical\" = c(\"cat1\", \"cat2\"), ...) # wrong and bad metric = list(\"categorical\" = \"Species\") # OK metric = list(\"categorical\" = \"Species\", \"euclidean\" = 1:4)"},{"path":"https://jlmelville.github.io/uwot/articles/mixed-data-types.html","id":"overriding-global-options","dir":"Articles","previous_headings":"","what":"Overriding global options","title":"Mixed Data Types","text":"global parameters can overridden specific data block providing list value metric, containing vector columns unnamed element, -riding keyword arguments. example: case, first euclidean block reduced 40 dimensions PCA centering applied. second euclidean block PCA applied . manhattan block PCA applied , centering carried . Currently, pca pca_center supported overriding method, feature exists allow case mixed real-valued binary data, want carry PCA . ’s typical carry centering real-value data PCA, binary data.","code":"umap(     X,     pca = 40,     pca_center = TRUE,     metric = list(       euclidean = 1:200,       euclidean = list(201:300, pca = NULL),       manhattan = list(300:500, pca_center = FALSE)     )   )"},{"path":"https://jlmelville.github.io/uwot/articles/mixed-data-types.html","id":"y-data","dir":"Articles","previous_headings":"","what":"y data","title":"Mixed Data Types","text":"handling y data extended allow data frames, target_metric works like metric: multiple numeric blocks different metrics can specified, categorical data can specified categorical. However, unlike X, default behavior y include factor columns. numeric data found treated one block, multiple numeric columns want treated separately, specify column separately: suspect vast majority y data one column, default behavior fine time.","code":"target_metric = list(\"euclidean\" = 1, \"euclidean\" = 2, ...)"},{"path":"https://jlmelville.github.io/uwot/articles/nearest-neighbors-format.html","id":"nearest-neighbor-graph-format","dir":"Articles","previous_headings":"","what":"Nearest Neighbor Graph Format","title":"Nearest Neighbor Format","text":"format expected nn_method list containing following two entries: idx: matrix dimension n_vertices x n_neighbors, row contains indexes (starting 1) nearest neighbors item (vertex) dataset. item always nearest neighbor , first element row always . isn’t either using really weird non-metric distance approximate nearest neighbor method returning way approximate results. either case, expect bad results. dist: matrix dimension n_vertices x n_neighbors, row contains distances nearest neighbors item (vertex) dataset, item always nearest neighbor , first element row always 0.0.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/nearest-neighbors-format.html","id":"sparse-distance-matrix-format","dir":"Articles","previous_headings":"","what":"Sparse Distance Matrix Format","title":"Nearest Neighbor Format","text":"Alternatively, can pass sparse distance matrix : format dgCMatrix (typical sparse matrix format). non-zero entries distances. put another way: neighbor distances arranged non-zero entries ith column matrix contains distances observation nearest neighbors. advantage using sparse distance matrix: restricted fixed value n_neighbors observation. column can contain different number non-zero distances. See paper Dalmia Sia might want . graph edge weight calculation adjusted account different number neighbors observation. must least one neighbor observation. Explicit zero distances removed matrix. contrast use nearest neighbor list matrix format typically zero distance observation found part nearest neighbor search routine. sparse distance matrix approach account zero self-distance implicit. keep explicit zero distances observations set small non-zero value, e.g. 1e-10. slight disadvantage using distance matrix distances need sorted. Sparse distance matrix input currently supported lvish method. use pre-computed nearest neighbor data, aware : can’t use pre-computed nearest neighbor data also use metric. can explicitly set X NULL, long don’t try use initialization method makes use X (init = \"pca\" init = \"spca\"). can transform new data setting ret_model = TRUE. must provide umap_transform distances new data original data via nn_method parameter. ’s example using pre-computed nearest neighbor data using even-numbered observations iris build initial model transforming odd-numbered observations. relies internal uwot functions promise stable API (.e. may example may broken read ), gives general idea:","code":"iris_even <- iris[seq(2, nrow(iris), 2), ] iris_odd <- iris[seq(1, nrow(iris), 2), ]  iris_even_nn <- uwot:::annoy_nn(   X = uwot:::x2m(iris_even),   k = 15,   metric = \"euclidean\",   ret_index = TRUE )  iris_odd_nn <- annoy_search(   X = uwot:::x2m(iris_odd),   k = 15,   ann = iris_even_nn$index )  # Delete the Annoy index, force the transform method to use the nn distances  # directly iris_even_nn$index <- NULL  iris_even_umap <-   umap(     X = NULL,     nn_method = iris_even_nn,     ret_model = TRUE   )  iris_odd_transform <-   umap_transform(X = NULL, iris_even_umap, nn_method = iris_odd_nn)"},{"path":"https://jlmelville.github.io/uwot/articles/nearest-neighbors-format.html","id":"exporting-nearest-neighbor-data-from-uwot","dir":"Articles","previous_headings":"","what":"Exporting nearest neighbor data from uwot","title":"Nearest Neighbor Format","text":"set ret_nn = TRUE, return value umap list, nn item contains nearest neighbor data format can used nn_method. handy going running UMAP multiple times data n_neighbors scale settings, nearest neighbor calculation can time-consuming part calculation. Normally contents nn list, value nearest neighbor data. name type metric generated data. example, ’s first items iris 5-NN data look like: reason specify ret_nn supplying precomputed nearest neighbor data nn_method, returned data identical passed , list item names precomputed.","code":"lapply(umap(iris, ret_nn = TRUE, n_neighbors = 5)$nn$euclidean, head)  $`idx`      [,1] [,2] [,3] [,4] [,5] [1,]    1   18    5   40   29 [2,]    2   35   46   13   10 [3,]    3   48    4    7   13 [4,]    4   48   30   31    3 [5,]    5   38    1   18   41 [6,]    6   19   11   49   45  $dist      [,1]      [,2]      [,3]      [,4]      [,5] [1,]    0 0.1000000 0.1414214 0.1414214 0.1414214 [2,]    0 0.1414214 0.1414214 0.1414214 0.1732051 [3,]    0 0.1414214 0.2449490 0.2645751 0.2645751 [4,]    0 0.1414214 0.1732051 0.2236068 0.2449490 [5,]    0 0.1414214 0.1414214 0.1732051 0.1732051 [6,]    0 0.3316625 0.3464102 0.3605551 0.3741657"},{"path":"https://jlmelville.github.io/uwot/articles/nearest-neighbors-format.html","id":"multiple-neighbor-data","dir":"Articles","previous_headings":"","what":"Multiple neighbor data","title":"Nearest Neighbor Format","text":"discussed Mixed Data Types article, can apply multiple distance metrics different parts matrix data frame input data. , ret_nn return neighbor data. list nn now contain many items metrics, order specified. instance, metric argument : nn list contain two list entries. first called euclidean second cosine. access multiple distance metrics, may also provide multiple precomputed neighbor data nn_method format: list lists, sublist format described (.e. two matrices, idx dist). names list items ignored, don’t need set . Roughly, something like : different neighbor data must number neighbors, .e. number columns matrices must .","code":"metric = list(\"euclidean\" = c(\"Petal.Width\", \"Petal.Length\"),               \"cosine\" = c(\"Sepal.Width\", \"Sepal.Length\")) nn_metric1 <- list(idx = matrix(...), dist = matrix(...)) nn_metric2 <- list(idx = matrix(...), dist = matrix(...)) umap_res <- umap(nn_method = list(nn_metric1, nn_metric2), ...)"},{"path":"https://jlmelville.github.io/uwot/articles/nearest-neighbors-format.html","id":"numeric-y","dir":"Articles","previous_headings":"","what":"Numeric y","title":"Nearest Neighbor Format","text":"using supervised UMAP numeric y, can also pass nearest neighbor data y, using format . case nearest neighbors respect data y. Note pass categorical y nearest neighbor data. processing data goes different code path doesn’t directly calculate nearest neighbors: y factor, small number levels, number neighbors item can vastly larger n_neighbors. Nearest neighbor data y returned umap re-use.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/performance.html","id":"performance","dir":"Articles","previous_headings":"","what":"Performance","title":"Performance","text":"December 31 2018 Updated timings, keeping better track versions numbers. get feel performance uwot, timings processing MNIST dataset, compared methods. wouldn’t take seriously, show uwot competitive methods. notes numbers generated: ran Windows machine, using R 3.5.2 Python 3.7.0. official LargeVis implementation built Visual Studio 2017 Community Edition may properly optimized (VS solution available fork). R packages, MNIST data downloaded via snedata package. Python packages, sklearn.datasets.fetch_mldata('MNIST original') used. LargeVis source code contains MNIST example data already present. FIt-SNE, used provided Windows binary via R wrapper (hence used MNIST data snedata package). reported time second FIt-SNE entry table includes 13 seconds takes reduce dimensionality 50 via PCA, using irlba (package dimension reduction used Rtsne last reported time uwot). default openTSNE uses FFT approach FIt-SNE , don’t know ’s much slower, apart use numpy version FFT rather FFTW library, understanding shouldn’t make much difference dataset size MNIST. Perhaps Windows thing. uwot, bottleneck typical settings nearest neighbor search, currently provided Annoy, whereas Python implementation uses pynndescent, nearest neighbor descent approach. optimization side things, uwot defaults conservative. Using approx_pow = TRUE uses fastPrecisePow approximation pow function suggested Martin Ankerl. think seem like typical values b (0.7 0.9) squared distance (0-1000), found maximum relative error 0.06. However, haven’t done much testing, beyond looking see results examples page obviously worsened. Results table approx_pow = TRUE show worthwhile improvement. Using n_sgd_threads 1 thread give reproducible results, behave worse LargeVis regard, many visualization needs, also worth trying.","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/pycompare.html","id":"norb","dir":"Articles","previous_headings":"Results","what":"norb","title":"Python Comparison","text":"’s result seems worth commenting . uwot results show slightly less structure, large blue loop seems broken . suspect PCA preprocessing 100 dimensions used uwot aggressive might throwing away much variance dataset. Compared ground truth 15 nearest neighbors using FNN, Annoy results pca = 100 82% accurate. Without PCA, accuracy goes 99% uwot results much closer UMAP results. image , left. Meanwhile, use PCA results input UMAP, output now looks bit like uwot results. ’s image lower right. Unfortunately, high dimensionality norb makes running Annoy directly slow indeed: single-thread, took 4 half hours, versus around 5 minutes PCA case. revisit nearest neighbor accuracies later. Note also norb one datasets graph contains two separate components, initialization output coordinates different uwot UMAP, UMAP’s “meta-embedding” approach better retaining local structure.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/pycompare.html","id":"macosko2015","dir":"Articles","previous_headings":"Results","what":"macosko2015","title":"Python Comparison","text":"result shows largest deviation uwot UMAP. big cyan cluster obviously different shape two plots. source difference seems nearest neighbor results. turns uwot UMAP trouble find good approximations nearest neighbors dataset, UMAP lot better. get detail nearest neighbor accuracies across datasets next section. now, two plots. left-hand plot uwot results uses accurate nearest neighbor data calculated UMAP: uwot results now resemble UMAP results, reassuring. right hand plot uses exact nearest neighbor results calculated via FNN. Even though UMAP nearest neighbor data better uwot produced, can still see obvious difference two plots, ’s clear data set presents challenge approximate nearest neighbor methods considered .","code":""},{"path":"https://jlmelville.github.io/uwot/articles/pycompare.html","id":"cifar10","dir":"Articles","previous_headings":"Results","what":"cifar10","title":"Python Comparison","text":"Based results dataset UMAP examples gallery, wasn’t expecting results feast eyes. seeing difference results macosko2015, just relieved two results look disappointing similar way.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/pycompare.html","id":"nearest-neighbor-accuracy","dir":"Articles","previous_headings":"","what":"Nearest Neighbor Accuracy","title":"Python Comparison","text":"norb macosko2015 results suggest use PCA nearest neighbor search methods present biggest source difference uwot UMAP. details. small datasets, implementations means less 4,096 observations, uwot UMAP calculate exact nearest neighbors. larger datasets, uwot uses Annoy method, UMAP uses pynndescent, uses random projection trees, followed nearest neighbor descent. larger datasets, used FNN package generate exact 15 nearest neighbors compared results uwot UMAP. UMAP, used default settings. uwot, applied pca = 100 currently use high dimensional datasets. third column shows amount variance explained keeping 100 components. UMAP pretty incredible job nearly every dataset, ’s fast . doesn’t need use PCA reduce dimensionality. obvious exception macosko2015. unable find combination parameters pynndescent produced good results, e.g. increasing number trees RP tree initialization number iterations nearest neighbor descent phase. n_trees=250, able get 54.9% accuracy, started get memory errors much beyond setting, nearest neighbor descent tends converge 5-6 iterations anyway. better may require manipulation sampling rate candidate list size used nearest neighbor descent, can quickly cause large increase memory run time aren’t careful. Also, can’t fiddle parameters UMAP version 0.3.8 anyway. uwot results… well, ’re less impressive UMAP’s. tasic2018 results pretty bad, macosko2015 results can fairly described horrendous. accuracies show rough correlation percentage variance explained 100 components. Results tasic2018 especially macosko2015 show 100 components may enough extract meaningful variation datasets, might explain inability find true nearest neighbors. uwot defaults Annoy search somewhat arbitrary, copied LargeVis. package used nearest neighbor descent improve accuracy, increasing Annoy’s n_trees parameter can give small improvement datasets, increase computation time probably worth small increase accuracy ’ve seen trying . seems cases current defaults don’t work well, ’s much information thrown away PCA pre-processing. tasic2018 macosko2015 transcriptomics datasets clearly represent interesting challenge Annoy, macosko2015 troubles even pynndescent. datasets image datasets. Possibly way data scaled prepared biology datasets makes life harder approximate nearest neighbors? ’s hard know start comes unpicking causes macosko2015 perform differently, histogram nearest neighbor distances macosko2015 tasic2018 (top row), misbehave Annoy, bottom row histograms mnist fashion, better behaved: biomodal distribution macosko2015 tasic2018 seem similar mnist fashion. said, tasic2018 behaves perfectly well pynndescent method, see obvious answers .","code":""},{"path":"https://jlmelville.github.io/uwot/articles/pycompare.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"Python Comparison","text":"good news datasets, differences uwot’s UMAP implementation Python UMAP implementation makes little difference. bad news couple cases see differences. biggest offender macosko2015 neither method perfect job. Nonetheless, UMAP able calculate nearest neighbors accurately quickly uwot. One source difference use Annoy nearest neighbor calculation method. seem solidly slower pynndescent, experience, can reach high accuracies. even takes . March 8 2020: Windows, used issue Annoy index larger 2GB disk couldn’t read back . Make sure using version RcppAnnoy 0.0.15 later avoid . time search Annoy index seems start scaling quite badly dimensionality > 1000, can get away picking 500-1000 random features use , try . 1,152 random features (.e. 1/16th pixels innorb), visual results aren’t much worse approach nearest neighbor search runs 100 times faster (50 minutes vs 20 seconds): Otherwise, high-dimensional dataset, consider PCA. don’t solid advice number components retain use PCA preprocessing. Less usual t-SNE default 50 probably unwise. use pca = 100 examples, ’ve seen, can still sufficiently perturbs pairwise distances input data give differences output. nice use components get better trade-accuracy speed, even fast partial PCA routines used uwot (courtesy irlba), extracting 100 components can quite time-consuming.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/pycompare.html","id":"what-about-t-sne","dir":"Articles","previous_headings":"Conclusions","what":"What about t-SNE?","title":"Python Comparison","text":"uwot dimensionality reduction use PCA Annoy approximate nearest neighbor search. LargeVis uses , t-SNE package Fit-SNE Annoy option openTSNE seems least considered (see e.g. https://github.com/pavlin-policar/openTSNE/issues/28). also standard practice t-SNE reduce dimensionality: Rtsne, wrapper around de-facto standard Barnes Hut t-SNE routine. keep 50 components via PCA default. PCA issue effect t-SNE? Well, little bit. results norb macosko2015 (openTSNE’s documentation discovered dataset, happens) t-SNE results Rtsne, using perplexity = 15. images left use PCA reduce initial dimensionality 100, ones right use raw input data. norb results seem fairly unaffected, also see macosko2015 ’s change main large cluster. Usually, don’t set perplexity low 15, also differences perplexity set typical value, like 50? Also, default PCA dimensionality setting Rtsne 50, 100, t-SNE plots repeated perplexity = 50 either pca = TRUE, partial_pca = TRUE, initial_dims = 50 left, pca = FALSE right: , norb dataset less affected macosko2015 latter definite effect preprocessing PCA. something bear mind applying PCA data even t-SNE. balance, ’d probably live effect applying PCA data, just like uwot, Rtsne input processing can take hours high dimensional datasets. Finally, people consider results obtained PCA reliable using “raw” input, PCA can considered way denoise data, lower-variance components assumed irrelevant. might bit biased, UMAP plots macosko2015, think default uwot one shows best separation clusters, maybe ’s something . also reason tabulated amount variance 100 components explained dataset. datasets keep large majority variance 100 components, case macosko2015, prepared argue ok throw away 60% variance data. ’m sure can done, need solid, ideally biology-based justification. future version uwot hopefully contain implementation nearest neighbor descent optimization reduce gap nearest neighbor accuracy UMAP.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/rnndescent-umap.html","id":"using-rnndescent","dir":"Articles","previous_headings":"","what":"Using rnndescent","title":"Using rnndescent for nearest neighbors","text":"Now install rnndescent CRAN:","code":"install.packages(\"rnndescent\") library(rnndescent)"},{"path":"https://jlmelville.github.io/uwot/articles/rnndescent-umap.html","id":"build-an-index-for-the-training-data","dir":"Articles","previous_headings":"Using rnndescent","what":"Build an index for the training data","title":"Using rnndescent for nearest neighbors","text":"First, build search index using training data. use many threads (n_threads) feel comfortable . can also set verbose = TRUE, building index see lot output won’t reproduce . k parameter number nearest neighbors looking . index tuned work well number neighbors. metric parameter distance metric use, default Euclidean, don’t need specify . Finally, stochastic aspect index building. may get slightly different results , leaving seed unset must live inherently random nature index building process. ’s returned index? bunch stuff: support searching index. want graph. list two elements, idx dist, contain nearest neighbor indices distances respectively. nearest neighbor item , expect first column idx sequence 1, 2, 3, … first column dist zeros. nearest neighbor methods like HNSW Annoy, run separate query step get k-nearest neighbors data used build index. One advantages rnndescent can get data directly index without separate query step. Note can still query index want , results might bit accurate, quietly confident results index sufficient UMAP.","code":"fashion_index <- rnnd_build(fashion_train, k = 15, n_threads = 6) names(fashion_index) [1] \"graph\"           \"prep\"            \"data\"            \"original_metric\" \"use_alt_metric\"  \"search_forest\"   \"search_graph\" fashion_index$graph$idx[1:3, 1:3] [,1]  [,2]  [,3] [1,]    1 25720 27656 [2,]    2 42565 37551 [3,]    3 53514 35425 fashion_index$graph$dist[1:3, 1:3] [,1]      [,2]      [,3] [1,]    0 1188.7826 1215.3440 [2,]    0 1048.0482 1068.3951 [3,]    0  532.6199  632.1653"},{"path":"https://jlmelville.github.io/uwot/articles/rnndescent-umap.html","id":"query-the-test-data","dir":"Articles","previous_headings":"Using rnndescent","what":"Query the test data","title":"Using rnndescent for nearest neighbors","text":"get test set neighbors, query index test data: querying test data training data, nearest neighbor test item test item . now nearest neighbor data need. good news ’s already format uwot can use can proceed straight running UMAP.","code":"fashion_test_query_neighbors <-   rnnd_query(     index = fashion_index,     query = fashion_test,     k = 15,     n_threads = 6,     verbose = TRUE   ) fashion_test_query_neighbors$idx[1:3, 1:3] [,1]  [,2]  [,3] [1,] 18095 53940 18353 [2,]  8573 31349  3885 [3,]   286 38144  3422 fashion_test_query_neighbors$dist[1:3, 1:3] [,1]      [,2]      [,3] [1,]  482.2966  681.9905  708.4991 [2,] 1308.0019 1329.3134 1382.7317 [3,]  466.0322  538.5378  555.8795"},{"path":"https://jlmelville.github.io/uwot/articles/rnndescent-umap.html","id":"using-rnndescent-nearest-neighbors-with-umap","dir":"Articles","previous_headings":"","what":"Using rnndescent nearest neighbors with UMAP","title":"Using rnndescent for nearest neighbors","text":"","code":"library(uwot)"},{"path":"https://jlmelville.github.io/uwot/articles/rnndescent-umap.html","id":"umap-on-training-data","dir":"Articles","previous_headings":"Using rnndescent nearest neighbors with UMAP","what":"UMAP on training data","title":"Using rnndescent for nearest neighbors","text":"use pre-computed nearest neighbor data uwot pass nn_method parameter. case, graph item fashion_index. See HSNW article details parameters, designed give pretty typical UMAP results.","code":"fashion_train_umap <-   umap(     X = NULL,     nn_method = fashion_index$graph,     batch = TRUE,     n_epochs = 500,     n_sgd_threads = 6,     ret_model = TRUE,     verbose = TRUE   ) UMAP embedding parameters a = 1.896 b = 0.8006 Commencing smooth kNN distance calibration using 6 threads with target n_neighbors = 15 Initializing from normalized Laplacian + noise (using irlba) Commencing optimization for 500 epochs, with 1359454 positive edges using 6 threads Using method 'umap' Optimizing with Adam alpha = 1 beta1 = 0.5 beta2 = 0.9 eps = 1e-07 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----| **************************************************| Optimization finished Note: model requested with precomputed neighbors. For transforming new data, distance data must be provided separately"},{"path":"https://jlmelville.github.io/uwot/articles/rnndescent-umap.html","id":"transforming-test-data","dir":"Articles","previous_headings":"Using rnndescent nearest neighbors with UMAP","what":"Transforming test data","title":"Using rnndescent for nearest neighbors","text":"Now UMAP model can transform test set data. don’t need pass test set data except neighbors nn_method:","code":"fashion_test_umap <-   umap_transform(     X = NULL,     model = fashion_train_umap,     nn_method = fashion_test_query_neighbors,     n_sgd_threads = 6,     verbose = TRUE   ) Read 10000 rows Processing block 1 of 1 Commencing smooth kNN distance calibration using 6 threads with target n_neighbors = 15 Initializing by weighted average of neighbor coordinates using 6 threads Commencing optimization for 167 epochs, with 150000 positive edges using 6 threads Using method 'umap' Optimizing with Adam alpha = 1 beta1 = 0.5 beta2 = 0.9 eps = 1e-07 0%   10   20   30   40   50   60   70   80   90   100% [----|----|----|----|----|----|----|----|----|----| **************************************************| Finished"},{"path":"https://jlmelville.github.io/uwot/articles/rnndescent-umap.html","id":"plotting-the-results","dir":"Articles","previous_headings":"Using rnndescent nearest neighbors with UMAP","what":"Plotting the results","title":"Using rnndescent for nearest neighbors","text":"Now take look results, using ggplot2 plotting, Polychrome suitable categorical palette. following code creates palette 10 (hopefully) visually distinct colors map point type fashion item represents. found Description factor column original data. results:  results typical Fashion MNIST result UMAP. example, see first image part Python UMAP documentation. looks like rnndescent default settings good job dataset.","code":"install.packages(c(\"ggplot2\", \"Polychrome\")) library(ggplot2) library(Polychrome) palette <- as.vector(Polychrome::createPalette(   length(levels(fashion$Description)) + 2,   seedcolors = c(\"#ffffff\", \"#000000\"),   range = c(10, 90) )[-(1:2)]) ggplot(   data.frame(fashion_train_umap$embedding, Description = fashion_train$Description),   aes(x = X1, y = X2, color = Description) ) +   geom_point(alpha = 0.1, size = 1.0) +   scale_color_manual(values = palette) +   theme_minimal() +   labs(     title = \"Fashion MNIST training set UMAP\",     x = \"\",     y = \"\",     color = \"Description\"   ) +   theme(legend.position = \"right\") +   guides(color = guide_legend(override.aes = list(size = 5, alpha = 1))) ggplot(   data.frame(fashion_test_umap, Description = fashion_test$Description),   aes(x = X1, y = X2, color = Description) ) +   geom_point(alpha = 0.4, size = 1.0) +   scale_color_manual(values = palette) +   theme_minimal() +   labs(     title = \"Fashion MNIST test set UMAP\",     x = \"\",     y = \"\",     color = \"Description\"   ) +   theme(legend.position = \"right\") +   guides(color = guide_legend(override.aes = list(size = 5, alpha = 1)))"},{"path":"https://jlmelville.github.io/uwot/articles/rnndescent-umap.html","id":"a-potentially-simpler-workflow","dir":"Articles","previous_headings":"Using rnndescent nearest neighbors with UMAP","what":"A potentially simpler workflow","title":"Using rnndescent for nearest neighbors","text":"don’t want transform new data, can make life bit easier. Instead rnnd_build, use rnnd_knn behaves lot like rnnd_build doesn’t preserve index index preparation. saves time nearest neighbor results returned directly return result. Let’s use full Fashion MNIST results example: can pass fashion_knn nn_method: (spared log output). don’t need pass ret_model = TRUE can’t transform new data. UMAP plot full Fashion MNIST dataset: Everything looks expected. However, think might want transform new data future, need completely restart process, rnnd_build using ret_model = TRUE.","code":"fashion_knn <- rnnd_knn(fashion, k = 15, n_threads = 6) names(fashion_knn) [1] \"idx\"  \"dist\" fashion_umap <-   umap(     X = NULL,     nn_method = fashion_knn,     batch = TRUE,     n_epochs = 500,     n_sgd_threads = 6,     verbose = TRUE   ) ggplot(   data.frame(fashion_umap, Description = fashion$Description),   aes(x = X1, y = X2, color = Description) ) +   geom_point(alpha = 0.1, size = 1.0) +   scale_color_manual(values = palette) +   theme_minimal() +   labs(     title = \"Fashion MNIST UMAP\",     x = \"\",     y = \"\",     color = \"Description\"   ) +   theme(legend.position = \"right\") +   guides(color = guide_legend(override.aes = list(size = 5, alpha = 1)))"},{"path":"https://jlmelville.github.io/uwot/articles/rnndescent-umap.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"Using rnndescent for nearest neighbors","text":"want use rnndescent uwot, : Run rnndescent::rnnd_build training data. Run uwot::umap nn_method set graph item result rnnd_build remember set ret_model = TRUE. transform new data: Run rnndescent::rnnd_query test data, using result rnnd_build index parameter. Run uwot::umap_transform nn_method set result rnnd_query. don’t want transform new data, ’s even easier: Run rnndescent::rnnd_knn training data. Run uwot::umap nn_method set result rnnd_knn.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"text-analysis-of-the-20-newsgroups-dataset","dir":"Articles","previous_headings":"","what":"Text Analysis of the 20 Newsgroups Dataset","title":"Working with Sparse Data","text":"great use case working sparse data TF-IDF text analysis. Despite prevalence deep-learning-based text embedding, TF-IDF simple, far less compute intensive can surprisingly effective. dimensionality reduction methods don’t work well sparse data, need input data dense. uwot different, one Python UMAP’s unique features can work sparse data, due use PyNNDescent nearest neighbor search. Python UMAP sparse data tutorial uses 20 Newsgroups dataset long wanted replicate R. main barriers lack straight-forward way download data lack fast approximate nearest neighbor search package can work sparse data. first problem solved snedata package, recently added download_twenty_newsgroups function. took large chunk one afternoon evening. second problem solved replicating PyNNDescent package R. took bit longer, mere 4 years later, rnndescent package finally usable state. let’s get started. Apart Python example , good resources text analysis dataset can found chapter 9 text mining R juicily-titled Python notebook 20 Newsgroups Secrets.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"downloading-the-20-newsgroups-dataset","dir":"Articles","previous_headings":"","what":"Downloading the 20 Newsgroups Dataset","title":"Working with Sparse Data","text":"20 Newsgroups dataset (nearly) 20,000 usenet posts sampled (nearly) equally 20 different newsgroups. webpage linked contains information data’s provenance structure. can use snedata::download_twenty_newsgroups download : subset argument can used download training test set data use . verbose argument useful see ’s : take minutes download . ’s tar.gz file unfortunately, unaware way stream tar data directly R, download_twenty_newsgroups download file temporary directory, extract everything, attempt clean afterwards. something goes wrong log directory downloaded . data returned data frame 6 columns: Id column unique identifier post, FileId filename combined Subset, Text post text, Subset either \"train\" \"test\", Label integer label newsgroup Newsgroup name newsgroup. columns interested Text Newsgroup, data manipulation need , Id can keep track rows. ’s first row without Text column (can safely ignore row id): ’s first characters text: lot came , invite investigate (fill terminal ) . Notably, snedata doesn’t processing posts, headers still , footers quotations previous posts current poster might replying .","code":"devtools::install_github(\"jlmelville/snedata\")  ng20 <- snedata::download_twenty_newsgroups(subset = \"all\", verbose = TRUE) dim(ng20) [1] 18846     6 names(ng20) [1] \"Id\"        \"FileId\"    \"Text\"      \"Subset\"    \"Label\"     \"Newsgroup\" ng20[1, -3] Id FileId Subset Label   Newsgroup alt.atheism.1 train_1  49960  train     0 alt.atheism substr(ng20[1, ]$Text, 1, 72) [1] \"From: mathew <mathew@mantis.co.uk>\\nSubject: Alt.Atheism FAQ: Atheist Res\""},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"preprocessing-the-data","dir":"Articles","previous_headings":"","what":"Preprocessing the Data","title":"Working with Sparse Data","text":"Unless get luck pre-cleaned datasets, text data usually requires fair amount processing turn vector numbers UMAP can work , need text wrangling start . ’ll need install dplyr, tidyr stringr, : Text Mining R good place start get ideas , bear mind dataset structured bit differently, let’s temporarily expand dataset splitting text new lines like tidy text mining example row line original post: Now first row looks like: first filtering remove headers (anything first blank line) footers (assumed anything first line repeated hyphens): attempts removed quoted text: functions bit different ones used python side scikit-learn – see functions strip_newsgroup_header, strip_newsgroup_quoting strip_newsgroup_footer, R functions good enough tidy text modelers, ’s good enough . Also, tidy text book, explicitly remove two items “contained large amount non-text content”. Specifically images, can find “20 Newsgroups Secrets” notebook linked (don’t get excited). seems binary data posts, try catch , end filtering later. tidy text example goes examine word frequencies newsgroup whole, go back analysing per post basis. Now, split posts lines, need unsplit , also re-associate columns original ng20 data: Back original structure. processing isn’t perfect, decent job. Now can actual text processing.","code":"install.packages(c(\"dplyr\", \"tidyr\", \"stringr\")) library(dplyr) library(tidyr) library(stringr) ng20spl <- ng20 |> separate_longer_delim(`Text`, delim = \"\\n\") dim(ng20spl) [1] 834780      6 ng20spl[1, ] Id FileId                               Text Subset Label   Newsgroup 1 train_1  49960 From: mathew <mathew@mantis.co.uk>  train     0 alt.atheism cleaned_text <- ng20spl |>   group_by(`Newsgroup`, `Id`) |>   filter(     cumsum(`Text` == \"\") > 0,     cumsum(str_detect(`Text`, \"^--\")) == 0   ) |>   ungroup() dim(cleaned_text) [1] 599249      6 cleaned_text <- cleaned_text |>   filter(     str_detect(`Text`, \"^[^>]+[A-Za-z\\\\d]\") |       `Text` == \"\",     !str_detect(`Text`, \"writes(:|\\\\.\\\\.\\\\.)$\"),     !str_detect(`Text`, \"^In article <\")   ) dim(cleaned_text) [1] 445080      6 text_unsplit <-   cleaned_text |>   group_by(`Id`) |>   summarise(`Text` = paste(`Text`, collapse = \" \")) text_unsplit <- ng20 |>   select(-`Text`) |>   left_join(text_unsplit, by = \"Id\") text_unsplit <-   text_unsplit[, c(\"Id\", \"FileId\", \"Text\", \"Subset\", \"Label\", \"Newsgroup\")] dim(text_unsplit) [1] 18846     6"},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"text-mining","dir":"Articles","previous_headings":"","what":"Text Mining","title":"Working with Sparse Data","text":"’m going use tm package text mining: going create “corpus” text apply various processing steps normalize text, handling whitespace, case, removing punctuation . create Corpus initially, also convert text encoding latin1 UTF-8 via iconv function. far can tell, 20 Newsgroups data latin1 encoding (sklearn also reads encoding), convert UTF-8 rest processing. still documents odd formatting characters tm seems deal without issue.","code":"install.packages(\"tm\") library(tm) corpus <-   Corpus(VectorSource(iconv(text_unsplit$Text, \"latin1\", \"UTF-8\"))) |>   tm_map(content_transformer(tolower)) |>   tm_map(removePunctuation) |>   tm_map(removeNumbers) |>   tm_map(removeWords, stopwords(\"english\")) |>   tm_map(stripWhitespace) length(corpus) [1] 18846"},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"filtering-out-bad-documents","dir":"Articles","previous_headings":"Text Mining","what":"Filtering Out Bad Documents","title":"Working with Sparse Data","text":"point, take bit inspiration 20 Newsgroups Secrets start looking distribution document lengths terms number words number characters. corpus really mainly English text expect see kind rough relationship sorts values deviations likely anomalies need investigating. bit diversion business hand, might well see can come reasonable filter values using simple methods. Clearly quite skew terms number words. messages now zero words, one nearly 7000. median less 50 words. Let’s look distribution number words, stop 95% data avoid outliers: 95% data contains 250 words less. massive document. Let’s (carefully) take look: Ah, ’s FAQ. Ok makes sense. Now let’s look distribution raw length documents terms number characters: Seems reminiscent word count distribution. let’s see related (’d assume related never know): Ok ’s good relation, makes documents don’t appear main trendline suspicious. easy way deal define average word length: NA’s documents zero words. looks like another rather skewed distribution: 95% data average word length 8 less. gives us bit feel might reasonable filter settings get rid unsuitable data. filters established 20 Newsgroups Secrets document length based word seem reasonable : less 10 words suspicious 2000 words also suspicious. avoid making even longer needs , won’t investigating suspicious documents article. Ok, think ’ve done enough move .","code":"count_words <- function(doc) {   doc |>     strsplit(\" \") |>     unlist() |>     length() }  nwords <- sapply(corpus, count_words) summary(nwords) Min. 1st Qu.  Median    Mean 3rd Qu.    Max.     0.00   24.00   46.00   84.42   86.00 6592.00 hist(nwords[nwords <= quantile(nwords, 0.95)], main = \"0-95% word count distribution\") substr(ng20[which.max(nwords), ]$Text, 0, 80) [1] \"From: jeh@cmkrnl.com\\nSubject: Electrical wiring FAQ (was: A question about 120VA\" nchars <- sapply(corpus, function(doc) {   doc |> str_length() }) summary(nchars) Min. 1st Qu.  Median    Mean 3rd Qu.    Max.      0.0   158.0   310.0   594.6   593.0 67441.0 plot(nwords, nchars, main = \"Number of words vs number of characters\") avg_word_lengths <- nchars / nwords summary(avg_word_lengths) Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's    1.000   6.294   6.750   6.720   7.213  27.533      77 hist(avg_word_lengths[avg_word_lengths <= quantile(avg_word_lengths, 0.95, na.rm = TRUE)],   main = \"0-95% average word length distribution\" ) is_suspiciously_short <- function(doc) {   doc |> count_words() <= 9 } suspiciously_short_indices <- sapply(corpus, is_suspiciously_short) corpus <- corpus[!suspiciously_short_indices] text_unsplit <- text_unsplit[!suspiciously_short_indices, ] length(corpus) [1] 17708 is_suspiciously_long <- function(doc) {   doc |> count_words() >= 2000 } suspiciously_long_indices <- sapply(corpus, is_suspiciously_long) corpus <- corpus[!suspiciously_long_indices] text_unsplit <- text_unsplit[!suspiciously_long_indices, ] length(corpus) [1] 17675 avg_word_len <- function(doc) {   (doc |> str_length()) / (doc |> count_words()) } has_suspiciously_short_words <- function(doc) {   doc |> avg_word_len() < 4 }  suspiciously_short_word_indices <- sapply(corpus, has_suspiciously_short_words) corpus <- corpus[!suspiciously_short_word_indices] text_unsplit <- text_unsplit[!suspiciously_short_word_indices, ] length(corpus) [1] 17670 has_suspiciously_long_words <- function(doc) {   doc |> avg_word_len() > 15 } suspiciously_long_word_indices <- sapply(corpus, has_suspiciously_long_words) corpus <- corpus[!suspiciously_long_word_indices] text_unsplit <- text_unsplit[!suspiciously_long_word_indices, ] length(corpus) [1] 17666"},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"tf-idf","dir":"Articles","previous_headings":"","what":"TF-IDF","title":"Working with Sparse Data","text":"next step convert corpus matrix TF-IDF values: Nearly 90,000 dimensions, one weighted word frequency. sparse : less 0.1% matrix non-zero. One way proceed point use SVD can work sparse matrices, irlba, turn dense representation far fewer dimensions. However slow takes lot memory spend time working many dimensions use, slows things even don’t get right first time. let’s use rnndescent find nearest neighbors sparse data directly.","code":"tfidf <- weightTfIdf(DocumentTermMatrix(corpus)) dim(tfidf) [1] 17666 88894 Matrix::nnzero(tfidf) / prod(dim(tfidf)) 0.0006920568"},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"finding-nearest-neighbors","dir":"Articles","previous_headings":"","what":"Finding Nearest Neighbors","title":"Working with Sparse Data","text":"need convert sparse matrix format Matrix package rnndescent can handle: , following example Python UMAP tutorial, want use Hellinger distance. show-cases another example rnndescent, implements lot distances methods. Hellinger distance, divergence designed work probability distributions, going L1-normalize row sparse matrix: Now can find neighbors. use brute-force search rnndescent dataset small enough us handle exact search comfortably, least enough cores: took minute laptop.","code":"library(Matrix) tfidf_sp <-   sparseMatrix(     i = tfidf$i,     j = tfidf$j,     x = tfidf$v,     dims = dim(tfidf)   ) l1_normalize <- function(X) {   res <- rep(0, nrow(X))   dgt <- as(X, \"TsparseMatrix\")   tmp <- tapply(dgt@x, dgt@i, function(x) {     sum(abs(x))   })   res[as.integer(names(tmp)) + 1] <- tmp   Diagonal(x = 1 / res) %*% X } tfidf_spl1 <- l1_normalize(tfidf_sp) install.packages(\"rnndescent\") library(rnndescent) tfidfl1_hell_bf <-   brute_force_knn(     tfidf_spl1,     k = 15,     metric = \"hellinger\",     n_threads = 6,     verbose = TRUE   )"},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"umap","dir":"Articles","previous_headings":"","what":"UMAP","title":"Working with Sparse Data","text":"last can run UMAP. nearest neighbors data already, ’s input data need. settings ensure slightly longer optimization usual (save bit time already done nearest neighbor search), use LEOPOLD model original density differences data. follow along code , get results plot , due stochastic nature UMAP. see something similar (don’t, please file issue).","code":"library(uwot) ng20_umap <-   umap(     X = NULL,     nn_method = tfidfl1_hell_bf,     n_sgd_threads = 6,     n_epochs = 1000,     batch = TRUE,     init_sdev = \"range\",     dens_scale = 0.5,     verbose = TRUE   )"},{"path":"https://jlmelville.github.io/uwot/articles/sparse-data-example.html","id":"plot-the-results","dir":"Articles","previous_headings":"","what":"Plot the Results","title":"Working with Sparse Data","text":"going bit challenging plot, 20 unique colors 20 newsgroups. palettes don’t offer anything size, going use Polychrome package generate palette 20 colors hopefully reasonably distinguishable. similar approach Python package glasbey. also rotate coordinates align along principal axes: Ok, now make plot ggplot2: Although can’t compare result easily Python example, one plots training test set separately, isn’t legend, think done pretty good job separating least 20 newsgroups.","code":"library(Polychrome)  palette <- as.vector(Polychrome::createPalette(   length(levels(text_unsplit$Newsgroup)) + 2,   seedcolors = c(\"#ffffff\", \"#000000\"),   range = c(10, 90) )[-(1:2)]) ng20_umap_rotated <- prcomp(ng20_umap)$x library(ggplot2)  ggplot(   data.frame(ng20_umap_rotated, Newsgroup = text_unsplit$Newsgroup),   aes(x = X1, y = X2, color = Newsgroup) ) +   geom_point(alpha = 0.4, size = 0.5) +   scale_color_manual(values = palette) +   theme_minimal() +   labs(     title = \"Sparse Hellinger 20Newsgroups UMAP\",     x = \"\",     y = \"\",     color = \"Newsgroup\"   ) +   theme(legend.position = \"right\") +   guides(color = guide_legend(override.aes = list(size = 5, alpha = 1)))"},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"data-preparation","dir":"Articles","previous_headings":"","what":"Data preparation","title":"UMAP Examples","text":"details datasets, follow links. Somewhat detail also given smallvis documentation. iris already using R. s1k part sneer package. frey, oli, mnist, fashion, kuzushiji, norb cifar10 can downloaded via snedata. coil20 coil100 can fetched via coil20. time generated document (late December 2018), kuzushiji dataset duplicate -black images needed filtering. seems remedied early February 2019. re-ran UMAP t-SNE fixed dataset, results weren’t noticeably different. record, clean-routines ran :","code":"mnist <- snedata::download_mnist()  # For some functions we need to strip out non-numeric columns and convert data to matrix x2m <- function(X) {   if (!methods::is(X, \"matrix\")) {     m <- as.matrix(X[, which(vapply(X, is.numeric, logical(1)))])   }   else {     m <- X   }   m } # Remove all-black images in Kuzushiji MNIST (https://github.com/rois-codh/kmnist/issues/1) kuzushiji <- kuzushiji[-which(apply(x2m(kuzushiji), 1, sum) == 0), ] # Remove duplicate images (https://github.com/rois-codh/kmnist/issues/5) kuzushiji <- kuzushiji[-which(duplicated(x2m(kuzushiji))), ]"},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"umap-settings","dir":"Articles","previous_headings":"","what":"UMAP settings","title":"UMAP Examples","text":"UMAP, stick defaults, exception iris, coil20, coil100 norb. spectral initialization default n_neighbors leads disconnected components, can lead poor global picture data. Python UMAP implementation goes fairly involved lengths ameliorate theses issues, uwot . datasets, perfectly good alternative provides global initialization use first two components PCA, scaled standard deviations initially 1e-4 (via init = \"spca\"). usually results embedding isn’t different starting via raw PCA compact, .e. less space clusters. visualizations, long relative orientation rough distances clusters maintained, exact distances interesting . Dimensionality reduced 100 applying PCA data. ’s commonly applied data part t-SNE, avoid differences pre-processing, applied PCA UMAP data. used 100 components, rather usual 50 just safe side. seemed major effect resulting visualizations.","code":"# For iris  iris_umap <- umap(iris, init = \"spca\")  # Small datasets (s1k, oli, frey) s1k_umap <- umap(s1k)  # Big datasets (mnist, fashion, kuzushiji) mnist_umap <- umap(mnist, pca = 100)  # norb, coil20 and coil100, tasic2018 coil20_umap <- umap(coil20, pca = 100, init = \"spca\")"},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"t-sne-settings","dir":"Articles","previous_headings":"","what":"t-SNE settings","title":"UMAP Examples","text":"Rtsne package used t-SNE calculations, except iris dataset, proving troublesome . time ’s Rtsne doesn’t allow duplicates. iris , used smallvis package. t-SNE, also employ following non-defaults: perplexity = 15, closer neighborhood size used UMAP mentioned UMAP settings section, large datasets, initial_dims = 100, applies PCA input, keeping first 100 components, rather usual 50 minmize distortion initial PCA. uwot also makes use irlba, reason use partial_pca. nice use initial coordinates methods, unfortunately Rtsne doesn’t apply early exaggeration user-supplied input. Without early exaggeration, t-SNE results aren’t good, especially larger datasets. Therefore t-SNE plots use random initialization.","code":"# For iris only iris_tsne <- smallvis::smallvis(iris, perplexity = 15, Y_init = \"rand\", exaggeration_factor = 4)  # Small datasets (s1k, oli, frey) s1k_tsne <- Rtsne::Rtsne(x2m(s1k), perplexity = 15, initial_dims = 100,                        partial_pca = TRUE, exaggeration_factor = 4)  # Big datasets (coil20, coil100, mnist, fashion, kuzushiji etc.) mnist_tsne <- Rtsne::Rtsne(x2m(mnist), perplexity = 15, initial_dims = 100,                        partial_pca = TRUE, exaggeration_factor = 12)"},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"visualization","dir":"Articles","previous_headings":"t-SNE settings","what":"Visualization","title":"UMAP Examples","text":"visualization, used vizier package. plots colored class membership (’s obvious choice every dataset considered), except frey, points colored according position sequence images. UMAP, non-default initialization used, ’s noted title plot (e.g. “(spca)”).","code":"embed_img <- function(X, Y, k = 15, ...) {   args <- list(...)   args$coords <- Y   args$x <- X    do.call(vizier::embed_plot, args) } embed_img(iris, iris_umap, pc_axes = TRUE, equal_axes = TRUE, alpha_scale = 0.5, title = \"iris UMAP\", cex = 1)"},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"iris","dir":"Articles","previous_headings":"","what":"iris","title":"UMAP Examples","text":"standard iris dataset, known loved .","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"s1k","dir":"Articles","previous_headings":"","what":"s1k","title":"UMAP Examples","text":"9-dimensional fuzzy simplex, created testing t-SNE related methods, original sneer package.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"oli","dir":"Articles","previous_headings":"","what":"oli","title":"UMAP Examples","text":"ORL database faces.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"frey","dir":"Articles","previous_headings":"","what":"frey","title":"UMAP Examples","text":"Images Brendan Frey’s face, far know originating page belonging Saul Roweis.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"isofaces","dir":"Articles","previous_headings":"","what":"isofaces","title":"UMAP Examples","text":"Yet faces, time dataset used Isomap, consisting images face different rotations lighting conditions. Unfortunately, ’s longer available MIT website, can found via Wayback Machine. wrote gist processing data R. images points colored first pose angle.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"coil20","dir":"Articles","previous_headings":"","what":"coil20","title":"UMAP Examples","text":"COIL-20 Columbia Object Image Library.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"coil100","dir":"Articles","previous_headings":"","what":"coil100","title":"UMAP Examples","text":"COIL-100 Columbia Object Image Library. UMAP results rather hard visualize static plot. pan zoom around, see rather indistinct blobs mainly correctly preserved loops. example choosing different value b parameters good idea. t-UMAP variant, available tumap uses = 1, b = 1 (effectively using Cauchy kernel t-SNE) better job shown left. correct choice parameters also important t-SNE. right t-SNE result default perplexity = 50, retain much loop structure lower perplexity:","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"swiss-roll","dir":"Articles","previous_headings":"","what":"swiss roll","title":"UMAP Examples","text":"Swiss Roll data used Isomap. famous dataset, perhaps representative typical real world datasets. t-SNE know handle well, UMAP makes impressive go unfolding .","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"mnist","dir":"Articles","previous_headings":"","what":"mnist","title":"UMAP Examples","text":"MNIST database handwritten digits.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"fashion","dir":"Articles","previous_headings":"","what":"fashion","title":"UMAP Examples","text":"Fashion MNIST database, images fashion objects.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"kuzushiji-kmnist","dir":"Articles","previous_headings":"","what":"kuzushiji (KMNIST)","title":"UMAP Examples","text":"Kuzushiji MNIST database, images cursive Japanese handwriting.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"norb","dir":"Articles","previous_headings":"","what":"norb","title":"UMAP Examples","text":"small NORB dataset, pairs images 50 toys photographed different angles different lighting conditions.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"cifar10","dir":"Articles","previous_headings":"","what":"cifar10","title":"UMAP Examples","text":"CIFAR-10 dataset, consisting 60000 32 x 32 color images evenly divided across 10 classes (e.g. airplane, cat, truck, bird). t-SNE applied CIFAR-10 Barnes-Hut t-SNE paper, main paper, results passing convolutional neural network published. t-SNE original pixel data given supplementary information (PDF) oddly hard find link via JMLR article , less widely-cited preliminary investigation BH t-SNE. outlying orange cluster (isn’t easy see) top right UMAP plot. see thing Python implementation, don’t think bug uwot (although also said previous version page, turned bug uwot. current result really closer Python version now, though). cluster images automobiles seem variations image. cluster present t-SNE plot (bottom left), comfortably close rest data. existence near-duplicates CIFAR-10 doesn’t seem widely known appreciated quite recently, see instance twitter thread paper Recht co-workers. manipulations line practice data augmentation popular deep learning, need aware avoid test set results contaminated. images seem like good argument applying UMAP t-SNE dataset way spot sort thing. can get better results UMAP using scaled PCA initialization (left) using t-UMAP settings (, right): rogue cluster still present (lower right now), either image comparable t-SNE result. visualization CIFAR-10 isn’t successful UMAP t-SNE, results using activations convnet, similar used BH t-SNE paper. convnet, used keras implementation, taken Machine Learning Action blog. Features Z-scaled carried blog, used 100 epochs batch size 128 also used RMSprop (PDF) optimizer favored Deep Learning Python book (lr=1e-4 decay=1e-6). Without data augmentation, gave test set error 0.1655, slightly lower test set result given BH t-SNE paper (used different architecture without benefit extra 4 years deep learning research). retraining 60000 images, flattened output final max-pool layer used, giving 2048 features (BH t-SNE paper network 1024 output activations). UMAP t-SNE results . UMAP results, used t-UMAP settings scaled PCA initialization. Results don’t look quite good BH t-SNE paper, still improvement. orange cluster automobiles remains outlier, even activation space. can also see BH t-SNE paper lower image Figure 5 (orange cluster bottom, slightly left center).","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"tasic2018","dir":"Articles","previous_headings":"","what":"tasic2018","title":"UMAP Examples","text":"tasic2018 dataset transcriptomics dataset mouse brain cell RNA-seq data Allen Brain Atlas (originally reported Tasic co-workers. gene expression data 14,249 cells primary visual cortex, 9,573 cells anterior lateral motor cortex give dataset size n = 23,822 overall. Expression data 45,768 genes obtained original data, dataset used follows pre-processing treatment Kobak Berens applied normalization log transformation kept top 3000 variable genes. data can generated Allen Brain Atlas website processed Python following instructions Berens lab notebook. output data CSV format reading R assembling data frame following extra exporting code: , default UMAP settings produce clusters bit well-separated clearly see images. results using t-UMAP (setting = 1, b = 1), left, using = 2, b = 2 right:","code":"np.savetxt(\"path/to/allen-visp-alm/tasic2018-log3k.csv\", logCPM, delimiter=\",\") np.savetxt(\"path/to/allen-visp-alm/tasic2018-areas.csv\", tasic2018.areas, delimiter=\",\", fmt = \"%d\") np.savetxt(\"path/to/allen-visp-alm/tasic2018-genes.csv\", tasic2018.genes[selectedGenes], delimiter=\",\", fmt='%s') np.savetxt(\"path/to/allen-visp-alm/tasic2018-clusters.csv\", tasic2018.clusters, delimiter=\",\", fmt='%d') np.savetxt(\"path/to/allen-visp-alm/tasic2018-cluster-names.csv\", tasic2018.clusterNames, delimiter=\",\", fmt='%s') np.savetxt(\"path/to/allen-visp-alm/tasic2018-cluster-colors.csv\", tasic2018.clusterColors, delimiter=\",\", fmt='%s')"},{"path":"https://jlmelville.github.io/uwot/articles/umap-examples.html","id":"macosko2015","dir":"Articles","previous_headings":"","what":"macosko2015","title":"UMAP Examples","text":"Another transcriptomics data set, used example openTSNE. contains data 44,808 cells mouse retina. raw data fetched similarly shell script Hemberg Lab data prepared using openTSNE notebook Pavlin Policar. Similarly tasic2018 dataset, data log normalized 3,000 variable genes retained. exported data (without Z-scaling PCA dimensionality reduction), CSV file, e.g.: another result UMAP defaults might need bit fiddling don’t like separated clusters . openTSNE results use Z-scaling inputs applying t-SNE, results UMAP t-SNE Z-scaling applied (umap, pass scale = \"Z\" argument don’t manually):","code":"np.savetxt(\"/path/to/macosko2015/macosko2015-log3k.csv\", x, delimiter=\",\") # Use these as column names np.savetxt(\"/path/to/macosko2015/macosko2015-genenames.csv\", data.T.columns.values[gene_mask].astype(str), delimiter=\",\", fmt = \"%s\") np.savetxt(\"/path/to/macosko2015/macosko2015-clusterids.csv\", cluster_ids.values.astype(int), delimiter=\",\", fmt = \"%d\")"},{"path":"https://jlmelville.github.io/uwot/articles/umap-for-tsne.html","id":"asymmetric-sne","dir":"Articles","previous_headings":"","what":"(Asymmetric) SNE","title":"UMAP for t-SNE","text":"Given \\(N\\) observations high dimensional data, pair, \\(\\mathbf{x_{}}\\) \\(\\mathbf{x_{j}}\\), SNE defines similarity (aka affinity weight) , \\(v_{j|}\\), using Gaussian kernel function: \\[ v_{j|} = \\exp(-\\beta_{} r_{ij}^2) \\] \\(r_{ij}\\) distance \\(\\mathbf{x_{}}\\) \\(\\mathbf{x_{j}}\\) \\(\\beta_{}\\) must determined method (’ll get back ). notation \\(v_{j|}\\) rather \\(v_{ij}\\), indicate quantity symmetric, .e. \\(v_{j|} \\neq v_{|j}\\). ’ve borrowed notation conditional versus joint probability definitions used symmetric SNE (see ) ’ll also need quantities probabilities. \\(r_{ij}\\) notation indicates distances symmetric convention used symmetric values. weights normalized form \\(N\\) probability distributions: \\[ p_{j|} = \\frac{v_{j|}}{\\sum_{k}^{N} v_{k|}} \\] \\(\\beta_{}\\) chosen finding value results probability distribution specific perplexity. perplexity chosen user, interpreted continuous version number nearest neighbors, generally chosen take values 5 50. \\(p_{j|}\\) conditional probability, interpreted meaning “probability pick item \\(j\\) similar item \\(\\), given ’ve already picked \\(\\)”. output space embedded coordinates, similarity points \\(\\mathbf{y_i}\\) \\(\\mathbf{y_j}\\) also defined Gaussian: \\[ w_{ij} = \\exp(-d_{ij}^2) \\] \\(d_{ij}\\) Euclidean distance \\(\\mathbf{y_i}\\) \\(\\mathbf{y_j}\\). \\(\\beta\\) weight definition weights symmetric. output probabilities, \\(q_{j|}\\) calculated \\(w_{ij}\\) way go \\(v_{j|}\\) \\(p_{j|}\\), creating \\(N\\) probability distributions. Due normalizing rows, \\(q_{j|}\\) asymmetric despite symmetric weights generated . SNE cost function sum Kullback-Leibler divergences \\(N\\) distributions: \\[ C_{SNE} = \\sum_{}^{N} \\sum_{j}^{N} p_{j|} \\log \\frac{p_{j|}}{q_{j|}} \\] (follows), weights probabilities \\(= j\\) defined. don’t want clutter notation , assume excluded sums.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-for-tsne.html","id":"symmetric-sne","dir":"Articles","previous_headings":"","what":"Symmetric SNE","title":"UMAP for t-SNE","text":"Symmetric SNE, input probability matrix symmetrized averaging \\(p_{j|}\\) \\(p_{|j}\\) re-normalized pairs points, create single (joint) probability distribution, \\(p_{ij}\\): \\[ p_{ij} = \\frac{p_{j|} + p_{|j}}{2N} \\] output probabilities, \\(q_{ij}\\) now defined normalizing output weights pairs, creating single probability distribution: \\[ q_{ij} = \\frac{w_{ij}}{\\sum_{k}^N \\sum_{l}^N w_{kl}} \\] cost function SSNE : \\[ C_{SSNE} = \\sum_{}^{N} \\sum_{j}^{N} p_{ij} \\log \\frac{p_{ij}}{q_{ij}} \\]","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-for-tsne.html","id":"t-sne","dir":"Articles","previous_headings":"","what":"t-SNE","title":"UMAP for t-SNE","text":"purposes discussion, t-SNE differs symmetric SNE weight function: \\[ w_{ij} = \\frac{1}{1 + d_{ij}^2} \\]","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-for-tsne.html","id":"sne-optimization","dir":"Articles","previous_headings":"","what":"SNE Optimization","title":"UMAP for t-SNE","text":"Optimization t-SNE proceeds : Calibrate input probabilities, \\(p_{ij}\\) according desired perplexity (needs done ). Iteratively: Calculate pairwise distances, \\(d_{ij}\\) \\(\\mathbf{y_{}}\\) \\(\\mathbf{y_{j}}\\) Calculate weights, \\(w_{ij}\\) Calculate output probabilities \\(q_{ij}\\) Use gradient descent update \\(\\mathbf{y_{}}\\) \\(\\mathbf{y_{j}}\\) fundamentally \\(O(N^2)\\) need calculate pairwise distances: t-SNE gradient requires \\(q_{ij}\\) calculated normalization step converts \\(w_{ij}\\) \\(q_{ij}\\) requires \\(w_{ij}\\) calculated also need distances. Approaches like Barnes-Hut t-SNE others (e.g. Flt-SNE) attempt improve taking advantage t-SNE gradient: attractive part gradient depends \\(p_{ij}\\), constant large neighbors close input space. Therefore ’s necessary calculate attractive gradient nearest neighbors \\(\\mathbf{x_i}\\). Barnes-Hut t-SNE, number nearest neighbors used three times whatever perplexity . larger datasets, perplexity 50 common, usually looking 150-nearest neighbors point. repulsive part gradient dependent \\(q_{ij}\\) changes iteration, improvements focus grouping together points distant output space treating single point purposes gradient calculation. able tell perusing publications linked , approaches increasing sophistication complexity.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-for-tsne.html","id":"largevis","dir":"Articles","previous_headings":"","what":"LargeVis","title":"UMAP for t-SNE","text":"LargeVis takes different approach: re-uses lot definitions t-SNE, makes sufficient modifications ’s possible use stochastic gradient descent. Also, rather talk probabilities, LargeVis uses language graph theory. observation dataset now considered vertex node similarity weight edge two vertices. Conceptually ’re still talking elements matrix, start slipping language “edges” “vertices”. key change cost function, now maximum likelihood function: \\[ L_{LV} = \\sum_{ \\left(, j\\right) \\E} p_{ij} \\log w_{ij} +\\gamma \\sum_{\\left(, j\\right) \\\\bar{E}} \\log \\left(1 - w_{ij} \\right) \\] \\(p_{ij}\\) \\(w_{ij}\\) t-SNE (authors try alternative \\(w_{ij}\\) definitions, aren’t effective). new concepts \\(\\gamma\\) \\(E\\). \\(\\gamma\\) user-defined positive scalar weight repulsive versus attractive forces. default reference implementation 7. \\(E\\) set edges non-zero weight. graph theory way talk nearest neighbors input space. Just Barnes-Hut t-SNE, find set nearest neighbors point \\(\\mathbf{x_i}\\) define input weights probabilities pairs points nearest neighbors. official Barnes-Hut t-SNE implementation, LargeVis reference implementation uses default perplexity 50, default number nearest neighbors 3 times perplexity. cost function therefore consists two disjoint contributions: nearest neighbors input space contribute attractive part cost function (first part). Everything else contributes second, repulsive part. key advantage cost function KL divergence doesn’t contain \\(q_{ij}\\). output normalization, don’t need calculate output pairwise distances. cost function amenable stochastic gradient descent techniques.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-for-tsne.html","id":"the-largevis-sampling-strategy","dir":"Articles","previous_headings":"","what":"The LargeVis sampling strategy","title":"UMAP for t-SNE","text":"calculate stochastic gradient, LargeVis following: Samples edge \\(E\\), .e. chooses \\(\\) \\(j\\) \\(p_{ij} \\neq 0\\). called “positive edge” LargeVis paper. \\(\\) \\(j\\) used calculate attractive part gradient. Samples one \\(N\\) vertices, let’s call \\(k\\). datasets grow larger, probability \\(k\\) \\(E\\) grows smaller, ’s actually checked (\\(k \\neq \\)). used calculate repulsive part gradient. called “negative samples” LargeVis paper. Repeat negative sample step number times. default LargeVis sample 5 negatives positive edge. coordinates \\(\\), \\(j\\) various \\(k\\) updated according gradients. concludes one iteration SGD. attractive repulsive gradients LargeVis respectively: \\[ \\frac{\\partial L_{LV}}{\\partial \\mathbf{y_i}}^+ = \\frac{-2}{1 + d_{ij}^2}p_{ij} \\left(\\mathbf{y_i - y_j}\\right) \\\\ \\frac{\\partial L_{LV}}{\\partial \\mathbf{y_i}}^- = \\frac{2\\gamma}{\\left(0.1 + d_{ij}^2\\right)\\left(1 + d_{ij}^2\\right)} \\left(\\mathbf{y_i - y_j}\\right) \\] value 0.1 appears repulsive gradient prevent division zero. Sampling edges vertices uniform. attractive gradient, authors note factor \\(p_{ij}\\) appears means magnitude gradient can differ hugely samples extent choosing appropriate learning rate can difficult. Instead sample edges proportionally \\(p_{ij}\\) gradient calculation, treat edge weights equal. attractive gradient used part LargeVis SGD therefore: \\[ \\frac{\\partial L_{LV}}{\\partial \\mathbf{y_i}}^+ = \\frac{-2}{1 + d_{ij}^2} \\left(\\mathbf{y_i - y_j}\\right) \\] \\(p_{ij}\\) doesn’t appear repulsive part gradient, seem uniform sampling work negative sampling. However, vertices sampled using “noisy” distribution proportional degree ^ 0.75, degree vertex sum weights edges incident . doesn’t seem theoretical reason use degree ^ 0.75. ’s based results field word embeddings: LargeVis authors reference skip-gram paper, power also shows GloVE. cases justified purely empirically. uwot version LargeVis (lvish) samples negative edges uniformly, doesn’t seem cause problems.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-for-tsne.html","id":"umap-at-last","dir":"Articles","previous_headings":"","what":"UMAP (at last)","title":"UMAP for t-SNE","text":"UMAP cost function cross-entropy two fuzzy sets, can represented symmetric weight matrices: \\[ C_{UMAP} = \\sum_{ij} \\left[ v_{ij} \\log \\left( \\frac{v_{ij}}{w_{ij}} \\right) + (1 - v_{ij}) \\log \\left( \\frac{1 - v_{ij}}{1 - w_{ij}} \\right) \\right] \\] \\(v_{ij}\\) symmetrized input affinities, probabilities. graph interpretation weights edges graph still applies, though. arrived differently t-SNE LargeVis. unsymmetrized UMAP input weights given : \\[ v_{j|} = \\exp \\left[ -\\left( r_{ij} - \\rho_{} \\right) / \\sigma_{} \\right] \\] \\(r_{ij}\\) input distances, \\(\\rho_{}\\) distance nearest neighbor (ignoring zero distances neighbors duplicates) \\(\\sigma_{}\\) analogous \\(\\beta_{}\\) perplexity calibration used SNE. case, \\(\\sigma_{}\\) determined \\(\\sum_{j} v_{j|} = \\log_{2} k\\) \\(k\\) number nearest neighbors. January 1 2020: assume connection local scaling advocated self-tuning spectral clustering, given spectral decomposition affinity graph default initialization method UMAP. weights symmetrized slightly different method SNE: \\[ v_{ij} = \\left(v_{j|} + v_{|j}\\right) - v_{j|}v_{|j} \\] matrix operation: \\[ V_{symm} = V + V^{T} - V \\circ V^{T} \\] \\(T\\) indicates transpose \\(\\circ\\) Hadamard (.e. entry-wise) product. effectively carries fuzzy set union. output weights given : \\[ w_{ij} = 1 / \\left(1 + ad_{ij}^{2b}\\right) \\] \\(\\) \\(b\\) determined non-linear least squares fit based min_dist spread parameters control tightness squashing function. setting \\(= 1\\) \\(b = 1\\) get t-SNE weighting back. current UMAP defaults result = 1.929 b = 0.7915. April 7 2019: Actually, got wrong. UMAP defaults use min_dist = 0.1, spread = 1, results \\(= 1.577\\) \\(b = 0.8951\\). use min_dist = 0.001, spread = 1 get result \\(= 1.929\\) \\(b = 0.7915\\). attractive repulsive UMAP gradient expressions , respectively: \\[ \\frac{\\partial C_{UMAP}}{\\partial \\mathbf{y_i}}^+ = \\frac{-2abd_{ij}^{2\\left(b - 1\\right)}}{1 + ad_{ij}^{2b}}  v_{ij} \\left(\\mathbf{y_i - y_j}\\right) \\\\ \\frac{\\partial C_{UMAP}}{\\partial \\mathbf{y_i}}^- = \\frac{2b}{\\left(0.001 + d_{ij}^2\\right)\\left(1 + ad_{ij}^{2b}\\right)}\\left(1 - v_{ij}\\right)\\left(\\mathbf{y_i - y_j}\\right) \\] (April 13 2020: previous versions document completely messed expression omitting factor 2 repulsive gradient equation missed important \\(\\)s \\(b\\)s. also affected SGD version two equations . Thank Dmitry Kobak spotting .) complex-looking LargeVis gradient, obvious similarities, become clearer set \\(=1\\) \\(b=1\\), get back t-SNE/LargeVis output weight function. 0.001 term denominator repulsive gradient plays role 0.1 LargeVis gradient (preventing division zero). UMAP uses sampling strategy LargeVis, sampling positive edges proportional weight edge (case \\(v_{ij}\\)), value gradient calculated assuming \\(v_{ij} = 1\\) edges. SGD purposes, attractive gradient UMAP : \\[ \\frac{\\partial C_{UMAP}}{\\partial \\mathbf{y_i}}^+ = \\frac{-2abd_{ij}^{2\\left(b - 1\\right)}}{1 + ad_{ij}^{2b}}\\left(\\mathbf{y_i - y_j}\\right) = \\frac{-2abd_{ij}^{2b}}{d_{ij}^2 \\left(1 + ad_{ij}^{2b}\\right)}\\left(\\mathbf{y_i - y_j}\\right) \\] final expression might computationally convenient saves extra power calculation. repulsive part gradient contains \\(1 - v_{ij}\\) term, \\(v_{ij} = 0\\) pairs edges, term effectively disappears, leaving: \\[ \\frac{\\partial C_{UMAP}}{\\partial \\mathbf{y_i}}^- = \\frac{2b}{\\left(0.001 + d_{ij}^2\\right)\\left(1 + ad_{ij}^{2b}\\right)}   \\left(\\mathbf{y_i - y_j}\\right) \\] Unlike LargeVis, negative sampling UMAP uses uniform distribution. ’s worth considering , although LargeVis uses \\(p_{ij}\\) UMAP uses \\(v_{ij}\\) cost functions, difference isn’t important, sampling proportionally \\(p_{ij}\\) exactly sampling proportionally \\(v_{ij}\\). fact, look LargeVis reference implementation (lvish uwot), input affinities symmetrized, divided \\(N\\). Nonetheless, affinities used construct sampling probabilities, presence \\(\\gamma\\) parameter repulsive part gradient means effectively using \\(p_{ij}\\) LargeVis cost function optimized, \\(v_{ij}\\).","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-for-tsne.html","id":"minor-umap-variations","dir":"Articles","previous_headings":"","what":"Minor UMAP Variations","title":"UMAP for t-SNE","text":"extra parameters UMAP make minor changes modified default-values: local_connectivity affects \\(\\rho_{}\\) using distance local_connectivityth non-zero near neighbor (interpolating distances local_connectivity non-integral). set_op_mix_ratio changes form symmetrization fuzzy set union fuzzy set intersection just \\(v_{j|}v_{|j}\\), can also blend two. gamma works exactly like value LargeVis, -weighting repulsive contribution gradient. 2 August 2018: follow parameter longer appears reference UMAP implementation: bandwidth affects \\(v_{j|}\\) multiplying value \\(\\sigma_{}\\): \\[ v_{j|} = \\exp \\left[ -\\left( r_{ij} - \\rho_{} \\right) / \\beta \\sigma_{} \\right] \\] bandwidth represented \\(\\beta\\). value \\(\\sigma_{}\\) determined using calculations \\(v_{j|}\\) without \\(\\beta\\), recalculating \\(v_{j|}\\) using bandwidth. ’m sure useful changed defaults. thanks Dmitry Kobak helpful discussions typo-spotting.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/umap-simple.html","id":"grid","dir":"Articles","previous_headings":"","what":"Grid","title":"UMAP on some simple datasets","text":"2D grid regularly spaced points.  Like t-SNE, UMAP tends expand denser regions data, bigger gap points middle grid.","code":"grid2d <- snedata::grid_data(n = 20)"},{"path":"https://jlmelville.github.io/uwot/articles/umap-simple.html","id":"clusters","dir":"Articles","previous_headings":"","what":"2 Clusters","title":"UMAP on some simple datasets","text":"Two 2D Gaussian clusters equal variance, 50 points . Setting n_neighbors low clearly gives results local.","code":"gauss2d <- snedata::two_clusters_data(n = 50, dim = 2)"},{"path":"https://jlmelville.github.io/uwot/articles/umap-simple.html","id":"cluster-densities","dir":"Articles","previous_headings":"","what":"Cluster Densities","title":"UMAP on some simple datasets","text":"example, one clusters (yellow one) much denser (hence smaller) . like t-SNE, UMAP reproduce relative cluster densities.","code":"gauss2d_scale <- snedata::two_different_clusters_data(n = 75, scale = 10, dim = 2)"},{"path":"https://jlmelville.github.io/uwot/articles/umap-simple.html","id":"cluster-size","dir":"Articles","previous_headings":"","what":"Cluster Size","title":"UMAP on some simple datasets","text":"aside, two clusters density different numbers points? example two clusters equal sizes (100 points ), orange cluster contains 1000 points: example can see UMAP display clusters members larger. can implications visualization minority class interested .","code":"x100a <- snedata::gaussian_data(n = 100, dim = 50, color = \"blue\") x1000b <- snedata::gaussian_data(n = 1000, dim = 50, color = \"orange\") x1000b[, 1:50] <- x1000b[, 1:50] <- x1000b[, 1:50] + 10 x200 <- rbind(x100a, x100b) x1100 <- rbind(x100a, x1000b)"},{"path":"https://jlmelville.github.io/uwot/articles/umap-simple.html","id":"distances-between-clusters","dir":"Articles","previous_headings":"","what":"Distances Between Clusters","title":"UMAP on some simple datasets","text":"example, back gaussians variances, now one (green one) much away two. ’s really value n_neighbors correct relative distances reproduced. hand, least don’t see strange distortion size green cluster high values n_neighbors, t-SNE results start showing distortions high perplexity. repeat larger number points cluster: Results consistent sensible value n_neighbors, ’s clear UMAP reproduce relative distances case.","code":"gauss_3clusters <- snedata::three_clusters_data(n = 50, dim = 2) gauss_3clusters200 <- snedata::three_clusters_data(n = 200, dim = 2)"},{"path":"https://jlmelville.github.io/uwot/articles/umap-simple.html","id":"random-noise","dir":"Articles","previous_headings":"","what":"Random Noise","title":"UMAP on some simple datasets","text":"single high-dimensional Guassian: , see t-SNE-like behavior: density points projection even linear projection provided PCA. ’s also clear low values n_neighbors mislead seeing large numbers small clusters aren’t really .","code":"gauss100d <- snedata::gaussian_data(n = 500, dim = 100, color = \"#003399\")"},{"path":"https://jlmelville.github.io/uwot/articles/umap-simple.html","id":"elongated-shapes","dir":"Articles","previous_headings":"","what":"Elongated Shapes","title":"UMAP on some simple datasets","text":"ellipsoidal cluster: , UMAP behaves pretty well long n_neighbors sufficiently high. Now, two ellipsoidal clusters: density distortion effect also apparent , causing clusters curve.","code":"gauss_long <- snedata::long_gaussian_data(n = 100, dim = 50, color = \"#003399\") gauss_2long <- snedata::long_cluster_data(n = 75)"},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/umap-simple.html","id":"containment","dir":"Articles","previous_headings":"Topology","what":"Containment","title":"UMAP on some simple datasets","text":"dataset, two 50D gaussian clusters, centered location, PCA plot top left row shows, blue cluster much smaller variance “contained” inside yellow cluster. last difference t-SNE results. t-SNE, containment relationship can displayed suitable choice perplexity, cost yellow cluster gaining ring-like shape. UMAP, however, stubbornly refuses show anything sort, blue cluster expanded overlap yellow cluster even higher values n_neighbors.","code":"subset50d <- snedata::subset_clusters_data(n = 75, dim = 50)"},{"path":"https://jlmelville.github.io/uwot/articles/umap-simple.html","id":"linked-rings","dir":"Articles","previous_headings":"Topology","what":"Linked Rings","title":"UMAP on some simple datasets","text":"2D linked rings, embedding 3D (one right angles ). t-SNE results show rings separate low perplexity, linked high perplexities. UMAP results always unlinked except low value n_neighbors, even seems artifact number epochs random seed. set n_epochs higher, rings invariably unlinked.","code":"linked_rings <- snedata::link_data(n = 100)"},{"path":"https://jlmelville.github.io/uwot/articles/umap-simple.html","id":"trefoil-knot","dir":"Articles","previous_headings":"Topology","what":"Trefoil Knot","title":"UMAP on some simple datasets","text":"Results quite similar t-SNE results. low values n_neighbors, knot unfolded circle, higher values, folded form appears. linked rings, results get low values n_neighbors consistent higher values n_epochs. make ? Mainly, UMAP results bit consistent t-SNE, sense changing n_neighbors doesn’t lead different results way changing perplexity t-SNE, although effects mainly restricted three cluster containment example. may see advantage t-SNE. Personally, bit skeptical see sort thing real world datasets. ’s also worth noting , like t-SNE: Inter-cluster distances reproduced. Relative cluster sizes reproduced. Denser regions space expanded less dense parts.","code":"trefoil <- snedata::trefoil_data(n = 150)"},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"basic-umap","dir":"Articles","previous_headings":"","what":"Basic UMAP","title":"uwot","text":"defaults umap function work datasets. scaling input data done, non-numeric columns ignored:","code":"set.seed(42) iris_umap <- umap(iris) plot_umap(iris_umap)"},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"parameters","dir":"Articles","previous_headings":"Basic UMAP","what":"Parameters","title":"uwot","text":"uwot accumulated many parameters time, time handful need worry . important ones :","code":""},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"min_dist","dir":"Articles","previous_headings":"Basic UMAP > Parameters","what":"min_dist","title":"uwot","text":"mainly aesthetic parameter, defines close points can get output space. smaller value tends make clusters output compact. experiment values 0 1, although don’t choose exactly zero. default 0.01, seems like ’s bit small iris. Let’s crank min_dist 0.3:  made clusters bigger closer together, ’ll use min_dist = 0.3 examples iris.","code":"set.seed(42) iris_umap_md05 <- umap(iris, min_dist = 0.3) plot_umap(iris_umap_md05)"},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"n_neighbors","dir":"Articles","previous_headings":"Basic UMAP > Parameters","what":"n_neighbors","title":"uwot","text":"defines number items dataset define neighborhood around point. Set low get fragmented layout. Set high get something miss lot local structure. ’s result 5 neighbors:  ’s hugely different default 15 neighbors, clusters bit broken . pronounced difference going way looking 100 neighbors:  much uniform appearance results. ’s always worth trying different values n_neighbors, especially larger values, although larger values n_neighbors lead longer run times. Sometimes small clusters think meaningful may fact artifacts setting n_neighbors small, starting larger value looking effect reducing n_neighbors can help avoid interpreting results.","code":"set.seed(42) iris_umap_nbrs5 <- umap(iris, n_neighbors = 5, min_dist = 0.3) plot_umap(iris_umap_nbrs5) set.seed(42) iris_umap_nbrs100 <- umap(iris, n_neighbors = 100, min_dist = 0.3) plot_umap(iris_umap_nbrs100)"},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"init","dir":"Articles","previous_headings":"Basic UMAP > Parameters","what":"init","title":"uwot","text":"default initialization UMAP use spectral initialization, acts upon (symmetrized) k-nearest neighbor graph determined choice n_neighbors. usually good choice, involves sparse matrix, can sometimes bit sparse, leads numerical difficulties manifest slow run times even hanging calculations. dataset causes issues, can either try increasing n_neighbors seen cases inconvenient terms CPU RAM usage. alternative use first two principal components data, least uses data provide give solid global picture data UMAP can refine. ’s appropriate every dataset, cases, ’s perfectly good alternative. gotcha depending scaling data, initial coordinates can large inter-point distances. UMAP optimize well, output scaled small standard deviation. set init = \"spca\", , although aligned UMAP coordinate initialization, recommend also set init_sdev = \"range\" well. init_sdev can also take numerical value standard deviation. Values 1e-4 10 reasonable, recommend stick default \"range\".  doesn’t big effect iris, ’s good know option: can also smooth effect changing n_neighbors initial coordinates standard spectral initialization, can make easier see effect changing n_neighbors final result. init options know : \"random\": worst comes worst, can always fall back randomly assigning initial coordinates. really want avoid can though, take longer optimize coordinates quality, need increase n_epochs compensate. Even , ’s much likely end minimum less desirable one based good initialization. make interpreting results harder, likely end different clusters beings split mixed . coordinates like another method, can pass matrix. remember probably want scale init_sdev though.","code":"set.seed(42) iris_umap_spca <-   umap(iris,     init = \"spca\",     init_sdev = \"range\",     min_dist = 0.3   ) plot_umap(iris_umap_spca)"},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"dens_scale","dir":"Articles","previous_headings":"Basic UMAP > Parameters","what":"dens_scale","title":"uwot","text":"dens_scale parameter varies 0 1 controls much relative densities input data attempted preserved output.  shrunk black cluster left plot (species setosa), reflect density setosa points less spread input data two species. dens_scale please read dedicated article.","code":"set.seed(42) iris_umapds <- umap(iris, min_dist = 0.3, dens_scale = 0.5) plot_umap(iris_umapds)"},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"embedding-new-data","dir":"Articles","previous_headings":"","what":"Embedding New Data","title":"uwot","text":"embedding, can use embed new data, although need remember ask “model” return. Instead just coordinates, now get back list contains extra parameters need transforming new data. coordinates still available $embedding component. Let’s try building UMAP just setosa versicolor iris species:  Next, can use umap_transform embed new points:  green points top-right show embedded data. Note original (black red) clusters get optimized . haven’t perfectly reproduced full UMAP, virginica points located less right place, close versicolor items. Just like machine learning method, must careful choose training set.","code":"set.seed(42)  iris_train <- iris[iris$Species %in% c(\"setosa\", \"versicolor\"), ] iris_train_umap <-   umap(iris_train, min_dist = 0.3, ret_model = TRUE) plot(   iris_train_umap$embedding,   col = iris_train$Species,   xlab = \"\",   ylab = \"\",   main = \"UMAP setosa + versicolor\" ) iris_test <- iris[iris$Species == \"virginica\", ] set.seed(42) iris_test_umap <- umap_transform(iris_test, iris_train_umap) plot(   rbind(iris_train_umap$embedding, iris_test_umap),   col = iris$Species,   xlab = \"\",   ylab = \"\",   main = \"UMAP transform virginica\" )"},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"supported-distances","dir":"Articles","previous_headings":"","what":"Supported Distances","title":"uwot","text":"small (N < 4096) Euclidean distance, exact nearest neighbors found using FNN package. Otherwise, approximate nearest neighbors found using RcppAnnoy. supported distance metrics (set metric parameter) : Euclidean Cosine Pearson Correlation (correlation) Manhattan Hamming Exactly constitutes cosine distance can differ packages. uwot tries follow Python version UMAP defines , 1 minus cosine similarity. differs slightly Annoy defines angular distance, aware uwot internally converts Annoy version distance. Also aware Pearson correlation distance cosine distance applied row-centered vectors. need metrics, can generate nearest neighbor info externally, can pass data directly uwot via nn_method parameter. Please note Hamming support lot slower metrics. recommend using hundred features, even expect take several minutes index building phase situations Euclidean metric take seconds.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"multi-threading-support","dir":"Articles","previous_headings":"","what":"Multi-threading support","title":"uwot","text":"Parallelization can used nearest neighbor index search, smooth knn/perplexity calibration, optimization, approach LargeVis takes. can () adjust number threads via n_threads, controls nearest neighbor smooth knn calibration, n_sgd_threads parameter, controls number threads used optimization. n_threads, default number available cores. n_sgd_threads default 0, ensures reproducibility results fixed seed.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"python-comparison","dir":"Articles","previous_headings":"","what":"Python Comparison","title":"uwot","text":"datasets ’ve tried , results look least reminiscent obtained using official Python implementation. results 70,000 MNIST digits (downloaded using snedata package). , result using official Python UMAP implementation (via reticulate package). result using uwot. MNIST UMAP (Python) MNIST UMAP (R) project documentation contains examples, comparison Python.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"nearest-neighbor-calculation","dir":"Articles","previous_headings":"Limitations and Other Issues","what":"Nearest Neighbor Calculation","title":"uwot","text":"uwot leans heavily Annoy library approximate nearest neighbor search. result, compared Python version UMAP, uwot much limited support different distance measurements, support sparse matrix data input. However, uwot let pass nearest neighbor data. access nearest neighbor methods, can generate data can used uwot. See Nearest Neighbor Data Format article. can calculate distance matrix data, can pass dist object. larger distance matrices, can pass sparseMatrix (Matrix package). Experience COIL-100, 49,152 features, suggests Annoy definitely struggle datasets dimensionality. Even 3000 dimensions can cause problems, although difficulty specific Annoy. Reducing dimensionality PCA intermediate dimensionality (e.g. 100) can help. Use e.g. pca = 100 . can also slow platforms without good linear algebra support assure 100 principal components won’t throwing away excessive amounts information.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"spectral-initialization","dir":"Articles","previous_headings":"Limitations and Other Issues","what":"Spectral Initialization","title":"uwot","text":"spectral initialization default umap (Laplacian Eigenmap initialization, init = \"laplacian\") can sometimes run problems. fails converge fall back random initialization, occasion ’ve seen take extremely long time (couple hours) converge. Recent changes hopefully reduced chance happening, initialization taking minutes, suggest stopping calculation using scaled PCA (init = \"spca\") instead.","code":""},{"path":"https://jlmelville.github.io/uwot/articles/uwot.html","id":"supporting-libraries","dir":"Articles","previous_headings":"","what":"Supporting Libraries","title":"uwot","text":"credit following packages lot hard work: Coordinate initialization uses RSpectra eigendecomposition normalized Laplacian. optional PCA initialization initial dimensionality reduction uses irlba. smooth k-nearest neighbor distance stochastic gradient descent optimization routines written C++ (using Rcpp, aping Python code closely possible. multi-threading code based RcppParallel.","code":""},{"path":"https://jlmelville.github.io/uwot/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"James Melville. Author, maintainer, copyright holder. Aaron Lun. Contributor. Mohamed Nadhir Djekidel. Contributor. Yuhan Hao. Contributor. Dirk Eddelbuettel. Contributor.","code":""},{"path":"https://jlmelville.github.io/uwot/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Melville J (2024). uwot: Uniform Manifold Approximation Projection (UMAP) Method Dimensionality Reduction. R package version 0.1.16.9000,  https://jlmelville.github.io/uwot/, https://github.com/jlmelville/uwot.","code":"@Manual{,   title = {uwot: The Uniform Manifold Approximation and Projection (UMAP) Method for Dimensionality Reduction},   author = {James Melville},   year = {2024},   note = {R package version 0.1.16.9000,  https://jlmelville.github.io/uwot/},   url = {https://github.com/jlmelville/uwot}, }"},{"path":"https://jlmelville.github.io/uwot/index.html","id":"uwot","dir":"","previous_headings":"","what":"The Uniform Manifold Approximation and Projection (UMAP) Method for\n    Dimensionality Reduction","title":"The Uniform Manifold Approximation and Projection (UMAP) Method for\n    Dimensionality Reduction","text":"R implementation Uniform Manifold Approximation Projection (UMAP) method dimensionality reduction McInnes et al. (2018). Also included supervised metric (--sample) learning extensions basic method. Translated Python implementation.","code":""},{"path":"https://jlmelville.github.io/uwot/index.html","id":"news","dir":"","previous_headings":"","what":"News","title":"The Uniform Manifold Approximation and Projection (UMAP) Method for\n    Dimensionality Reduction","text":"November 26 2023 Happy 1 million CRAN downloads uwot. celebrate (actually ’s just coincidence) reorganized horror-show sprawling README actual articles, via fantastic pkgdown package. Find https://jlmelville.github.io/uwot/.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/index.html","id":"from-cran","dir":"","previous_headings":"Installing","what":"From CRAN","title":"The Uniform Manifold Approximation and Projection (UMAP) Method for\n    Dimensionality Reduction","text":"","code":"install.packages(\"uwot\")"},{"path":"https://jlmelville.github.io/uwot/index.html","id":"from-github","dir":"","previous_headings":"Installing","what":"From github","title":"The Uniform Manifold Approximation and Projection (UMAP) Method for\n    Dimensionality Reduction","text":"uwot makes use C++ code must compiled. may carry extra steps able build package: Windows: install Rtools ensure C:\\Rtools\\bin path. Mac OS X: using custom ~/.R/Makevars may cause linking errors. sort thing potential problem platforms seems bite Mac owners . R Mac OS X FAQ may helpful work can get away . safe side, advise building uwot without custom Makevars.","code":"install.packages(\"devtools\") devtools::install_github(\"jlmelville/uwot\")"},{"path":"https://jlmelville.github.io/uwot/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"The Uniform Manifold Approximation and Projection (UMAP) Method for\n    Dimensionality Reduction","text":"","code":"library(uwot)  iris_umap <- umap(iris)  # Load mnist from somewhere, e.g. # devtools::install_github(\"jlmelville/snedata\") # mnist <- snedata::download_mnist()  mnist_umap <- umap(mnist, n_neighbors = 15, min_dist = 0.001, verbose = TRUE) plot(   mnist_umap,   cex = 0.1,   col = grDevices::rainbow(n = length(levels(mnist$Label)))[as.integer(mnist$Label)] |>     grDevices::adjustcolor(alpha.f = 0.1),   main = \"R uwot::umap\",   xlab = \"\",   ylab = \"\" )"},{"path":"https://jlmelville.github.io/uwot/index.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"The Uniform Manifold Approximation and Projection (UMAP) Method for\n    Dimensionality Reduction","text":"https://jlmelville.github.io/uwot/. examples see get started doc. plenty articles describing various aspects package.","code":""},{"path":"https://jlmelville.github.io/uwot/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"The Uniform Manifold Approximation and Projection (UMAP) Method for\n    Dimensionality Reduction","text":"GPLv3 later.","code":""},{"path":"https://jlmelville.github.io/uwot/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"The Uniform Manifold Approximation and Projection (UMAP) Method for\n    Dimensionality Reduction","text":"want cite use uwot, use output running citation(\"uwot\") (can R package).","code":""},{"path":"https://jlmelville.github.io/uwot/index.html","id":"see-also","dir":"","previous_headings":"","what":"See Also","title":"The Uniform Manifold Approximation and Projection (UMAP) Method for\n    Dimensionality Reduction","text":"UMAP reference implementation publication. UMAP R package (see also github repo), predates uwot’s arrival CRAN. Another R package umapr, longer maintained. umappp full C++ implementation, yaumap provides R wrapper. batch implementation umappp basis uwot’s attempt . uwot uses RcppProgress package show text-based progress bar verbose = TRUE.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/load_uwot.html","id":null,"dir":"Reference","previous_headings":"","what":"Save or Load a Model — load_uwot","title":"Save or Load a Model — load_uwot","text":"Functions write UMAP model file, restore.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/load_uwot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save or Load a Model — load_uwot","text":"","code":"load_uwot(file, verbose = FALSE)"},{"path":"https://jlmelville.github.io/uwot/reference/load_uwot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save or Load a Model — load_uwot","text":"file name file model saved read . verbose TRUE, log information console.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/load_uwot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save or Load a Model — load_uwot","text":"model saved file, use  umap_transform. Additionally, contains extra item:  mod_dir, contains path temporary working directory   used loading model. directory removed   model unloaded using unload_uwot.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/reference/load_uwot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Save or Load a Model — load_uwot","text":"","code":"iris_train <- iris[c(1:10, 51:60), ] iris_test <- iris[100:110, ]  # create model model <- umap(iris_train, ret_model = TRUE, n_epochs = 20)  # save without unloading: this leaves behind a temporary working directory model_file <- tempfile(\"iris_umap\") model <- save_uwot(model, file = model_file)  # The model can continue to be used test_embedding <- umap_transform(iris_test, model)  # To manually unload the model from memory when finished and to clean up # the working directory (this doesn't touch your model file) unload_uwot(model)  # At this point, model cannot be used with umap_transform, this would fail: # test_embedding2 <- umap_transform(iris_test, model)  # restore the model: this also creates a temporary working directory model2 <- load_uwot(file = model_file) test_embedding2 <- umap_transform(iris_test, model2)  # Unload and clean up the loaded model temp directory unload_uwot(model2)  # clean up the model file unlink(model_file)  # save with unloading: this deletes the temporary working directory but # doesn't allow the model to be re-used model3 <- umap(iris_train, ret_model = TRUE, n_epochs = 20) model_file3 <- tempfile(\"iris_umap\") model3 <- save_uwot(model3, file = model_file3, unload = TRUE)"},{"path":"https://jlmelville.github.io/uwot/reference/lvish.html","id":null,"dir":"Reference","previous_headings":"","what":"Dimensionality Reduction with a LargeVis-like method — lvish","title":"Dimensionality Reduction with a LargeVis-like method — lvish","text":"Carry dimensionality reduction dataset using method similar LargeVis (Tang et al., 2016).","code":""},{"path":"https://jlmelville.github.io/uwot/reference/lvish.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dimensionality Reduction with a LargeVis-like method — lvish","text":"","code":"lvish(   X,   perplexity = 50,   n_neighbors = perplexity * 3,   n_components = 2,   metric = \"euclidean\",   n_epochs = -1,   learning_rate = 1,   scale = \"maxabs\",   init = \"lvrandom\",   init_sdev = NULL,   repulsion_strength = 7,   negative_sample_rate = 5,   nn_method = NULL,   n_trees = 50,   search_k = 2 * n_neighbors * n_trees,   n_threads = NULL,   n_sgd_threads = 0,   grain_size = 1,   kernel = \"gauss\",   pca = NULL,   pca_center = TRUE,   pcg_rand = TRUE,   fast_sgd = FALSE,   ret_nn = FALSE,   ret_extra = c(),   tmpdir = tempdir(),   verbose = getOption(\"verbose\", TRUE),   batch = FALSE,   opt_args = NULL,   epoch_callback = NULL,   pca_method = NULL,   binary_edge_weights = FALSE )"},{"path":"https://jlmelville.github.io/uwot/reference/lvish.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dimensionality Reduction with a LargeVis-like method — lvish","text":"X Input data. Can data.frame, matrix, dist object sparseMatrix. Matrix data frames contain one observation per row. Data frames non-numeric columns removed, although factor columns used explicitly included via metric (see help metric details). sparse matrix interpreted distance matrix, assumed symmetric, can also pass explicitly upper lower triangular sparse matrix save storage. must least n_neighbors non-zero distances row. implicit explicit zero entries ignored. Set zero distances want keep arbitrarily small non-zero value (e.g. 1e-10). X can also NULL pre-computed nearest neighbor data passed nn_method, init \"spca\" \"pca\". perplexity Controls size local neighborhood used manifold approximation. analogous n_neighbors umap. Change , rather n_neighbors. n_neighbors number neighbors use calculating perplexity. Usually set three times value perplexity. Must least large perplexity. n_components dimension space embed . defaults 2 provide easy visualization, can reasonably set integer value range 2 100. metric Type distance metric use find nearest neighbors. One   : \"euclidean\" (default) \"cosine\" \"manhattan\" \"hamming\" \"correlation\" (distance based Pearson correlation) \"categorical\" (see ) applies nn_method = \"annoy\" (nn_method = \"fnn\", distance metric always \"euclidean\"). X data frame matrix, multiple metrics can specified, passing list argument, name item list one metric names . value list item vector giving names integer ids columns included calculation, e.g. metric = list(euclidean = 1:4, manhattan = 5:10). metric calculation results separate fuzzy simplicial set, intersected together produce final set. Metric names can repeated. non-numeric columns removed data frame, safer use column names integer ids. Factor columns can also used specifying metric name \"categorical\". Factor columns treated different numeric columns although multiple factor columns can specified vector, factor column specified processed individually. specify non-factor column, coerced factor. given data block, may override pca pca_center arguments block, providing list one unnamed item containing column names ids, pca pca_center overrides named items, e.g. metric = list(euclidean = 1:4, manhattan = list(5:10, pca_center = FALSE)). exists allow mixed binary real-valued data included PCA applied , centering applied real-valued data (typical apply centering binary data PCA applied). n_epochs Number epochs use optimization embedded coordinates. default calculate number epochs dynamically based dataset size, give number edge samples LargeVis defaults. usually substantially larger UMAP defaults. n_epochs = 0, coordinates determined \"init\" returned. learning_rate Initial learning rate used optimization coordinates. scale Scaling apply X data frame matrix: \"none\" FALSE NULL scaling. \"Z\" \"scale\" TRUE Scale column   zero mean variance 1. \"maxabs\" Center column mean 0, divide   element maximum absolute value entire matrix. \"range\" Range scale entire matrix, smallest   element 0 largest 1. \"colrange\" Scale column range (0,1). lvish, default \"maxabs\", consistency LargeVis. init Type initialization coordinates. Options : \"spectral\" Spectral embedding using normalized Laplacian    fuzzy 1-skeleton, Gaussian noise added. \"normlaplacian\". Spectral embedding using normalized    Laplacian fuzzy 1-skeleton, without noise. \"random\". Coordinates assigned using uniform random    distribution -10 10. \"lvrandom\". Coordinates assigned using Gaussian    distribution standard deviation 1e-4, used LargeVis    (Tang et al., 2016) t-SNE. \"laplacian\". Spectral embedding using Laplacian Eigenmap    (Belkin Niyogi, 2002). \"pca\". first two principal components PCA    X X data frame, 2-dimensional classical    MDS X class \"dist\". \"spca\". Like \"pca\", dimension scaled    standard deviation 1e-4, give distribution similar    used t-SNE LargeVis. alias init = \"pca\",    init_sdev = 1e-4. \"agspectral\" \"approximate global\" modification    \"spectral\" edges graph value 1,    sets random number edges (negative_sample_rate edges per    vertex) 0.1, approximate effect non-local affinities. matrix initial coordinates. spectral initializations, (\"spectral\", \"normlaplacian\", \"laplacian\", \"agspectral\"), one connected component identified, spectral initialization attempted. Instead PCA-based initialization attempted. verbose = TRUE number connected components logged console. existence multiple connected components implies global view data attained initialization. Increasing value n_neighbors may help. init_sdev non-NULL, scales dimension initialized coordinates (including user-supplied matrix) standard deviation. default scaling carried , except init = \"spca\", case value 0.0001. Scaling input may help unscaled versions result initial coordinates large inter-point distances outliers. usually results small gradients optimization little progress made layout. Shrinking initial embedding rescaling can help circumstances. Scaling result init = \"pca\" usually recommended init = \"spca\" alias init = \"pca\", init_sdev = 1e-4 spectral initializations scaled versions usually necessary unless using large value n_neighbors (e.g. n_neighbors = 150 higher). compatibility recent versions Python UMAP package, using init = \"spectral\", also set init_sdev = \"range\", range scale columns containing initial data 0-10. set default maintain backwards compatibility previous versions uwot. repulsion_strength Weighting applied negative samples low dimensional embedding optimization. Values higher one result greater weight given negative samples. negative_sample_rate number negative edge/1-simplex samples use per positive edge/1-simplex sample optimizing low dimensional embedding. nn_method Method finding nearest neighbors. Options : \"fnn\". Use exact nearest neighbors via     FNN package. \"annoy\" Use approximate nearest neighbors via     RcppAnnoy package. default, X less 4,096 vertices, exact nearest neighbors found. Otherwise, approximate nearest neighbors used. may also pass precalculated nearest neighbor data argument. must list consisting two elements: \"idx\". n_vertices x n_neighbors matrix   containing integer indexes nearest neighbors X.   vertex considered nearest neighbor, .e.   idx[, 1] == 1:n_vertices. \"dist\". n_vertices x n_neighbors matrix   containing distances nearest neighbors. Multiple nearest neighbor data (e.g. two different precomputed metrics) can passed passing list containing nearest neighbor data lists items. n_neighbors parameter ignored using precomputed nearest neighbor data. n_trees Number trees build constructing nearest neighbor index. trees specified, larger index, better results. search_k, determines accuracy Annoy nearest neighbor search. used nn_method \"annoy\". Sensible values 10 100. search_k Number nodes search neighbor retrieval. larger k, accurate results, longer search takes. n_trees, determines accuracy Annoy nearest neighbor search. used nn_method \"annoy\". n_threads Number threads use (except stochastic gradient descent). Default half number concurrent threads supported system. nearest neighbor search, applies nn_method = \"annoy\". n_threads > 1, Annoy index temporarily written disk location determined tempfile. n_sgd_threads Number threads use stochastic gradient descent. set > 1, aware batch = FALSE, results reproducible, even set.seed called fixed seed running. Set \"auto\" use value n_threads. grain_size minimum amount work thread. value set high enough, less n_threads n_sgd_threads used processing, might give performance improvement overhead thread management context switching outweighing improvement due concurrent processing. left default (1) work spread evenly threads specified. kernel Type kernel function create input probabilities. Can one \"gauss\" (default) \"knn\". \"gauss\" uses usual Gaussian weighted similarities. \"knn\" assigns equal probabilities every edge nearest neighbor graph, zero otherwise, using perplexity nearest neighbors. n_neighbors parameter ignored case. pca set positive integer value, reduce data number columns using PCA. applied distance metric \"hamming\", dimensions data larger number specified (.e. number rows columns must larger value parameter). > 100 columns data frame matrix, reducing number columns way may substantially increase performance nearest neighbor search cost potential decrease accuracy. many t-SNE applications, value 50 recommended, although guarantee appropriate settings. pca_center TRUE, center columns X carrying PCA. binary data, recommended set FALSE. pcg_rand TRUE, use PCG random number generator (O'Neill, 2014) optimization. Otherwise, use faster (probably less statistically good) Tausworthe \"taus88\" generator. default TRUE. fast_sgd TRUE, following combination parameters set: pcg_rand = TRUE n_sgd_threads = \"auto\". default FALSE. Setting TRUE speed stochastic optimization phase, give potentially less accurate embedding, exactly reproducible even fixed seed. visualization, fast_sgd = TRUE give perfectly good results. generic dimensionality reduction, safer leave fast_sgd = FALSE. fast_sgd = TRUE, user-supplied values pcg_rand n_sgd_threads, ignored. ret_nn TRUE, addition embedding, also return nearest neighbor data can used input nn_method avoid overhead repeatedly calculating nearest neighbors manipulating unrelated parameters (e.g. min_dist, n_epochs, init). See \"Value\" section names list items. FALSE, just return coordinates. Note nearest neighbors sensitive data scaling, wary reusing nearest neighbor data modifying scale parameter. ret_extra vector indicating extra data return. May contain combination following strings: \"nn\" setting ret_nn = TRUE. \"P\" high dimensional probability matrix. graph     returned sparse symmetric N x N matrix class     dgCMatrix-class, non-zero entry (, j) gives     input probability (similarity affinity) edge connecting     vertex vertex j. Note graph sparsified     removing edges sufficiently low membership strength     sampled probabilistic edge sampling employed     optimization therefore number non-zero elements     matrix dependent n_epochs. interested     fuzzy input graph (e.g. clustering), setting     n_epochs = 0 avoid sparsifying. aware     setting binary_edge_weights = TRUE affect graph (    non-zero edge weights 1). sigma vector bandwidths used calibrate input     Gaussians reproduce target \"perplexity\". tmpdir Temporary directory store nearest neighbor indexes nearest neighbor search. Default tempdir. index written disk n_threads > 1 nn_method = \"annoy\"; otherwise, parameter ignored. verbose TRUE, log details console. batch TRUE, embedding coordinates updated end epoch rather epoch. batch mode, results reproducible fixed random seed even n_sgd_threads > 1, cost slightly higher memory use. may also modify learning_rate increase n_epochs, whether provides speed increase single-threaded optimization likely dataset hardware-dependent. opt_args list optimizer parameters, used batch = TRUE. default optimization method used Adam (Kingma Ba, 2014). method optimization method use. Either \"adam\"   \"sgd\" (stochastic gradient descent). Default: \"adam\". beta1 (Adam ). weighting parameter   exponential moving average first moment estimator. Effectively   momentum parameter. floating point value 0 1.   Higher values can smooth oscillatory updates poorly-conditioned   situations may allow larger learning_rate   specified, high can cause divergence. Default: 0.5. beta2 (Adam ). weighting parameter   exponential moving average uncentered second moment estimator.   floating point value 0 1. Controls degree   adaptivity step-size. Higher values put weight previous   time steps. Default: 0.9. eps (Adam ). Intended small value prevent   division zero, practice can also affect convergence due   interaction beta2. Higher values reduce effect   step-size adaptivity bring behavior closer stochastic gradient   descent momentum. Typical values 1e-8 1e-3. Default:   1e-7. alpha initial learning rate. Default: value   learning_rate parameter. epoch_callback function invoked end every epoch. signature : (epoch, n_epochs, coords), : epoch current epoch number (1   n_epochs). n_epochs Number epochs use optimization   embedded coordinates. coords embedded coordinates end current   epoch, matrix dimensions (N, n_components). pca_method Method carry PCA dimensionality reduction pca parameter specified. Allowed values : \"irlba\". Uses prcomp_irlba   irlba package. \"rsvd\". Uses 5 iterations svdr   irlba package.   likely give much faster potentially less accurate results   using \"irlba\". purposes nearest neighbor   calculation coordinates initialization, loss accuracy   seem matter much. \"bigstatsr\". Uses big_randomSVD   bigstatsr   package. SVD methods used bigstatsr may faster   systems without access efficient linear algebra libraries (e.g.   Windows). Note: bigstatsr dependency   uwot: choose use package PCA, must install   . \"svd\". Uses svd SVD.   likely slow smallest datasets. \"auto\" (default). Uses \"irlba\", unless   50   case \"svd\" used. binary_edge_weights TRUE edge weights input graph treated binary (0/1) rather real valued. affects sampling frequency neighbors strategy used PaCMAP method (Wang co-workers, 2020). Practical (Böhm co-workers, 2020) theoretical (Damrich Hamprecht, 2021) work suggests little effect UMAP's performance.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/lvish.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Dimensionality Reduction with a LargeVis-like method — lvish","text":"matrix optimized coordinates, :  ret_nn = TRUE (ret_extra contains \"nn\"),     returns nearest neighbor data list called nn.     contains one list metric calculated, containing     matrix idx integer ids neighbors; matrix     dist distances. nn list (sub-list) can     used input nn_method parameter. ret_extra contains \"P\", returns high     dimensional probability matrix sparse matrix called P,     type dgCMatrix-class. ret_extra contains \"sigma\", returns vector     high dimensional gaussian bandwidths point,     \"dint\" vector estimates intrinsic dimensionality     point, based method given Lee co-workers (2015). returned list contains combined data combination   specifying ret_nn ret_extra.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/lvish.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Dimensionality Reduction with a LargeVis-like method — lvish","text":"lvish differs official LargeVis implementation following: nearest-neighbor index search phase multi-threaded. Matrix input data normalized. n_trees parameter dynamically chosen based   data set size. Nearest neighbor results refined via   neighbor---neighbor method. search_k parameter twice   large default compensate. Gradient values clipped 4.0 rather 5.0. Negative edges generated uniform sampling vertexes rather   degree ^ 0.75. default number samples much reduced. default number   epochs, n_epochs, set 5000, much larger   umap, may need increased depending   dataset. Using init = \"spectral\" can help.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/lvish.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Dimensionality Reduction with a LargeVis-like method — lvish","text":"Tang, J., Liu, J., Zhang, M., & Mei, Q. (2016, April). Visualizing large-scale high-dimensional data. Proceedings 25th International Conference World Wide Web (pp. 287-297). International World Wide Web Conferences Steering Committee. https://arxiv.org/abs/1602.00370 Lee, J. ., Peluffo-Ordóñez, D. H., & Verleysen, M. (2015). Multi-scale similarities stochastic neighbour embedding: Reducing dimensionality preserving local global structure. Neurocomputing, 169, 246-261.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/lvish.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Dimensionality Reduction with a LargeVis-like method — lvish","text":"","code":"# Default number of epochs is much larger than for UMAP, assumes random # initialization. Use perplexity rather than n_neighbors to control the size # of the local neighborhood 20 epochs may be too small for a random # initialization iris_lvish <- lvish(iris,   perplexity = 50, learning_rate = 0.5,   init = \"random\", n_epochs = 20 )"},{"path":"https://jlmelville.github.io/uwot/reference/optimize_graph_layout.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimize Graph Layout — optimize_graph_layout","title":"Optimize Graph Layout — optimize_graph_layout","text":"Carry dimensionality reduction input graph, distances low dimensional space attempt reproduce neighbor relations input data. default, cost function used optimize output coordinates use Uniform Manifold Approximation Projection (UMAP) method (McInnes et al., 2018), approach LargeVis (Tang et al., 2016) can also used. function can used produce low dimensional representation graph produced similarity_graph.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/optimize_graph_layout.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimize Graph Layout — optimize_graph_layout","text":"","code":"optimize_graph_layout(   graph,   X = NULL,   n_components = 2,   n_epochs = NULL,   learning_rate = 1,   init = \"spectral\",   init_sdev = NULL,   spread = 1,   min_dist = 0.01,   repulsion_strength = 1,   negative_sample_rate = 5,   a = NULL,   b = NULL,   method = \"umap\",   approx_pow = FALSE,   pcg_rand = TRUE,   fast_sgd = FALSE,   n_sgd_threads = 0,   grain_size = 1,   verbose = getOption(\"verbose\", TRUE),   batch = FALSE,   opt_args = NULL,   epoch_callback = NULL,   pca_method = NULL,   binary_edge_weights = FALSE )"},{"path":"https://jlmelville.github.io/uwot/reference/optimize_graph_layout.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimize Graph Layout — optimize_graph_layout","text":"graph sparse, symmetric N x N weighted adjacency matrix representing graph. Non-zero entries indicate edge two nodes given edge weight. can varying number non-zero entries row/column. X Optional input data. Used PCA-based initialization. n_components dimension space embed . defaults 2 provide easy visualization, can reasonably set integer value range 2 100. n_epochs Number epochs use optimization   embedded coordinates. default, value set 500   datasets containing 10,000 vertices less, 200 otherwise.   n_epochs = 0, coordinates determined \"init\"   returned. UMAP, default \"none\". learning_rate Initial learning rate used optimization coordinates. init Type initialization coordinates. Options : \"spectral\" Spectral embedding using normalized Laplacian    fuzzy 1-skeleton, Gaussian noise added. \"normlaplacian\". Spectral embedding using normalized    Laplacian fuzzy 1-skeleton, without noise. \"random\". Coordinates assigned using uniform random    distribution -10 10. \"lvrandom\". Coordinates assigned using Gaussian    distribution standard deviation 1e-4, used LargeVis    (Tang et al., 2016) t-SNE. \"laplacian\". Spectral embedding using Laplacian Eigenmap. \"pca\". first two principal components PCA    X X data frame, 2-dimensional classical    MDS X class \"dist\". \"spca\". Like \"pca\", dimension scaled    standard deviation 1e-4, give distribution similar    used t-SNE. alias init = \"pca\", init_sdev =    1e-4. \"agspectral\" \"approximate global\" modification    \"spectral\" edges graph value 1,    sets random number edges (negative_sample_rate edges per    vertex) 0.1, approximate effect non-local affinities. matrix initial coordinates. spectral initializations, (\"spectral\", \"normlaplacian\", \"laplacian\", \"agspectral\"), one connected component identified, spectral initialization attempted. Instead PCA-based initialization attempted. verbose = TRUE number connected components logged console. existence multiple connected components implies global view data attained initialization. Increasing value n_neighbors may help. init_sdev non-NULL, scales dimension initialized coordinates (including user-supplied matrix) standard deviation. default scaling carried , except init = \"spca\", case value 0.0001. Scaling input may help unscaled versions result initial coordinates large inter-point distances outliers. usually results small gradients optimization little progress made layout. Shrinking initial embedding rescaling can help circumstances. Scaling result init = \"pca\" usually recommended init = \"spca\" alias init = \"pca\", init_sdev = 1e-4 spectral initializations scaled versions usually necessary unless using large value n_neighbors (e.g. n_neighbors = 150 higher). compatibility recent versions Python UMAP package, using init = \"spectral\", also set init_sdev = \"range\", range scale columns containing initial data 0-10. set default maintain backwards compatibility previous versions uwot. spread effective scale embedded points. combination min_dist, determines clustered/clumped embedded points . min_dist effective minimum distance embedded points. Smaller values result clustered/clumped embedding nearby points manifold drawn closer together, larger values result even dispersal points. value set relative spread value, determines scale embedded points spread . repulsion_strength Weighting applied negative samples low dimensional embedding optimization. Values higher one result greater weight given negative samples. negative_sample_rate number negative edge/1-simplex samples use per positive edge/1-simplex sample optimizing low dimensional embedding. specific parameters controlling embedding. NULL values set automatically determined min_dist spread. b specific parameters controlling embedding. NULL values set automatically determined min_dist spread. method Cost function optimize. One : \"umap\". UMAP method McInnes co-workers (2018). \"tumap\". UMAP b parameters fixed   1. \"largevis\". LargeVis method Tang co-workers (2016). approx_pow TRUE, use approximation power function UMAP gradient, https://martin.ankerl.com/2012/01/25/optimized-approximative-pow--c--cpp/. pcg_rand TRUE, use PCG random number generator (O'Neill, 2014) optimization. Otherwise, use faster (probably less statistically good) Tausworthe \"taus88\" generator. default TRUE. fast_sgd TRUE, following combination parameters set: pcg_rand = TRUE, n_sgd_threads = \"auto\" approx_pow = TRUE. default FALSE. Setting TRUE speed stochastic optimization phase, give potentially less accurate embedding, exactly reproducible even fixed seed. visualization, fast_sgd = TRUE give perfectly good results. generic dimensionality reduction, safer leave fast_sgd = FALSE. fast_sgd = TRUE, user-supplied values pcg_rand, n_sgd_threads, approx_pow ignored. n_sgd_threads Number threads use stochastic gradient descent. set > 1, aware batch = FALSE, results reproducible, even set.seed called fixed seed running. set \"auto\" half number concurrent threads supported system used. grain_size minimum amount work thread. value set high enough, less n_threads n_sgd_threads used processing, might give performance improvement overhead thread management context switching outweighing improvement due concurrent processing. left default (1) work spread evenly threads specified. verbose TRUE, log details console. batch TRUE, embedding coordinates updated end epoch rather epoch. batch mode, results reproducible fixed random seed even n_sgd_threads > 1, cost slightly higher memory use. may also modify learning_rate increase n_epochs, whether provides speed increase single-threaded optimization likely dataset hardware-dependent. opt_args list optimizer parameters, used batch = TRUE. default optimization method used Adam (Kingma Ba, 2014). method optimization method use. Either \"adam\"   \"sgd\" (stochastic gradient descent). Default: \"adam\". beta1 (Adam ). weighting parameter   exponential moving average first moment estimator. Effectively   momentum parameter. floating point value 0 1.   Higher values can smooth oscillatory updates poorly-conditioned   situations may allow larger learning_rate   specified, high can cause divergence. Default: 0.5. beta2 (Adam ). weighting parameter   exponential moving average uncentered second moment estimator.   floating point value 0 1. Controls degree   adaptivity step-size. Higher values put weight previous   time steps. Default: 0.9. eps (Adam ). Intended small value prevent   division zero, practice can also affect convergence due   interaction beta2. Higher values reduce effect   step-size adaptivity bring behavior closer stochastic gradient   descent momentum. Typical values 1e-8 1e-3. Default:   1e-7. alpha initial learning rate. Default: value   learning_rate parameter. epoch_callback function invoked end every epoch. signature : (epoch, n_epochs, coords), : epoch current epoch number (1   n_epochs). n_epochs Number epochs use optimization   embedded coordinates. coords embedded coordinates end current   epoch, matrix dimensions (N, n_components). pca_method Method carry PCA dimensionality reduction pca parameter specified. Allowed values : \"irlba\". Uses prcomp_irlba   irlba package. \"rsvd\". Uses 5 iterations svdr   irlba package.   likely give much faster potentially less accurate results   using \"irlba\". purposes nearest neighbor   calculation coordinates initialization, loss accuracy   seem matter much. \"bigstatsr\". Uses big_randomSVD   bigstatsr   package. SVD methods used bigstatsr may faster   systems without access efficient linear algebra libraries (e.g.   Windows). Note: bigstatsr dependency   uwot: choose use package PCA, must install   . \"svd\". Uses svd SVD.   likely slow smallest datasets. \"auto\" (default). Uses \"irlba\", unless   50   case \"svd\" used. binary_edge_weights TRUE edge weights input graph treated binary (0/1) rather real valued.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/optimize_graph_layout.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimize Graph Layout — optimize_graph_layout","text":"matrix optimized coordinates.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/optimize_graph_layout.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimize Graph Layout — optimize_graph_layout","text":"Kingma, D. P., & Ba, J. (2014). Adam: method stochastic optimization. arXiv preprint arXiv:1412.6980. https://arxiv.org/abs/1412.6980 McInnes, L., Healy, J., & Melville, J. (2018). UMAP: Uniform Manifold Approximation Projection Dimension Reduction arXiv preprint arXiv:1802.03426. https://arxiv.org/abs/1802.03426 Tang, J., Liu, J., Zhang, M., & Mei, Q. (2016, April). Visualizing large-scale high-dimensional data. Proceedings 25th International Conference World Wide Web (pp. 287-297). International World Wide Web Conferences Steering Committee. https://arxiv.org/abs/1602.00370","code":""},{"path":"https://jlmelville.github.io/uwot/reference/optimize_graph_layout.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimize Graph Layout — optimize_graph_layout","text":"","code":"iris30 <- iris[c(1:10, 51:60, 101:110), ]  # return a 30 x 30 sparse matrix with similarity data based on 10 nearest # neighbors per item iris30_sim_graph <- similarity_graph(iris30, n_neighbors = 10) # produce 2D coordinates replicating the neighbor relations in the similarity # graph set.seed(42) iris30_opt <- optimize_graph_layout(iris30_sim_graph, X = iris30)  # the above two steps are the same as: # set.seed(42); iris_umap <- umap(iris30, n_neighbors = 10)"},{"path":"https://jlmelville.github.io/uwot/reference/save_uwot.html","id":null,"dir":"Reference","previous_headings":"","what":"Save or Load a Model — save_uwot","title":"Save or Load a Model — save_uwot","text":"Functions write UMAP model file, restore.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/save_uwot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save or Load a Model — save_uwot","text":"","code":"save_uwot(model, file, unload = FALSE, verbose = FALSE)"},{"path":"https://jlmelville.github.io/uwot/reference/save_uwot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save or Load a Model — save_uwot","text":"model UMAP model create umap. file name file model saved read . unload TRUE, unload nearest neighbor indexes model. model longer valid use umap_transform temporary working directory used model saving deleted. need reload model load_uwot use model. FALSE, model can re-used without reloading, must manually unload NN index finished using want delete temporary working directory. unload manually, use unload_uwot. absolute path working directory found mod_dir item return value. verbose TRUE, log information console.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/save_uwot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save or Load a Model — save_uwot","text":"model one extra item: mod_dir, contains   path working directory. unload = FALSE directory   still exists function returns, can cleaned  unload_uwot. care cleaning   directory, unload = TRUE, can ignore return value.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/reference/save_uwot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Save or Load a Model — save_uwot","text":"","code":"iris_train <- iris[c(1:10, 51:60), ] iris_test <- iris[100:110, ]  # create model model <- umap(iris_train, ret_model = TRUE, n_epochs = 20)  # save without unloading: this leaves behind a temporary working directory model_file <- tempfile(\"iris_umap\") model <- save_uwot(model, file = model_file)  # The model can continue to be used test_embedding <- umap_transform(iris_test, model)  # To manually unload the model from memory when finished and to clean up # the working directory (this doesn't touch your model file) unload_uwot(model)  # At this point, model cannot be used with umap_transform, this would fail: # test_embedding2 <- umap_transform(iris_test, model)  # restore the model: this also creates a temporary working directory model2 <- load_uwot(file = model_file) test_embedding2 <- umap_transform(iris_test, model2)  # Unload and clean up the loaded model temp directory unload_uwot(model2)  # clean up the model file unlink(model_file)  # save with unloading: this deletes the temporary working directory but # doesn't allow the model to be re-used model3 <- umap(iris_train, ret_model = TRUE, n_epochs = 20) model_file3 <- tempfile(\"iris_umap\") model3 <- save_uwot(model3, file = model_file3, unload = TRUE)"},{"path":"https://jlmelville.github.io/uwot/reference/similarity_graph.html","id":null,"dir":"Reference","previous_headings":"","what":"Similarity Graph — similarity_graph","title":"Similarity Graph — similarity_graph","text":"Create graph (sparse symmetric weighted adjacency matrix) representing similarities items data set. dimensionality reduction carried . default, similarities calculated using merged fuzzy simplicial set approach Uniform Manifold Approximation Projection (UMAP) method (McInnes et al., 2018), approach LargeVis (Tang et al., 2016) can also used.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/similarity_graph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Similarity Graph — similarity_graph","text":"","code":"similarity_graph(   X = NULL,   n_neighbors = NULL,   metric = \"euclidean\",   scale = NULL,   set_op_mix_ratio = 1,   local_connectivity = 1,   nn_method = NULL,   n_trees = 50,   search_k = 2 * n_neighbors * n_trees,   perplexity = 50,   method = \"umap\",   y = NULL,   target_n_neighbors = n_neighbors,   target_metric = \"euclidean\",   target_weight = 0.5,   pca = NULL,   pca_center = TRUE,   ret_extra = c(),   n_threads = NULL,   grain_size = 1,   kernel = \"gauss\",   tmpdir = tempdir(),   verbose = getOption(\"verbose\", TRUE),   pca_method = NULL,   binary_edge_weights = FALSE )"},{"path":"https://jlmelville.github.io/uwot/reference/similarity_graph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Similarity Graph — similarity_graph","text":"X Input data. Can data.frame, matrix, dist object sparseMatrix. Matrix data frames contain one observation per row. Data frames non-numeric columns removed, although factor columns used explicitly included via metric (see help metric details). sparse matrix interpreted distance matrix, assumed symmetric, can also pass explicitly upper lower triangular sparse matrix save storage. must least n_neighbors non-zero distances row. implicit explicit zero entries ignored. Set zero distances want keep arbitrarily small non-zero value (e.g. 1e-10). X can also NULL pre-computed nearest neighbor data passed nn_method. n_neighbors size local neighborhood (terms number neighboring sample points) used manifold approximation. Larger values result global views manifold, smaller values result local data preserved. general values range 2 100. metric Type distance metric use find nearest neighbors. One   : \"euclidean\" (default) \"cosine\" \"manhattan\" \"hamming\" \"correlation\" (distance based Pearson correlation) \"categorical\" (see ) applies nn_method = \"annoy\" (nn_method = \"fnn\", distance metric always \"euclidean\"). X data frame matrix, multiple metrics can specified, passing list argument, name item list one metric names . value list item vector giving names integer ids columns included calculation, e.g. metric = list(euclidean = 1:4, manhattan = 5:10). metric calculation results separate fuzzy simplicial set, intersected together produce final set. Metric names can repeated. non-numeric columns removed data frame, safer use column names integer ids. Factor columns can also used specifying metric name \"categorical\". Factor columns treated different numeric columns although multiple factor columns can specified vector, factor column specified processed individually. specify non-factor column, coerced factor. given data block, may override pca pca_center arguments block, providing list one unnamed item containing column names ids, pca pca_center overrides named items, e.g. metric = list(euclidean = 1:4, manhattan = list(5:10, pca_center = FALSE)). exists allow mixed binary real-valued data included PCA applied , centering applied real-valued data (typical apply centering binary data PCA applied). scale Scaling apply X data frame matrix: \"none\" FALSE NULL scaling. \"Z\" \"scale\" TRUE Scale column   zero mean variance 1. \"maxabs\" Center column mean 0, divide   element maximum absolute value entire matrix. \"range\" Range scale entire matrix, smallest   element 0 largest 1. \"colrange\" Scale column range (0,1). method \"umap\", default \"none\". \"largevis\", default \"maxabs\". set_op_mix_ratio Interpolate (fuzzy) union intersection set operation used combine local fuzzy simplicial sets obtain global fuzzy simplicial sets. fuzzy set operations use product t-norm. value parameter 0.0 1.0; value 1.0 use pure fuzzy union, 0.0 use pure fuzzy intersection. Ignored method = \"largevis\" local_connectivity local connectivity required -- .e. number nearest neighbors assumed connected local level. higher value connected manifold becomes locally. practice local intrinsic dimension manifold. Ignored method = \"largevis\". nn_method Method finding nearest neighbors. Options : \"fnn\". Use exact nearest neighbors via     FNN package. \"annoy\" Use approximate nearest neighbors via     RcppAnnoy package. default, X less 4,096 vertices, exact nearest neighbors found. Otherwise, approximate nearest neighbors used. may also pass pre-calculated nearest neighbor data argument. must one two formats, either list consisting two elements: \"idx\". n_vertices x n_neighbors matrix   containing integer indexes nearest neighbors X.   vertex considered nearest neighbor, .e.   idx[, 1] == 1:n_vertices. \"dist\". n_vertices x n_neighbors matrix   containing distances nearest neighbors. sparse distance matrix type dgCMatrix, dimensions n_vertices x n_vertices. Distances arranged column, .e. non-zero entry row j ith column indicates jth observation X nearest neighbor ith observation distance given value element. n_neighbors parameter ignored using precomputed nearest neighbor data. using sparse distance matrix input, column can contain different number neighbors. n_trees Number trees build constructing nearest neighbor index. trees specified, larger index, better results. search_k, determines accuracy Annoy nearest neighbor search. used nn_method \"annoy\". Sensible values 10 100. search_k Number nodes search neighbor retrieval. larger k, accurate results, longer search takes. n_trees, determines accuracy Annoy nearest neighbor search. used nn_method \"annoy\". perplexity Used method = \"largevis\". Controls size local neighborhood used manifold approximation. value 1 one less number items X. specified, specify value n_neighbors unless know . method generate similarities items. One : \"umap\" UMAP method McInnes et al. (2018). \"largevis\" LargeVis method Tang et al. (2016). y Optional target data add supervised semi-supervised weighting   similarity graph . Can vector, matrix data frame. Use   target_metric parameter specify metrics use, using   syntax metric. Usually either single numeric factor   column used, complex formats possible. following types   allowed: Factor columns length X. NA     allowed observation unknown level, case     UMAP operates form semi-supervised learning. column     treated separately. Numeric data. NA allowed case. Use     parameter target_n_neighbors set number neighbors used     y. unset, n_neighbors used. Unlike factors,     numeric columns grouped one block unless target_metric     specifies otherwise. example, wish columns     b treated separately, specify     target_metric = list(euclidean = \"\", euclidean = \"b\"). Otherwise,     data effectively treated matrix two columns. Nearest neighbor data, consisting list two matrices,     idx dist. represent precalculated nearest     neighbor indices distances, respectively.     format expected precalculated data     nn_method. format assumes underlying data     numeric vector. user-supplied value target_n_neighbors     parameter ignored case, number columns     matrices used value. Multiple nearest neighbor data using     different metrics can supplied passing list lists. Unlike X, factor columns included y automatically used. parameter ignored method = \"largevis\". target_n_neighbors Number nearest neighbors use construct target simplicial set. Default value n_neighbors. Applies y non-NULL numeric.  parameter ignored method = \"largevis\". target_metric metric used measure distance y using supervised dimension reduction. Used y numeric. parameter ignored method = \"largevis\". target_weight Weighting factor data topology target topology. value 0.0 weights entirely data, value 1.0 weights entirely target. default 0.5 balances weighting equally data target. applies y non-NULL. parameter ignored method = \"largevis\". pca set positive integer value, reduce data number columns using PCA. applied distance metric \"hamming\", dimensions data larger number specified (.e. number rows columns must larger value parameter). > 100 columns data frame matrix, reducing number columns way may substantially increase performance nearest neighbor search cost potential decrease accuracy. many t-SNE applications, value 50 recommended, although guarantee appropriate settings. pca_center TRUE, center columns X carrying PCA. binary data, recommended set FALSE. ret_extra vector indicating extra data return. May contain combination following strings: \"nn\" nearest neighbor data can used input     nn_method avoid overhead repeatedly calculating     nearest neighbors manipulating unrelated parameters. See     \"Value\" section names list items. Note nearest     neighbors sensitive data scaling, wary reusing     nearest neighbor data modifying scale parameter. \"sigma\" normalization value observation     dataset constructing smoothed distances     neighbors. gives sense local density     observation high dimensional space: higher values     sigma indicate higher dispersion lower density. n_threads Number threads use. Default half number concurrent threads supported system. nearest neighbor search, applies nn_method = \"annoy\". n_threads > 1, Annoy index temporarily written disk location determined tempfile. grain_size minimum amount work thread. value set high enough, less n_threads used processing, might give performance improvement overhead thread management context switching outweighing improvement due concurrent processing. left default (1) work spread evenly threads specified. kernel Used method = \"largevis\". Type kernel function create input similiarties. Can one \"gauss\" (default) \"knn\". \"gauss\" uses usual Gaussian weighted similarities. \"knn\" assigns equal similiarties. every edge nearest neighbor graph, zero otherwise, using perplexity nearest neighbors. n_neighbors parameter ignored case. tmpdir Temporary directory store nearest neighbor indexes nearest neighbor search. Default tempdir. index written disk n_threads > 1 nn_method = \"annoy\"; otherwise, parameter ignored. verbose TRUE, log details console. pca_method Method carry PCA dimensionality reduction pca parameter specified. Allowed values : \"irlba\". Uses prcomp_irlba   irlba package. \"rsvd\". Uses 5 iterations svdr   irlba package.   likely give much faster potentially less accurate results   using \"irlba\". purposes nearest neighbor   calculation coordinates initialization, loss accuracy   seem matter much. \"bigstatsr\". Uses big_randomSVD   bigstatsr   package. SVD methods used bigstatsr may faster   systems without access efficient linear algebra libraries (e.g.   Windows). Note: bigstatsr dependency   uwot: choose use package PCA, must install   . \"svd\". Uses svd SVD.   likely slow smallest datasets. \"auto\" (default). Uses \"irlba\", unless   50   case \"svd\" used. binary_edge_weights TRUE edge weights returned graph binary (0/1) rather reflecting degree similarity.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/similarity_graph.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Similarity Graph — similarity_graph","text":"sparse symmetrized matrix similarities items  X nn_method contains pre-computed nearest neighbor   data, items nn_method. symmetrization,   may non-zero items column specified value  n_neighbors (pre-computed neighbors nn_method).   ret_extra specified return value list   containing:  similarity_graph similarity graph sparse matrix     described . nn (ret_extra contained \"nn\") nearest     neighbor data list called nn. contains one list     metric calculated, containing matrix idx     integer ids neighbors; matrix dist     distances. nn list (sub-list) can used input     nn_method parameter. sigma (ret_extra contains \"sigma\"),     vector calibrated parameters, one item input data,     reflecting local data density item. exact definition     values depends choice method parameter. rho (ret_extra contains \"sigma\"),     vector containing largest distance locally connected neighbors     item input data. exist     method = \"umap\". localr (ret_extra contains \"localr\")     vector estimated local radii, sum \"sigma\"     \"rho\". exist method = \"umap\".","code":""},{"path":"https://jlmelville.github.io/uwot/reference/similarity_graph.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Similarity Graph — similarity_graph","text":"equivalent running umap ret_extra = c(\"fgraph\") parameter, without overhead calculating (returning) optimized low-dimensional coordinates.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/similarity_graph.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Similarity Graph — similarity_graph","text":"McInnes, L., Healy, J., & Melville, J. (2018). UMAP: Uniform Manifold Approximation Projection Dimension Reduction arXiv preprint arXiv:1802.03426. https://arxiv.org/abs/1802.03426 Tang, J., Liu, J., Zhang, M., & Mei, Q. (2016, April). Visualizing large-scale high-dimensional data. Proceedings 25th International Conference World Wide Web (pp. 287-297). International World Wide Web Conferences Steering Committee. https://arxiv.org/abs/1602.00370","code":""},{"path":"https://jlmelville.github.io/uwot/reference/similarity_graph.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Similarity Graph — similarity_graph","text":"","code":"iris30 <- iris[c(1:10, 51:60, 101:110), ]  # return a 30 x 30 sparse matrix with similarity data based on 10 nearest # neighbors per item iris30_sim_graph <- similarity_graph(iris30, n_neighbors = 10)  # Default is to use the UMAP method of calculating similarities, but LargeVis # is also available: for that method, use perplexity instead of n_neighbors # to control neighborhood size. Use ret_extra = \"nn\" to return nearest # neighbor data as well as the similarity graph. Return value is a list # containing similarity_graph' and 'nn' items. iris30_lv_graph <- similarity_graph(iris30,   perplexity = 10,   method = \"largevis\", ret_extra = \"nn\" ) # If you have the neighbor information you don't need the original data iris30_lv_graph_nn <- similarity_graph(   nn_method = iris30_lv_graph$nn,   perplexity = 10, method = \"largevis\" ) all(iris30_lv_graph_nn == iris30_lv_graph$similarity_graph) #> [1] TRUE"},{"path":"https://jlmelville.github.io/uwot/reference/simplicial_set_intersect.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge Similarity Graph by Simplicial Set Intersection — simplicial_set_intersect","title":"Merge Similarity Graph by Simplicial Set Intersection — simplicial_set_intersect","text":"Combine two similarity graphs treating fuzzy topological sets forming intersection.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/simplicial_set_intersect.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge Similarity Graph by Simplicial Set Intersection — simplicial_set_intersect","text":"","code":"simplicial_set_intersect(x, y, weight = 0.5, n_threads = NULL, verbose = FALSE)"},{"path":"https://jlmelville.github.io/uwot/reference/simplicial_set_intersect.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge Similarity Graph by Simplicial Set Intersection — simplicial_set_intersect","text":"x sparse matrix representing first similarity graph intersection operation. y sparse matrix representing second similarity graph intersection operation. weight value 0 - 1, controlling relative influence x y intersection. Default (0.5) gives equal influence. Values smaller 0.5 put weight x. Values greater 0.5 put weight y. n_threads Number threads use resetting local metric. Default half number concurrent threads supported system. verbose TRUE, log progress console.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/simplicial_set_intersect.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge Similarity Graph by Simplicial Set Intersection — simplicial_set_intersect","text":"sparse matrix containing intersection x  y.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/simplicial_set_intersect.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merge Similarity Graph by Simplicial Set Intersection — simplicial_set_intersect","text":"","code":"# Form two different \"views\" of the same data iris30 <- iris[c(1:10, 51:60, 101:110), ] iris_sg12 <- similarity_graph(iris30[, 1:2], n_neighbors = 5) iris_sg34 <- similarity_graph(iris30[, 3:4], n_neighbors = 5)  # Combine the two representations into one iris_combined <- simplicial_set_intersect(iris_sg12, iris_sg34)  # Optimize the layout based on the combined view iris_combined_umap <- optimize_graph_layout(iris_combined, n_epochs = 100)"},{"path":"https://jlmelville.github.io/uwot/reference/simplicial_set_union.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge Similarity Graph by Simplicial Set Union — simplicial_set_union","title":"Merge Similarity Graph by Simplicial Set Union — simplicial_set_union","text":"Combine two similarity graphs treating fuzzy topological sets forming union.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/simplicial_set_union.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge Similarity Graph by Simplicial Set Union — simplicial_set_union","text":"","code":"simplicial_set_union(x, y, n_threads = NULL, verbose = FALSE)"},{"path":"https://jlmelville.github.io/uwot/reference/simplicial_set_union.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge Similarity Graph by Simplicial Set Union — simplicial_set_union","text":"x sparse matrix representing first similarity graph union operation. y sparse matrix representing second similarity graph union operation. n_threads Number threads use resetting local metric. Default half number concurrent threads supported system. verbose TRUE, log progress console.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/simplicial_set_union.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge Similarity Graph by Simplicial Set Union — simplicial_set_union","text":"sparse matrix containing union x y.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/simplicial_set_union.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merge Similarity Graph by Simplicial Set Union — simplicial_set_union","text":"","code":"# Form two different \"views\" of the same data iris30 <- iris[c(1:10, 51:60, 101:110), ] iris_sg12 <- similarity_graph(iris30[, 1:2], n_neighbors = 5) iris_sg34 <- similarity_graph(iris30[, 3:4], n_neighbors = 5)  # Combine the two representations into one iris_combined <- simplicial_set_union(iris_sg12, iris_sg34)  # Optimize the layout based on the combined view iris_combined_umap <- optimize_graph_layout(iris_combined, n_epochs = 100)"},{"path":"https://jlmelville.github.io/uwot/reference/tumap.html","id":null,"dir":"Reference","previous_headings":"","what":"Dimensionality Reduction Using t-Distributed UMAP (t-UMAP) — tumap","title":"Dimensionality Reduction Using t-Distributed UMAP (t-UMAP) — tumap","text":"faster (less flexible) version UMAP gradient. detail UMAP, see  umap function.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/tumap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dimensionality Reduction Using t-Distributed UMAP (t-UMAP) — tumap","text":"","code":"tumap(   X,   n_neighbors = 15,   n_components = 2,   metric = \"euclidean\",   n_epochs = NULL,   learning_rate = 1,   scale = FALSE,   init = \"spectral\",   init_sdev = NULL,   set_op_mix_ratio = 1,   local_connectivity = 1,   bandwidth = 1,   repulsion_strength = 1,   negative_sample_rate = 5,   nn_method = NULL,   n_trees = 50,   search_k = 2 * n_neighbors * n_trees,   n_threads = NULL,   n_sgd_threads = 0,   grain_size = 1,   y = NULL,   target_n_neighbors = n_neighbors,   target_metric = \"euclidean\",   target_weight = 0.5,   pca = NULL,   pca_center = TRUE,   pcg_rand = TRUE,   fast_sgd = FALSE,   ret_model = FALSE,   ret_nn = FALSE,   ret_extra = c(),   tmpdir = tempdir(),   verbose = getOption(\"verbose\", TRUE),   batch = FALSE,   opt_args = NULL,   epoch_callback = NULL,   pca_method = NULL,   binary_edge_weights = FALSE,   seed = NULL )"},{"path":"https://jlmelville.github.io/uwot/reference/tumap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dimensionality Reduction Using t-Distributed UMAP (t-UMAP) — tumap","text":"X Input data. Can data.frame, matrix, dist object sparseMatrix. Matrix data frames contain one observation per row. Data frames non-numeric columns removed, although factor columns used explicitly included via metric (see help metric details). sparse matrix interpreted distance matrix, assumed symmetric, can also pass explicitly upper lower triangular sparse matrix save storage. must least n_neighbors non-zero distances row. implicit explicit zero entries ignored. Set zero distances want keep arbitrarily small non-zero value (e.g. 1e-10). X can also NULL pre-computed nearest neighbor data passed nn_method, init \"spca\" \"pca\". n_neighbors size local neighborhood (terms number neighboring sample points) used manifold approximation. Larger values result global views manifold, smaller values result local data preserved. general values range 2 100. n_components dimension space embed . defaults 2 provide easy visualization, can reasonably set integer value range 2 100. metric Type distance metric use find nearest neighbors. One   : \"euclidean\" (default) \"cosine\" \"manhattan\" \"hamming\" \"correlation\" (distance based Pearson correlation) \"categorical\" (see ) applies nn_method = \"annoy\" (nn_method = \"fnn\", distance metric always \"euclidean\"). X data frame matrix, multiple metrics can specified, passing list argument, name item list one metric names . value list item vector giving names integer ids columns included calculation, e.g. metric = list(euclidean = 1:4, manhattan = 5:10). metric calculation results separate fuzzy simplicial set, intersected together produce final set. Metric names can repeated. non-numeric columns removed data frame, safer use column names integer ids. Factor columns can also used specifying metric name \"categorical\". Factor columns treated different numeric columns although multiple factor columns can specified vector, factor column specified processed individually. specify non-factor column, coerced factor. given data block, may override pca pca_center arguments block, providing list one unnamed item containing column names ids, pca pca_center overrides named items, e.g. metric = list(euclidean = 1:4, manhattan = list(5:10, pca_center = FALSE)). exists allow mixed binary real-valued data included PCA applied , centering applied real-valued data (typical apply centering binary data PCA applied). n_epochs Number epochs use optimization embedded coordinates. default, value set 500 datasets containing 10,000 vertices less, 200 otherwise. n_epochs = 0, coordinates determined \"init\" returned. learning_rate Initial learning rate used optimization coordinates. scale Scaling apply X data frame matrix: \"none\" FALSE NULL scaling. \"Z\" \"scale\" TRUE Scale column   zero mean variance 1. \"maxabs\" Center column mean 0, divide   element maximum absolute value entire matrix. \"range\" Range scale entire matrix, smallest   element 0 largest 1. \"colrange\" Scale column range (0,1). t-UMAP, default \"none\". init Type initialization coordinates. Options : \"spectral\" Spectral embedding using normalized Laplacian    fuzzy 1-skeleton, Gaussian noise added. \"normlaplacian\". Spectral embedding using normalized    Laplacian fuzzy 1-skeleton, without noise. \"random\". Coordinates assigned using uniform random    distribution -10 10. \"lvrandom\". Coordinates assigned using Gaussian    distribution standard deviation 1e-4, used LargeVis    (Tang et al., 2016) t-SNE. \"laplacian\". Spectral embedding using Laplacian Eigenmap    (Belkin Niyogi, 2002). \"pca\". first two principal components PCA    X X data frame, 2-dimensional classical    MDS X class \"dist\". \"spca\". Like \"pca\", dimension scaled    standard deviation 1e-4, give distribution similar    used t-SNE. alias init = \"pca\", init_sdev =    1e-4. \"agspectral\" \"approximate global\" modification    \"spectral\" edges graph value 1,    sets random number edges (negative_sample_rate edges per    vertex) 0.1, approximate effect non-local affinities. matrix initial coordinates. spectral initializations, (\"spectral\", \"normlaplacian\", \"laplacian\", \"agspectral\"), one connected component identified, spectral initialization attempted. Instead PCA-based initialization attempted. verbose = TRUE number connected components logged console. existence multiple connected components implies global view data attained initialization. Increasing value n_neighbors may help. init_sdev non-NULL, scales dimension initialized coordinates (including user-supplied matrix) standard deviation. default scaling carried , except init = \"spca\", case value 0.0001. Scaling input may help unscaled versions result initial coordinates large inter-point distances outliers. usually results small gradients optimization little progress made layout. Shrinking initial embedding rescaling can help circumstances. Scaling result init = \"pca\" usually recommended init = \"spca\" alias init = \"pca\", init_sdev = 1e-4 spectral initializations scaled versions usually necessary unless using large value n_neighbors (e.g. n_neighbors = 150 higher). compatibility recent versions Python UMAP package, using init = \"spectral\", also set init_sdev = \"range\", range scale columns containing initial data 0-10. set default maintain backwards compatibility previous versions uwot. set_op_mix_ratio Interpolate (fuzzy) union intersection set operation used combine local fuzzy simplicial sets obtain global fuzzy simplicial sets. fuzzy set operations use product t-norm. value parameter 0.0 1.0; value 1.0 use pure fuzzy union, 0.0 use pure fuzzy intersection. local_connectivity local connectivity required -- .e. number nearest neighbors assumed connected local level. higher value connected manifold becomes locally. practice local intrinsic dimension manifold. bandwidth effective bandwidth kernel view algorithm similar Laplacian Eigenmaps. Larger values induce connectivity global view data, smaller values concentrate locally. repulsion_strength Weighting applied negative samples low dimensional embedding optimization. Values higher one result greater weight given negative samples. negative_sample_rate number negative edge/1-simplex samples use per positive edge/1-simplex sample optimizing low dimensional embedding. nn_method Method finding nearest neighbors. Options : \"fnn\". Use exact nearest neighbors via     FNN package. \"annoy\" Use approximate nearest neighbors via     RcppAnnoy package. default, X less 4,096 vertices, exact nearest neighbors found. Otherwise, approximate nearest neighbors used. may also pass pre-calculated nearest neighbor data argument. must one two formats, either list consisting two elements: \"idx\". n_vertices x n_neighbors matrix   containing integer indexes nearest neighbors X.   vertex considered nearest neighbor, .e.   idx[, 1] == 1:n_vertices. \"dist\". n_vertices x n_neighbors matrix   containing distances nearest neighbors. sparse distance matrix type dgCMatrix, dimensions n_vertices x n_vertices. Distances arranged column, .e. non-zero entry row j ith column indicates jth observation X nearest neighbor ith observation distance given value element. n_neighbors parameter ignored using precomputed nearest neighbor data. using sparse distance matrix input, column can contain different number neighbors. n_trees Number trees build constructing nearest neighbor index. trees specified, larger index, better results. search_k, determines accuracy Annoy nearest neighbor search. used nn_method \"annoy\". Sensible values 10 100. search_k Number nodes search neighbor retrieval. larger k, accurate results, longer search takes. n_trees, determines accuracy Annoy nearest neighbor search. used nn_method \"annoy\". n_threads Number threads use (except stochastic gradient descent). Default half number concurrent threads supported system. nearest neighbor search, applies nn_method = \"annoy\". n_threads > 1, Annoy index temporarily written disk location determined tempfile. n_sgd_threads Number threads use stochastic gradient descent. set > 1, aware batch = FALSE, results reproducible, even set.seed called fixed seed running. Set \"auto\" use value n_threads. grain_size minimum amount work thread. value set high enough, less n_threads n_sgd_threads used processing, might give performance improvement overhead thread management context switching outweighing improvement due concurrent processing. left default (1) work spread evenly threads specified. y Optional target data supervised dimension reduction. Can vector, matrix data frame. Use target_metric parameter specify metrics use, using syntax metric. Usually either single numeric factor column used, complex formats possible. following types allowed: Factor columns length X. NA     allowed observation unknown level, case     UMAP operates form semi-supervised learning. column     treated separately. Numeric data. NA allowed case. Use     parameter target_n_neighbors set number neighbors used     y. unset, n_neighbors used. Unlike factors,     numeric columns grouped one block unless target_metric     specifies otherwise. example, wish columns     b treated separately, specify     target_metric = list(euclidean = \"\", euclidean = \"b\"). Otherwise,     data effectively treated matrix two columns. Nearest neighbor data, consisting list two matrices,     idx dist. represent precalculated nearest     neighbor indices distances, respectively.     format expected precalculated data     nn_method. format assumes underlying data     numeric vector. user-supplied value target_n_neighbors     parameter ignored case, number columns     matrices used value. Multiple nearest neighbor data using     different metrics can supplied passing list lists. Unlike X, factor columns included y automatically used. target_n_neighbors Number nearest neighbors use construct target simplicial set. Default value n_neighbors. Applies y non-NULL numeric. target_metric metric used measure distance y using supervised dimension reduction. Used y numeric. target_weight Weighting factor data topology target topology. value 0.0 weights entirely data, value 1.0 weights entirely target. default 0.5 balances weighting equally data target. applies y non-NULL. pca set positive integer value, reduce data number columns using PCA. applied distance metric \"hamming\", dimensions data larger number specified (.e. number rows columns must larger value parameter). > 100 columns data frame matrix, reducing number columns way may substantially increase performance nearest neighbor search cost potential decrease accuracy. many t-SNE applications, value 50 recommended, although guarantee appropriate settings. pca_center TRUE, center columns X carrying PCA. binary data, recommended set FALSE. pcg_rand TRUE, use PCG random number generator (O'Neill, 2014) optimization. Otherwise, use faster (probably less statistically good) Tausworthe \"taus88\" generator. default TRUE. fast_sgd TRUE, following combination parameters set: pcg_rand = TRUE n_sgd_threads = \"auto\". default FALSE. Setting TRUE speed stochastic optimization phase, give potentially less accurate embedding, exactly reproducible even fixed seed. visualization, fast_sgd = TRUE give perfectly good results. generic dimensionality reduction, safer leave fast_sgd = FALSE. fast_sgd = TRUE, user-supplied values pcg_rand n_sgd_threads, ignored. ret_model TRUE, return extra data can used add new data existing embedding via umap_transform. embedded coordinates returned list item embedding. FALSE, just return coordinates. parameter can used conjunction ret_nn ret_extra. Note settings incompatible production UMAP model: external neighbor data (passed via list nn_method), factor columns included via metric parameter. latter case, model produced based numeric data. transformation using new data possible, factor columns new data ignored. Note setting ret_model = TRUE forces use approximate nearest neighbors method. small datasets otherwise use exact nearest neighbor calculations, setting ret_model = TRUE means different results may returned small datasets terms returned nearest neighbors (requested) final embedded coordinates, compared ret_model = FALSE, even random number seed fixed. avoid , explicitly set nn_method = \"annoy\" ret_model = FALSE case. ret_nn TRUE, addition embedding, also return nearest neighbor data can used input nn_method avoid overhead repeatedly calculating nearest neighbors manipulating unrelated parameters (e.g. min_dist, n_epochs, init). See \"Value\" section names list items. FALSE, just return coordinates. Note nearest neighbors sensitive data scaling, wary reusing nearest neighbor data modifying scale parameter. parameter can used conjunction ret_model ret_extra. ret_extra vector indicating extra data return. May contain combination following strings: \"model\" setting ret_model = TRUE. \"nn\" setting ret_nn = TRUE. \"fgraph\" high dimensional fuzzy graph (.e. fuzzy     simplicial set merged local views input data). graph     returned sparse symmetric N x N matrix class     dgCMatrix-class, non-zero entry (, j) gives     membership strength edge connecting vertex vertex j.     can considered analogous input probability (similarity     affinity) used t-SNE LargeVis. Note graph     sparsified removing edges sufficiently low membership strength     sampled probabilistic edge sampling     employed optimization therefore number non-zero elements     matrix dependent n_epochs.     interested fuzzy input graph (e.g. clustering), setting     n_epochs = 0 avoid sparsifying. aware     setting binary_edge_weights = TRUE affect graph (    non-zero edge weights 1). \"sigma\" normalization value observation     dataset constructing smoothed distances     neighbors. gives sense local density     observation high dimensional space: higher values     sigma indicate higher dispersion lower density. tmpdir Temporary directory store nearest neighbor indexes nearest neighbor search. Default tempdir. index written disk n_threads > 1 nn_method = \"annoy\"; otherwise, parameter ignored. verbose TRUE, log details console. batch TRUE, embedding coordinates updated end epoch rather epoch. batch mode, results reproducible fixed random seed even n_sgd_threads > 1, cost slightly higher memory use. may also modify learning_rate increase n_epochs, whether provides speed increase single-threaded optimization likely dataset hardware-dependent. opt_args list optimizer parameters, used batch = TRUE. default optimization method used Adam (Kingma Ba, 2014). method optimization method use. Either \"adam\"   \"sgd\" (stochastic gradient descent). Default: \"adam\". beta1 (Adam ). weighting parameter   exponential moving average first moment estimator. Effectively   momentum parameter. floating point value 0 1.   Higher values can smooth oscillatory updates poorly-conditioned   situations may allow larger learning_rate   specified, high can cause divergence. Default: 0.5. beta2 (Adam ). weighting parameter   exponential moving average uncentered second moment estimator.   floating point value 0 1. Controls degree   adaptivity step-size. Higher values put weight previous   time steps. Default: 0.9. eps (Adam ). Intended small value prevent   division zero, practice can also affect convergence due   interaction beta2. Higher values reduce effect   step-size adaptivity bring behavior closer stochastic gradient   descent momentum. Typical values 1e-8 1e-3. Default:   1e-7. alpha initial learning rate. Default: value   learning_rate parameter. epoch_callback function invoked end every epoch. signature : (epoch, n_epochs, coords), : epoch current epoch number (1   n_epochs). n_epochs Number epochs use optimization   embedded coordinates. coords embedded coordinates end current   epoch, matrix dimensions (N, n_components). pca_method Method carry PCA dimensionality reduction pca parameter specified. Allowed values : \"irlba\". Uses prcomp_irlba   irlba package. \"rsvd\". Uses 5 iterations svdr   irlba package.   likely give much faster potentially less accurate results   using \"irlba\". purposes nearest neighbor   calculation coordinates initialization, loss accuracy   seem matter much. \"bigstatsr\". Uses big_randomSVD   bigstatsr   package. SVD methods used bigstatsr may faster   systems without access efficient linear algebra libraries (e.g.   Windows). Note: bigstatsr dependency   uwot: choose use package PCA, must install   . \"svd\". Uses svd SVD.   likely slow smallest datasets. \"auto\" (default). Uses \"irlba\", unless   50   case \"svd\" used. binary_edge_weights TRUE edge weights input graph treated binary (0/1) rather real valued. affects sampling frequency neighbors strategy used PaCMAP method (Wang co-workers, 2020). Practical (Böhm co-workers, 2020) theoretical (Damrich Hamprecht, 2021) work suggests little effect UMAP's performance. seed Integer seed use initialize random number generator state. Combined n_sgd_threads = 1 batch = TRUE, give consistent output across multiple runs given installation. Setting value equivalent calling set.seed, may convenient situations call separate function. default set seed. ret_model = TRUE, seed stored output model used set seed inside umap_transform.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/tumap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Dimensionality Reduction Using t-Distributed UMAP (t-UMAP) — tumap","text":"matrix optimized coordinates, :  ret_model = TRUE (ret_extra contains     \"model\"), returns list containing extra information can     used add new data existing embedding via     umap_transform. case, coordinates available     list item embedding. NOTE: contents     model list considered stable part     public API, purposely left undocumented. ret_nn = TRUE (ret_extra contains \"nn\"),     returns nearest neighbor data list called nn.     contains one list metric calculated, containing     matrix idx integer ids neighbors; matrix     dist distances. nn list (sub-list) can     used input nn_method parameter. ret_extra contains \"fgraph\" returns high     dimensional fuzzy graph sparse matrix called fgraph, type     dgCMatrix-class. ret_extra contains \"sigma\", returns vector     smooth knn distance normalization terms observation     \"sigma\" vector \"rho\" containing largest     distance locally connected neighbors observation. ret_extra contains \"localr\", returns vector     estimated local radii, sum \"sigma\" \"rho\". returned list contains combined data combination   specifying ret_model, ret_nn ret_extra.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/tumap.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Dimensionality Reduction Using t-Distributed UMAP (t-UMAP) — tumap","text":"setting UMAP curve parameters b 1, get back Cauchy distribution used t-SNE LargeVis. also results substantially simplified gradient expression. can give speed improvement around 50%.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/tumap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Dimensionality Reduction Using t-Distributed UMAP (t-UMAP) — tumap","text":"","code":"iris_tumap <- tumap(iris, n_neighbors = 50, learning_rate = 0.5)"},{"path":"https://jlmelville.github.io/uwot/reference/umap.html","id":null,"dir":"Reference","previous_headings":"","what":"Dimensionality Reduction with UMAP — umap","title":"Dimensionality Reduction with UMAP — umap","text":"Carry dimensionality reduction dataset using Uniform Manifold Approximation Projection (UMAP) method (McInnes & Healy, 2018). following help text lifted verbatim Python reference implementation https://github.com/lmcinnes/umap.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/umap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dimensionality Reduction with UMAP — umap","text":"","code":"umap(   X,   n_neighbors = 15,   n_components = 2,   metric = \"euclidean\",   n_epochs = NULL,   learning_rate = 1,   scale = FALSE,   init = \"spectral\",   init_sdev = NULL,   spread = 1,   min_dist = 0.01,   set_op_mix_ratio = 1,   local_connectivity = 1,   bandwidth = 1,   repulsion_strength = 1,   negative_sample_rate = 5,   a = NULL,   b = NULL,   nn_method = NULL,   n_trees = 50,   search_k = 2 * n_neighbors * n_trees,   approx_pow = FALSE,   y = NULL,   target_n_neighbors = n_neighbors,   target_metric = \"euclidean\",   target_weight = 0.5,   pca = NULL,   pca_center = TRUE,   pcg_rand = TRUE,   fast_sgd = FALSE,   ret_model = FALSE,   ret_nn = FALSE,   ret_extra = c(),   n_threads = NULL,   n_sgd_threads = 0,   grain_size = 1,   tmpdir = tempdir(),   verbose = getOption(\"verbose\", TRUE),   batch = FALSE,   opt_args = NULL,   epoch_callback = NULL,   pca_method = NULL,   binary_edge_weights = FALSE,   dens_scale = NULL,   seed = NULL )"},{"path":"https://jlmelville.github.io/uwot/reference/umap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dimensionality Reduction with UMAP — umap","text":"X Input data. Can data.frame, matrix, dist object sparseMatrix. Matrix data frames contain one observation per row. Data frames non-numeric columns removed, although factor columns used explicitly included via metric (see help metric details). sparse matrix interpreted distance matrix, assumed symmetric, can also pass explicitly upper lower triangular sparse matrix save storage. must least n_neighbors non-zero distances row. implicit explicit zero entries ignored. Set zero distances want keep arbitrarily small non-zero value (e.g. 1e-10). X can also NULL pre-computed nearest neighbor data passed nn_method, init \"spca\" \"pca\". n_neighbors size local neighborhood (terms number neighboring sample points) used manifold approximation. Larger values result global views manifold, smaller values result local data preserved. general values range 2 100. n_components dimension space embed . defaults 2 provide easy visualization, can reasonably set integer value range 2 100. metric Type distance metric use find nearest neighbors. One   : \"euclidean\" (default) \"cosine\" \"manhattan\" \"hamming\" \"correlation\" (distance based Pearson correlation) \"categorical\" (see ) applies nn_method = \"annoy\" (nn_method = \"fnn\", distance metric always \"euclidean\"). X data frame matrix, multiple metrics can specified, passing list argument, name item list one metric names . value list item vector giving names integer ids columns included calculation, e.g. metric = list(euclidean = 1:4, manhattan = 5:10). metric calculation results separate fuzzy simplicial set, intersected together produce final set. Metric names can repeated. non-numeric columns removed data frame, safer use column names integer ids. Factor columns can also used specifying metric name \"categorical\". Factor columns treated different numeric columns although multiple factor columns can specified vector, factor column specified processed individually. specify non-factor column, coerced factor. given data block, may override pca pca_center arguments block, providing list one unnamed item containing column names ids, pca pca_center overrides named items, e.g. metric = list(euclidean = 1:4, manhattan = list(5:10, pca_center = FALSE)). exists allow mixed binary real-valued data included PCA applied , centering applied real-valued data (typical apply centering binary data PCA applied). n_epochs Number epochs use optimization embedded coordinates. default, value set 500 datasets containing 10,000 vertices less, 200 otherwise. n_epochs = 0, coordinates determined \"init\" returned. learning_rate Initial learning rate used optimization coordinates. scale Scaling apply X data frame matrix: \"none\" FALSE NULL scaling. \"Z\" \"scale\" TRUE Scale column   zero mean variance 1. \"maxabs\" Center column mean 0, divide   element maximum absolute value entire matrix. \"range\" Range scale entire matrix, smallest   element 0 largest 1. \"colrange\" Scale column range (0,1). UMAP, default \"none\". init Type initialization coordinates. Options : \"spectral\" Spectral embedding using normalized Laplacian    fuzzy 1-skeleton, Gaussian noise added. \"normlaplacian\". Spectral embedding using normalized    Laplacian fuzzy 1-skeleton, without noise. \"random\". Coordinates assigned using uniform random    distribution -10 10. \"lvrandom\". Coordinates assigned using Gaussian    distribution standard deviation 1e-4, used LargeVis    (Tang et al., 2016) t-SNE. \"laplacian\". Spectral embedding using Laplacian Eigenmap    (Belkin Niyogi, 2002). \"pca\". first two principal components PCA    X X data frame, 2-dimensional classical    MDS X class \"dist\". \"spca\". Like \"pca\", dimension scaled    standard deviation 1e-4, give distribution similar    used t-SNE. alias init = \"pca\", init_sdev =    1e-4. \"agspectral\" \"approximate global\" modification    \"spectral\" edges graph value 1,    sets random number edges (negative_sample_rate edges per    vertex) 0.1, approximate effect non-local affinities. matrix initial coordinates. spectral initializations, (\"spectral\", \"normlaplacian\", \"laplacian\", \"agspectral\"), one connected component identified, spectral initialization attempted. Instead PCA-based initialization attempted. verbose = TRUE number connected components logged console. existence multiple connected components implies global view data attained initialization. Increasing value n_neighbors may help. init_sdev non-NULL, scales dimension initialized coordinates (including user-supplied matrix) standard deviation. default scaling carried , except init = \"spca\", case value 0.0001. Scaling input may help unscaled versions result initial coordinates large inter-point distances outliers. usually results small gradients optimization little progress made layout. Shrinking initial embedding rescaling can help circumstances. Scaling result init = \"pca\" usually recommended init = \"spca\" alias init = \"pca\", init_sdev = 1e-4 spectral initializations scaled versions usually necessary unless using large value n_neighbors (e.g. n_neighbors = 150 higher). compatibility recent versions Python UMAP package, using init = \"spectral\", also set init_sdev = \"range\", range scale columns containing initial data 0-10. set default maintain backwards compatibility previous versions uwot. spread effective scale embedded points. combination min_dist, determines clustered/clumped embedded points . min_dist effective minimum distance embedded points. Smaller values result clustered/clumped embedding nearby points manifold drawn closer together, larger values result even dispersal points. value set relative spread value, determines scale embedded points spread . set_op_mix_ratio Interpolate (fuzzy) union intersection set operation used combine local fuzzy simplicial sets obtain global fuzzy simplicial sets. fuzzy set operations use product t-norm. value parameter 0.0 1.0; value 1.0 use pure fuzzy union, 0.0 use pure fuzzy intersection. local_connectivity local connectivity required -- .e. number nearest neighbors assumed connected local level. higher value connected manifold becomes locally. practice local intrinsic dimension manifold. bandwidth effective bandwidth kernel view algorithm similar Laplacian Eigenmaps. Larger values induce connectivity global view data, smaller values concentrate locally. repulsion_strength Weighting applied negative samples low dimensional embedding optimization. Values higher one result greater weight given negative samples. negative_sample_rate number negative edge/1-simplex samples use per positive edge/1-simplex sample optimizing low dimensional embedding. specific parameters controlling embedding. NULL values set automatically determined min_dist spread. b specific parameters controlling embedding. NULL values set automatically determined min_dist spread. nn_method Method finding nearest neighbors. Options : \"fnn\". Use exact nearest neighbors via     FNN package. \"annoy\" Use approximate nearest neighbors via     RcppAnnoy package. default, X less 4,096 vertices, exact nearest neighbors found. Otherwise, approximate nearest neighbors used. may also pass pre-calculated nearest neighbor data argument. must one two formats, either list consisting two elements: \"idx\". n_vertices x n_neighbors matrix   containing integer indexes nearest neighbors X.   vertex considered nearest neighbor, .e.   idx[, 1] == 1:n_vertices. \"dist\". n_vertices x n_neighbors matrix   containing distances nearest neighbors. sparse distance matrix type dgCMatrix, dimensions n_vertices x n_vertices. Distances arranged column, .e. non-zero entry row j ith column indicates jth observation X nearest neighbor ith observation distance given value element. n_neighbors parameter ignored using precomputed nearest neighbor data. using sparse distance matrix input, column can contain different number neighbors. n_trees Number trees build constructing nearest neighbor index. trees specified, larger index, better results. search_k, determines accuracy Annoy nearest neighbor search. used nn_method \"annoy\". Sensible values 10 100. search_k Number nodes search neighbor retrieval. larger k, accurate results, longer search takes. n_trees, determines accuracy Annoy nearest neighbor search. used nn_method \"annoy\". approx_pow TRUE, use approximation power function UMAP gradient, https://martin.ankerl.com/2012/01/25/optimized-approximative-pow--c--cpp/. Ignored dens_scale non-NULL. y Optional target data supervised dimension reduction. Can vector, matrix data frame. Use target_metric parameter specify metrics use, using syntax metric. Usually either single numeric factor column used, complex formats possible. following types allowed: Factor columns length X. NA     allowed observation unknown level, case     UMAP operates form semi-supervised learning. column     treated separately. Numeric data. NA allowed case. Use     parameter target_n_neighbors set number neighbors used     y. unset, n_neighbors used. Unlike factors,     numeric columns grouped one block unless target_metric     specifies otherwise. example, wish columns     b treated separately, specify     target_metric = list(euclidean = \"\", euclidean = \"b\"). Otherwise,     data effectively treated matrix two columns. Nearest neighbor data, consisting list two matrices,     idx dist. represent precalculated nearest     neighbor indices distances, respectively.     format expected precalculated data     nn_method. format assumes underlying data     numeric vector. user-supplied value target_n_neighbors     parameter ignored case, number columns     matrices used value. Multiple nearest neighbor data using     different metrics can supplied passing list lists. Unlike X, factor columns included y automatically used. target_n_neighbors Number nearest neighbors use construct target simplicial set. Default value n_neighbors. Applies y non-NULL numeric. target_metric metric used measure distance y using supervised dimension reduction. Used y numeric. target_weight Weighting factor data topology target topology. value 0.0 weights entirely data, value 1.0 weights entirely target. default 0.5 balances weighting equally data target. applies y non-NULL. pca set positive integer value, reduce data number columns using PCA. applied distance metric \"hamming\", dimensions data larger number specified (.e. number rows columns must larger value parameter). > 100 columns data frame matrix, reducing number columns way may substantially increase performance nearest neighbor search cost potential decrease accuracy. many t-SNE applications, value 50 recommended, although guarantee appropriate settings. pca_center TRUE, center columns X carrying PCA. binary data, recommended set FALSE. pcg_rand TRUE, use PCG random number generator (O'Neill, 2014) optimization. Otherwise, use faster (probably less statistically good) Tausworthe \"taus88\" generator. default TRUE. fast_sgd TRUE, following combination parameters set: pcg_rand = TRUE, n_sgd_threads = \"auto\" approx_pow = TRUE. default FALSE. Setting TRUE speed stochastic optimization phase, give potentially less accurate embedding, exactly reproducible even fixed seed. visualization, fast_sgd = TRUE give perfectly good results. generic dimensionality reduction, safer leave fast_sgd = FALSE. fast_sgd = TRUE, user-supplied values pcg_rand, n_sgd_threads, approx_pow ignored. ret_model TRUE, return extra data can used add new data existing embedding via umap_transform. embedded coordinates returned list item embedding. FALSE, just return coordinates. parameter can used conjunction ret_nn ret_extra. Note settings incompatible production UMAP model: external neighbor data (passed via list nn_method), factor columns included via metric parameter. latter case, model produced based numeric data. transformation using new data possible, factor columns new data ignored. Note setting ret_model = TRUE forces use approximate nearest neighbors method. small datasets otherwise use exact nearest neighbor calculations, setting ret_model = TRUE means different results may returned small datasets terms returned nearest neighbors (requested) final embedded coordinates, compared ret_model = FALSE, even random number seed fixed. avoid , explicitly set nn_method = \"annoy\" ret_model = FALSE case. ret_nn TRUE, addition embedding, also return nearest neighbor data can used input nn_method avoid overhead repeatedly calculating nearest neighbors manipulating unrelated parameters (e.g. min_dist, n_epochs, init). See \"Value\" section names list items. FALSE, just return coordinates. Note nearest neighbors sensitive data scaling, wary reusing nearest neighbor data modifying scale parameter. parameter can used conjunction ret_model ret_extra. ret_extra vector indicating extra data return. May contain combination following strings: \"model\" setting ret_model = TRUE. \"nn\" setting ret_nn = TRUE. \"fgraph\" high dimensional fuzzy graph (.e. fuzzy     simplicial set merged local views input data). graph     returned sparse symmetric N x N matrix class     dgCMatrix-class, non-zero entry (, j) gives     membership strength edge connecting vertex vertex j.     can considered analogous input probability (similarity     affinity) used t-SNE LargeVis. Note graph     sparsified removing edges sufficiently low membership strength     sampled probabilistic edge sampling     employed optimization therefore number non-zero elements     matrix dependent n_epochs.     interested fuzzy input graph (e.g. clustering), setting     n_epochs = 0 avoid sparsifying.     aware setting `binary_edge_weights = TRUE` affect     graph (non-zero edge weights 1). \"sigma\" normalization value observation     dataset constructing smoothed distances     neighbors. gives sense local density     observation high dimensional space: higher values     sigma indicate higher dispersion lower density. n_threads Number threads use (except stochastic gradient descent). Default half number concurrent threads supported system. nearest neighbor search, applies nn_method = \"annoy\". n_threads > 1, Annoy index temporarily written disk location determined tempfile. n_sgd_threads Number threads use stochastic gradient descent. set > 1, aware batch = FALSE, results reproducible, even set.seed called fixed seed running. Set \"auto\" use value n_threads. grain_size minimum amount work thread. value set high enough, less n_threads n_sgd_threads used processing, might give performance improvement overhead thread management context switching outweighing improvement due concurrent processing. left default (1) work spread evenly threads specified. tmpdir Temporary directory store nearest neighbor indexes nearest neighbor search. Default tempdir. index written disk n_threads > 1 nn_method = \"annoy\"; otherwise, parameter ignored. verbose TRUE, log details console. batch TRUE, embedding coordinates updated end epoch rather epoch. batch mode, results reproducible fixed random seed even n_sgd_threads > 1, cost slightly higher memory use. may also modify learning_rate increase n_epochs, whether provides speed increase single-threaded optimization likely dataset hardware-dependent. opt_args list optimizer parameters, used batch = TRUE. default optimization method used Adam (Kingma Ba, 2014). method optimization method use. Either \"adam\"   \"sgd\" (stochastic gradient descent). Default: \"adam\". beta1 (Adam ). weighting parameter   exponential moving average first moment estimator. Effectively   momentum parameter. floating point value 0 1.   Higher values can smooth oscillatory updates poorly-conditioned   situations may allow larger learning_rate   specified, high can cause divergence. Default: 0.5. beta2 (Adam ). weighting parameter   exponential moving average uncentered second moment estimator.   floating point value 0 1. Controls degree   adaptivity step-size. Higher values put weight previous   time steps. Default: 0.9. eps (Adam ). Intended small value prevent   division zero, practice can also affect convergence due   interaction beta2. Higher values reduce effect   step-size adaptivity bring behavior closer stochastic gradient   descent momentum. Typical values 1e-8 1e-3. Default:   1e-7. alpha initial learning rate. Default: value   learning_rate parameter. epoch_callback function invoked end every epoch. signature : (epoch, n_epochs, coords), : epoch current epoch number (1   n_epochs). n_epochs Number epochs use optimization   embedded coordinates. coords embedded coordinates end current   epoch, matrix dimensions (N, n_components). pca_method Method carry PCA dimensionality reduction pca parameter specified. Allowed values : \"irlba\". Uses prcomp_irlba   irlba package. \"rsvd\". Uses 5 iterations svdr   irlba package.   likely give much faster potentially less accurate results   using \"irlba\". purposes nearest neighbor   calculation coordinates initialization, loss accuracy   seem matter much. \"bigstatsr\". Uses big_randomSVD   bigstatsr   package. SVD methods used bigstatsr may faster   systems without access efficient linear algebra libraries (e.g.   Windows). Note: bigstatsr dependency   uwot: choose use package PCA, must install   . \"svd\". Uses svd SVD.   likely slow smallest datasets. \"auto\" (default). Uses \"irlba\", unless   50   case \"svd\" used. binary_edge_weights TRUE edge weights input graph treated binary (0/1) rather real valued. affects sampling frequency neighbors strategy used PaCMAP method (Wang co-workers, 2020). Practical (Böhm co-workers, 2020) theoretical (Damrich Hamprecht, 2021) work suggests little effect UMAP's performance. dens_scale value 0 1. > 0 output attempts preserve relative local density around observation. uses approximation densMAP method (Narayan co-workers, 2021). larger value dens_scale, greater range output densities used map input densities. option ignored using multiple metric blocks. seed Integer seed use initialize random number generator state. Combined n_sgd_threads = 1 batch = TRUE, give consistent output across multiple runs given installation. Setting value equivalent calling set.seed, may convenient situations call separate function. default set seed. ret_model = TRUE, seed stored output model used set seed inside umap_transform.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/umap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Dimensionality Reduction with UMAP — umap","text":"matrix optimized coordinates, :  ret_model = TRUE (ret_extra contains     \"model\"), returns list containing extra information can     used add new data existing embedding via     umap_transform. case, coordinates available     list item embedding. NOTE: contents     model list considered stable part     public API, purposely left undocumented. ret_nn = TRUE (ret_extra contains \"nn\"),     returns nearest neighbor data list called nn.     contains one list metric calculated, containing     matrix idx integer ids neighbors; matrix     dist distances. nn list (sub-list) can     used input nn_method parameter. ret_extra contains \"fgraph\", returns high     dimensional fuzzy graph sparse matrix called fgraph, type     dgCMatrix-class. ret_extra contains \"sigma\", returns vector     smooth knn distance normalization terms observation     \"sigma\" vector \"rho\" containing largest     distance locally connected neighbors observation. ret_extra contains \"localr\", returns vector     estimated local radii, sum \"sigma\" \"rho\". returned list contains combined data combination   specifying ret_model, ret_nn ret_extra.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/umap.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Dimensionality Reduction with UMAP — umap","text":"Belkin, M., & Niyogi, P. (2002). Laplacian eigenmaps spectral techniques embedding clustering. Advances neural information processing systems (pp. 585-591). http://papers.nips.cc/paper/1961-laplacian-eigenmaps--spectral-techniques--embedding--clustering.pdf Böhm, J. N., Berens, P., & Kobak, D. (2020). unifying perspective neighbor embeddings along attraction-repulsion spectrum. arXiv preprint arXiv:2007.08902. https://arxiv.org/abs/2007.08902 Damrich, S., & Hamprecht, F. . (2021). UMAP's true loss function. Advances Neural Information Processing Systems, 34. https://proceedings.neurips.cc/paper/2021/hash/2de5d16682c3c35007e4e92982f1a2ba-Abstract.html Kingma, D. P., & Ba, J. (2014). Adam: method stochastic optimization. arXiv preprint arXiv:1412.6980. https://arxiv.org/abs/1412.6980 McInnes, L., & Healy, J. (2018). UMAP: Uniform Manifold Approximation Projection Dimension Reduction arXiv preprint arXiv:1802.03426. https://arxiv.org/abs/1802.03426 Narayan, ., Berger, B., & Cho, H. (2021). Assessing single-cell transcriptomic variability density-preserving data visualization. Nature biotechnology, 39(6), 765-774. doi:10.1038/s41587-020-00801-7 O’Neill, M. E. (2014). PCG: family simple fast space-efficient statistically good algorithms random number generation (Report . HMC-CS-2014-0905). Harvey Mudd College. Tang, J., Liu, J., Zhang, M., & Mei, Q. (2016, April). Visualizing large-scale high-dimensional data. Proceedings 25th International Conference World Wide Web (pp. 287-297). International World Wide Web Conferences Steering Committee. https://arxiv.org/abs/1602.00370 Van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-SNE. Journal Machine Learning Research, 9 (2579-2605). https://www.jmlr.org/papers/v9/vandermaaten08a.html Wang, Y., Huang, H., Rudin, C., & Shaposhnik, Y. (2021). Understanding Dimension Reduction Tools Work: Empirical Approach Deciphering t-SNE, UMAP, TriMap, PaCMAP Data Visualization. Journal Machine Learning Research, 22(201), 1-73. https://www.jmlr.org/papers/v22/20-1061.html","code":""},{"path":"https://jlmelville.github.io/uwot/reference/umap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Dimensionality Reduction with UMAP — umap","text":"","code":"iris30 <- iris[c(1:10, 51:60, 101:110), ]  # Non-numeric columns are automatically removed so you can pass data frames # directly in a lot of cases without pre-processing iris_umap <- umap(iris30, n_neighbors = 5, learning_rate = 0.5, init = \"random\", n_epochs = 20)  # Faster approximation to the gradient and return nearest neighbors iris_umap <- umap(iris30, n_neighbors = 5, approx_pow = TRUE, ret_nn = TRUE, n_epochs = 20)  # Can specify min_dist and spread parameters to control separation and size # of clusters and reuse nearest neighbors for efficiency nn <- iris_umap$nn iris_umap <- umap(iris30, n_neighbors = 5, min_dist = 1, spread = 5, nn_method = nn, n_epochs = 20)  # Supervised dimension reduction using the 'Species' factor column iris_sumap <- umap(iris30,   n_neighbors = 5, min_dist = 0.001, y = iris30$Species,   target_weight = 0.5, n_epochs = 20 )  # Calculate Petal and Sepal neighbors separately (uses intersection of the resulting sets): iris_umap <- umap(iris30, metric = list(   \"euclidean\" = c(\"Sepal.Length\", \"Sepal.Width\"),   \"euclidean\" = c(\"Petal.Length\", \"Petal.Width\") ))"},{"path":"https://jlmelville.github.io/uwot/reference/umap_transform.html","id":null,"dir":"Reference","previous_headings":"","what":"Add New Points to an Existing Embedding — umap_transform","title":"Add New Points to an Existing Embedding — umap_transform","text":"Carry embedding new data using existing embedding. Requires using result calling umap tumap ret_model = TRUE.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/umap_transform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add New Points to an Existing Embedding — umap_transform","text":"","code":"umap_transform(   X = NULL,   model = NULL,   nn_method = NULL,   init_weighted = TRUE,   search_k = NULL,   tmpdir = tempdir(),   n_epochs = NULL,   n_threads = NULL,   n_sgd_threads = 0,   grain_size = 1,   verbose = FALSE,   init = \"weighted\",   batch = NULL,   learning_rate = NULL,   opt_args = NULL,   epoch_callback = NULL,   ret_extra = NULL,   seed = NULL )"},{"path":"https://jlmelville.github.io/uwot/reference/umap_transform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add New Points to an Existing Embedding — umap_transform","text":"X new data transformed, either matrix data frame. Must columns order input data used generate model. model Data associated existing embedding. nn_method Optional pre-calculated nearest neighbor data. two supported formats. first list consisting two elements: \"idx\". n_vertices x n_neighbors matrix   n_vertices number observations X. contents   matrix integer indexes data used generate   model, n_neighbors-nearest neighbors   data transformed. \"dist\". n_vertices x n_neighbors matrix   containing distances nearest neighbors. second supported format sparse distance matrix type dgCMatrix, dimensions n_model_vertices x n_vertices. n_model_vertices number observations original data generated model. Distances arranged column, .e. non-zero entry row j ith column indicates jth observation original data used generate model nearest neighbor ith observation new data, distance given value element. format, different number neighbors allowed observation, .e. column can contain different number non-zero values. Multiple nearest neighbor data (e.g. two different pre-calculated metrics) can passed passing list containing nearest neighbor data lists items. init_weighted TRUE, initialize embedded coordinates X using weighted average coordinates nearest neighbors original embedding model, weights used edge weights UMAP smoothed knn distances. Otherwise, use un-weighted average. parameter deprecated removed version 1.0 package. Use init parameter replacement, replacing init_weighted = TRUE init = \"weighted\" init_weighted = FALSE init = \"average\". search_k Number nodes search neighbor retrieval. larger k, accurate results, longer search takes. Default value used building model used. tmpdir Temporary directory store nearest neighbor indexes nearest neighbor search. Default tempdir. index written disk n_threads > 1; otherwise, parameter ignored. n_epochs Number epochs use optimization embedded coordinates. value 30 - 100 reasonable trade speed thoroughness. default, value set one third number epochs used build model. n_threads Number threads use, (except stochastic gradient descent). Default half number concurrent threads supported system. n_sgd_threads Number threads use stochastic gradient descent. set > 1, aware batch = FALSE, results reproducible, even set.seed called fixed seed running. Set \"auto\" use value n_threads. grain_size Minimum batch size multithreading. number items process thread falls number, threads used. Used conjunction n_threads n_sgd_threads. verbose TRUE, log details console. init initialize transformed coordinates. One : \"weighted\" (default). Use weighted average     coordinates nearest neighbors original embedding     model, weights used edge weights UMAP     smoothed knn distances. Equivalent init_weighted = TRUE. \"average\". Use mean average coordinates     nearest neighbors original embedding model.     Equivalent init_weighted = FALSE. matrix user-specified input coordinates, must     dimensions (nrow(X), ncol(model$embedding)). parameter used preference init_weighted. batch TRUE, embedding coordinates updated end epoch rather epoch. batch mode, results reproducible fixed random seed even n_sgd_threads > 1, cost slightly higher memory use. may also modify learning_rate increase n_epochs, whether provides speed increase single-threaded optimization likely dataset hardware-dependent. NULL, transform use value provided model, available. Default: FALSE. learning_rate Initial learning rate used optimization coordinates. overrides value associated model. left unspecified circumstances. opt_args list optimizer parameters, used batch = TRUE. default optimization method used Adam (Kingma Ba, 2014). method optimization method use. Either \"adam\"   \"sgd\" (stochastic gradient descent). Default: \"adam\". beta1 (Adam ). weighting parameter   exponential moving average first moment estimator. Effectively   momentum parameter. floating point value 0 1.   Higher values can smooth oscillatory updates poorly-conditioned   situations may allow larger learning_rate   specified, high can cause divergence. Default: 0.5. beta2 (Adam ). weighting parameter   exponential moving average uncentered second moment estimator.   floating point value 0 1. Controls degree   adaptivity step-size. Higher values put weight previous   time steps. Default: 0.9. eps (Adam ). Intended small value prevent   division zero, practice can also affect convergence due   interaction beta2. Higher values reduce effect   step-size adaptivity bring behavior closer stochastic gradient   descent momentum. Typical values 1e-8 1e-3. Default:   1e-7. alpha initial learning rate. Default: value   learning_rate parameter. NULL, transform use value provided model, available. epoch_callback function invoked end every epoch. signature : (epoch, n_epochs, coords, fixed_coords), : epoch current epoch number (1   n_epochs). n_epochs Number epochs use optimization   embedded coordinates. coords embedded coordinates end current   epoch, matrix dimensions (N, n_components). fixed_coords originally embedded coordinates   model. fixed change. matrix dimensions   (Nmodel, n_components) Nmodel number   observations original data. ret_extra vector indicating extra data return. May contain combination following strings: \"fgraph\" high dimensional fuzzy graph (.e. fuzzy     simplicial set merged local views input data). graph     returned sparse matrix class dgCMatrix-class     dimensions NX x Nmodel, NX number     items data transform X, NModel     number items data used build UMAP model.     non-zero entry (, j) gives membership strength edge     connecting vertex representing ith item X     jth item data used build model. Note     graph sparsified removing edges sufficiently low     membership strength sampled probabilistic     edge sampling employed optimization therefore number     non-zero elements matrix dependent n_epochs.     interested fuzzy input graph (e.g. clustering),     setting n_epochs = 0 avoid sparsifying. seed Integer seed use initialize random number generator state. Combined n_sgd_threads = 1 batch = TRUE, give consistent output across multiple runs given installation. Setting value equivalent calling set.seed, may convenient situations call separate function. default set seed, case function uses behavior specified supplied model: model specifies seed, model seed used seed random number generator, results still consistent (n_sgd_threads = 1). want force seed set, even set model, set seed = FALSE.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/umap_transform.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add New Points to an Existing Embedding — umap_transform","text":"matrix coordinates X transformed space   model, ret_extra specified, list   containing:  embedding matrix optimized coordinates. ret_extra contains \"fgraph\", item     name containing high-dimensional fuzzy graph sparse matrix,     type dgCMatrix-class. ret_extra contains \"sigma\", returns vector     smooth knn distance normalization terms observation     \"sigma\" vector \"rho\" containing largest     distance locally connected neighbors observation. ret_extra contains \"localr\", item     name containing vector estimated local radii, sum     \"sigma\" \"rho\". ret_extra contains \"nn\", item name     containing nearest neighbors item X (respect     items created model).","code":""},{"path":"https://jlmelville.github.io/uwot/reference/umap_transform.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add New Points to an Existing Embedding — umap_transform","text":"Note settings incompatible production UMAP model via umap: external neighbor data (passed via list argument nn_method parameter), factor columns included UMAP calculation via metric parameter. latter case, model produced based numeric data. transformation possible, factor columns new data ignored.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/umap_transform.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add New Points to an Existing Embedding — umap_transform","text":"","code":"iris_train <- iris[1:100, ] iris_test <- iris[101:150, ]  # You must set ret_model = TRUE to return extra data needed iris_train_umap <- umap(iris_train, ret_model = TRUE) iris_test_umap <- umap_transform(iris_test, iris_train_umap)"},{"path":"https://jlmelville.github.io/uwot/reference/unload_uwot.html","id":null,"dir":"Reference","previous_headings":"","what":"Unload a Model — unload_uwot","title":"Unload a Model — unload_uwot","text":"Unloads UMAP model. prevents model used umap_transform, allows temporary working directory associated saving loading model removed.","code":""},{"path":"https://jlmelville.github.io/uwot/reference/unload_uwot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Unload a Model — unload_uwot","text":"","code":"unload_uwot(model, cleanup = TRUE, verbose = FALSE)"},{"path":"https://jlmelville.github.io/uwot/reference/unload_uwot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Unload a Model — unload_uwot","text":"model UMAP model create umap. cleanup TRUE, attempt delete temporary working directory used either save load model. verbose TRUE, log information console.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/reference/unload_uwot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Unload a Model — unload_uwot","text":"","code":"iris_train <- iris[c(1:10, 51:60), ] iris_test <- iris[100:110, ]  # create model model <- umap(iris_train, ret_model = TRUE, n_epochs = 20)  # save without unloading: this leaves behind a temporary working directory model_file <- tempfile(\"iris_umap\") model <- save_uwot(model, file = model_file)  # The model can continue to be used test_embedding <- umap_transform(iris_test, model)  # To manually unload the model from memory when finished and to clean up # the working directory (this doesn't touch your model file) unload_uwot(model)  # At this point, model cannot be used with umap_transform, this would fail: # test_embedding2 <- umap_transform(iris_test, model)  # restore the model: this also creates a temporary working directory model2 <- load_uwot(file = model_file) test_embedding2 <- umap_transform(iris_test, model2)  # Unload and clean up the loaded model temp directory unload_uwot(model2)  # clean up the model file unlink(model_file)  # save with unloading: this deletes the temporary working directory but # doesn't allow the model to be re-used model3 <- umap(iris_train, ret_model = TRUE, n_epochs = 20) model_file3 <- tempfile(\"iris_umap\") model3 <- save_uwot(model3, file = model_file3, unload = TRUE)"},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-development-version","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot (development version)","text":"init_sdev = \"range\" caused error user-supplied init matrix. Transforming new data correlation metric actually using cosine metric saved reloaded model. Thank Holly Hall report helpful detective work (https://github.com/jlmelville/uwot/issues/117). umap_transform fail new data transformed scaled:center scaled:scale attributes set (e.g. applying scale function).","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-0116","dir":"Changelog","previous_headings":"","what":"uwot 0.1.16","title":"uwot 0.1.16","text":"CRAN release: 2023-06-29","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-16","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.16","text":"small change header file required fully support next version RcppAnnoy. Thank Dirk Eddelbuettel PR (https://github.com/jlmelville/uwot/issues/112).","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-0115","dir":"Changelog","previous_headings":"","what":"uwot 0.1.15","title":"uwot 0.1.15","text":"CRAN release: 2023-06-26","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-1-15","dir":"Changelog","previous_headings":"","what":"New features:","title":"uwot 0.1.15","text":"New function: optimize_graph_layout. Use produce optimized output coordinates reflect input similarity graph (produced similarity_graph function. similarity_graph followed optimize_graph_layout running umap, purpose functions allow flexibility decoupling generating nearest neighbor graph optimizing low-dimensional approximation . Based request user Chengwei94 (https://github.com/jlmelville/uwot/issues/98). New functions: simplicial_set_union simplicial_set_intersect. allow combination different fuzzy graph representations dataset single fuzzy graph using UMAP simplicial set operations. Based request Python UMAP issues tracker user Dhar xion. New parameter umap_transform: ret_extra. works like equivalent parameter umap, character vector specifying extra information like returned addition embedding, case list returned embedding member containing optimized coordinates. Supported values \"fgraph\", \"nn\", \"sigma\" \"localr\". Based request user PedroMilanezAlmeida (https://github.com/jlmelville/uwot/issues/104). New parameter umap, tumap umap_transform: seed. equivalent calling set.seed internally, hence help reproducibility. chosen seed exported ret_model = TRUE umap_transform use seed present, need specify umap_transform want change seed. default behavior remains modify random number state. Based request SuhasSrinivasan (https://github.com/jlmelville/uwot/issues/110).","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-15","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.15","text":"new setting init_sdev: set init_sdev = \"range\" initial coordinates range-scaled column takes values 0-10. pre-processing added Python UMAP package point uwot began development probably always used default init = \"spectral\" setting. However, set default maintain backwards compatibility older versions uwot. ret_extra = c(\"sigma\") now supported lvish. Gaussian bandwidths returned sigma vector. addition, vector intrinsic dimensionalities estimated point using analytical expression finite difference method given Lee co-workers returned dint vector. min_dist spread parameters now returned model umap run ret_model = TRUE. just documentation purposes, values used directly model umap_transform. parameters b set directly invoking umap, min_dist spread set NULL returned model. feature added response question kjiang18 (https://github.com/jlmelville/uwot/issues/95). new checks NA values input data added. Also warning emitted n_components seems set high. n_components greater n_neighbors umap_transform crash R session. Thank ChVav reporting (https://github.com/jlmelville/uwot/issues/102). Using umap_transform model dens_scale set cause segmentation fault, destroying session. Even didn’t give entirely artifactual “ring” structure. Thank FemkeSmit reporting providing assistance diagnosing underlying cause (https://github.com/jlmelville/uwot/issues/103). set binary_edge_weights = TRUE, setting exported ret_model = TRUE, therefore respected umap_transform. now fixed, need regenerate models used binary edge weights. rdoc init param said multiple disconnected components, spectral initialization attempt merge multiple sub-graphs. true: actually, spectral initialization abandoned favor PCA. documentation updated reflect true state affairs. idea thinking . load_model save_model didn’t work Windows 7 due version tar handles drive letters. Thank mytarmail report (https://github.com/jlmelville/uwot/issues/109). Warn initial coordinates large scale (standard deviation > 10.0), can lead small gradients poor optimization. Thank SuhasSrinivasan report (https://github.com/jlmelville/uwot/issues/110). change accommodate forthcoming version RcppAnnoy. Thank Dirk Eddelbuettel PR (https://github.com/jlmelville/uwot/issues/111).","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-0114","dir":"Changelog","previous_headings":"","what":"uwot 0.1.14","title":"uwot 0.1.14","text":"CRAN release: 2022-08-22","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-1-14","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.1.14","text":"New function: similarity_graph. interested high-dimensional graph/fuzzy simplicial set representation input data, don’t care low dimensional approximation, similarity_graph function offers similar API umap, neither initialization optimization low-dimensional coordinates performed. return value returned results list fgraph member provided ret_extra = c(\"fgraph\"). Compared getting result via running umap, function bit convenient use, makes intention clearer discarding embedding, saves small amount time. t-SNE/LargeVis similarity graph can returned setting method = \"largevis\".","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-14","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.14","text":"model generated without using pre-generated nearest neighbors, couldn’t use umap_transform pre-generated nearest neighbors (also error message completely useless). Thank AustinHartman reporting (https://github.com/jlmelville/uwot/issues/97).","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-0113","dir":"Changelog","previous_headings":"","what":"uwot 0.1.13","title":"uwot 0.1.13","text":"CRAN release: 2022-08-16 resubmission 0.1.12 internal function (fuzzy_simplicial_set) refactored behave like previous versions. change breaking behavior CRAN package bbknnR.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-1-12","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.1.12","text":"New parameter: dens_weight. set value 0 1, attempt made include relative local densities input data output coordinates. approximation densMAP method. large value dens_weight use larger range output densities reflect input data. data spread , reduce value dens_weight. information see documentation uwot repo. New parameter: binary_edge_weights. set TRUE, instead smoothed knn distances, non-zero edge weights value 1. PaCMAP works practical theoretical reasons believe won’t big effect UMAP can try . \"sigma\": return value contain sigma entry, vector smooth knn distance scaling normalization factors, one observation input data. small value indicates high density points local neighborhood observation. lvish equivalent bandwidths calculated input perplexity returned. also, vector rho exported, distance nearest neighbor number neighbors specified local_connectivity. applies umap tumap. \"localr\": exports vector local radii, sum sigma rho used scale output coordinates dens_weight set. Even using dens_weight, visualizing output coordinates using color scale based value localr can reveal regions input data different densities. functions umap tumap : new data type precomputed nearest neighbor data passed nn_method parameter: may use sparse distance matrix format dgCMatrix dimensions N x N N number observations input data. Distances arranged column, .e. non-zero entry row j ith column indicates jth observation input data nearest neighbor ith observation distance given value element. Note different format sparse distance matrix can passed input X: notably, matrix assumed symmetric. Unlike input formats, may different number neighbors observation (must least one neighbor defined per observation). umap_transform can also take sparse distance matrix nn_method parameter precomputed nearest neighbor data used generate initial model. format nn_method umap. distances arranged columns, expected dimensions sparse matrix N_model x N_new N_model number observations original data N_new number observations data transformed.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-12","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.12","text":"Models couldn’t re-saved loading. Thank ilyakorsunsky reporting (https://github.com/jlmelville/uwot/issues/88). RSpectra now ‘Suggests’, rather ‘Imports’. RSpectra installed, used automatically previous versions required (spectral initialization). Otherwise, irlba used. two-dimensional output, unlikely notice much difference speed accuracy real-world data. highly-structured simulation datasets (e.g. spectral initialization 1D line) RSpectra give much better, faster initializations, typical use cases envisaged package. embedding higher dimensions (e.g. n_components = 100 higher), RSpectra recommended likely -perform irlba even installed good linear algebra library. init = \"laplacian\" returned wrong coordinates slightly subtle issue around order eigenvectors using random walk transition matrix rather normalized graph laplacians. init_sdev parameter ignored init parameter user-supplied matrix. Now input scaled. Matrix input converted data frame pre-processing, causing R allocate memory disinclined ever give even function exited. unnecessary manipulation now avoided. behavior bandwidth parameter changed give results like current version (0.5.2) Python UMAP implementation. likely breaking change non-default settings bandwidth, parameter actually exposed Python UMAP public API , road deprecation uwot don’t recommend change . Transforming data multiple blocks give error number rows new data equal number number rows original data.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-0111","dir":"Changelog","previous_headings":"","what":"uwot 0.1.11","title":"uwot 0.1.11","text":"CRAN release: 2021-12-02","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-1-11","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.1.11","text":"New parameter: batch. TRUE, results reproducible n_sgd_threads > 1 (long use set.seed). price paid optimization slightly less efficient (coordinates updated quickly hence gradients staler longer), highly recommended set n_epochs = 500 higher. Thank Aaron Lun came way implement feature, also wrote entire C++ implementation UMAP (https://github.com/jlmelville/uwot/issues/83). New parameter: opt_args. default optimization method batch = TRUE Adam. can control parameters passing opt_args list. Adam momentum-based method requires extra storage previous gradient data. avoid extra memory overhead can also use opt_args = list(method = \"sgd\") use stochastic gradient descent method like used batch = FALSE. New parameter: epoch_callback. may now pass function invoked end epoch. Mainly useful producing image state embedding different points optimization. another feature taken umappp. \"irlba\" uses irlba::irlba calculate truncated SVD. routine deems trying extract 50% singular vectors, see warning effect logged console. \"rsvd\", uses irlba::svdr truncated SVD. method uses small number iterations give accuracy/speed trade-similar scikit-learn TruncatedSVD method. can much faster using \"irlba\" potentially cost accuracy. However, purposes dimensionality reduction input nearest neighbor search, doesn’t seem matter much. \"bigstatsr\", uses bigstatsr package used. Note: dependency uwot. want use bigstatsr, must install . platforms without easy access fast linear algebra libraries (e.g. Windows), using bigstatsr may give speed PCA calculations. \"svd\", uses base::svd. Warning: likely slow datasets exists fallback small datasets \"irlba\" method print warning. \"auto\" (default) uses \"irlba\" calculate truncated SVD, unless attempting extract 50% singular vectors, case \"svd\" used.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-11","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.11","text":"row names provided input data (nearest neighbor data, initialization data ’s matrix), used name rows output embedding (https://github.com/jlmelville/uwot/issues/81), also nearest neighbor data set ret_nn = TRUE. names exist one input data parameters listed , inconsistent, guarantees made names used. Thank jwijffels reporting . umap_transform, learning rate now -scaled factor 4, consistent Python implementation UMAP. need old behavior back, use (newly added) learning_rate parameter umap_transform set explicitly. used default value umap creating model, correct setting umap_transform learning_rate = 1.0. Setting nn_method = \"annoy\" verbose = TRUE lead error datasets fewer 50 items . Using multiple pre-computed nearest neighbors blocks now supported umap_transform (incorrectly documented work). Documentation around pre-calculated nearest neighbor data umap_transform wrong ways: now corrected indicate neighbor data item test data, neighbors distances refer items training data (.e. data used build model). n_neighbors parameter now correctly ignored model generation pre-calculated nearest neighbor data provided. Documentation incorrectly said grain_size didn’t anything.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-0110","dir":"Changelog","previous_headings":"","what":"uwot 0.1.10","title":"uwot 0.1.10","text":"CRAN release: 2020-12-15 release mainly allow internal changes keep compatibility RcppAnnoy, used nearest neighbor calculations.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-10","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.10","text":"Passing data missing values now raise error early. Missing data factor columns intended supervised UMAP still ok. Thank David McGaughey tweeting issue. documentation return value umap tumap now note contents model list subject change intended part uwot public API. recommend relying structure model, especially package intended appear CRAN Bioconductor, breakages delay future releases uwot CRAN.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-019","dir":"Changelog","previous_headings":"","what":"uwot 0.1.9","title":"uwot 0.1.9","text":"CRAN release: 2020-11-15","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-1-9","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.1.9","text":"New metric: metric = \"correlation\" distance based Pearson correlation (https://github.com/jlmelville/uwot/issues/22). Supporting required change internals nearest neighbor data stored. Backwards compatibility models generated previous versions using ret_model = TRUE preserved.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-9","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.9","text":"New parameter, nn_method, umap_transform: pass list containing pre-computed nearest neighbor data (identical used umap function). pass anything X parameter case. extends functionality transforming new points case nearest neighbor data original data new data can calculated external uwot. Thanks Yuhan Hao contributing PR (https://github.com/jlmelville/uwot/issues/63 https://github.com/jlmelville/uwot/issues/64). New parameter, init, umap_transform: provides variety options initializing output coordinates, analogously parameter umap function (without many options currently). intended replace init_weighted, considered deprecated, won’t removed uwot 1.0 (whenever ). Instead init_weighted = TRUE, use init = \"weighted\"; replace init_weighted = FALSE init = \"average\". Additionally, can pass matrix init act initial coordinates. Also umap_transform: previously, setting n_epochs = 0 ignored: least one iteration optimization applied. Now, n_epochs = 0 respected, return initialized coordinates without optimization. Minor performance improvement single-threaded nearest neighbor search verbose = TRUE: progress bar calculations taking detectable amount time now fixed. small data sets (< 50 items) progress bar longer appear building index. Passing sparse distance matrix input now supports upper/lower triangular matrix storage rather wasting storage using explicitly symmetric sparse matrix. Minor license change: uwot used licensed GPL-3 ; now GPL-3 later.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-018","dir":"Changelog","previous_headings":"","what":"uwot 0.1.8","title":"uwot 0.1.8","text":"CRAN release: 2020-03-16","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-8","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.8","text":"default n_threads now NULL provide bit protection changing dependencies. parallel code now uses standard C++11 implementation threading rather tinythread++. grain_size parameter undeprecated. version deprecated never made CRAN, unlikely affected many people.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-7","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.7","text":"uwot longer trigger undefined behavior sanitizers, due temporary replacement RcppParallel package code “borrowed” package using tinythread++ rather tbb (https://github.com/jlmelville/uwot/issues/52). sanitizer improvements nearest neighbor search code due upstream efforts erikbern eddelbuettel (https://github.com/jlmelville/uwot/issues/50). grain_size parameter now ignored remains avoid breaking backwards compatibility .","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-1-6","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.1.6","text":"New parameter, ret_extra, vector can contain combination : \"model\" (ret_model = TRUE), \"nn\" (ret_nn = TRUE) fgraph (see ). New return value data: ret_extra vector contains \"fgraph\", returned list contain fgraph item representing fuzzy simplicial input graph sparse N x N matrix. lvish, use \"P\" instead \"fgraph” (https://github.com/jlmelville/uwot/issues/47). Note sparsifying step edges low membership removed prospect edge sampled optimization. controlled n_epochs: smaller value, sparsifying occur. interested fuzzy graph embedded coordinates, set n_epochs = 0. New function: unload_uwot, unload Annoy nearest neighbor indices model. prevents model used umap_transform, allows temporary working directory created save_uwot load_uwot deleted. Previously, load_uwot save_uwot attempting delete temporary working directories used, always silently fail Annoy making use files directories. attempt made reduce variability results due different compiler C++ library versions different machines. Visually results unchanged cases, breaking change terms numerical output. best chance obtaining floating point determinism across machines use init = \"spca\", fixed values b (rather allowing calculated setting min_dist spread) approx_pow = TRUE. Using tumap method init = \"spca\" probably robust approach.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-6","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.6","text":"New behavior n_epochs = 0. used behave like (n_epochs = NULL) gave default number epochs (dependent number vertices dataset). Now usefully carries calculations except optimization, returned coordinates specified init parameter, easy way access e.g. spectral PCA initialization coordinates. want input fuzzy graph (ret_extra vector contains \"fgraph\"), also prevent graph edges low membership removed. still get old default epochs behavior setting n_epochs = NULL negative value. save_uwot load_uwot updated verbose parameter ’s easier see temporary files created. save_uwot new parameter, unload, set TRUE delete working directory , cost unloading model, .e. can’t used umap_transform reload load_uwot. save_uwot now returns saved model extra field, mod_dir, points location temporary working directory, now assign result calling save_uwot model saved, e.g. model <- save_uwot(model, \"my_model_file\"). field intended use unload_uwot. load_uwot also returns model mod_dir item use unload_uwot. save_uwot load_uwot correctly handling relative paths. previous bug fix load_uwot uwot 0.1.4 work newer versions RcppAnnoy (https://github.com/jlmelville/uwot/issues/31) failed typical case single metric nearest neighbor search using available columns, giving error message along lines : Error: index size <size> multiple vector size <size>. now fixed, required changes save_uwot load_uwot, existing saved models must regenerated. Thank reporter OuNao.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-015","dir":"Changelog","previous_headings":"","what":"uwot 0.1.5","title":"uwot 0.1.5","text":"CRAN release: 2019-12-04","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-5","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.5","text":"R API accessed inside multi-threaded code seed (non-R) random number generators. Probably causing users downstream projects (seurat monocle) experience strange RcppParallel-related crashes. Thanks aldojongejan reporting (https://github.com/jlmelville/uwot/issues/39). Passing floating point value smaller one n_threads caused crash. particularly insidious running system one default thread available default n_threads becomes 0.5. Now n_threads (n_sgd_threads) rounded nearest integer. Initialization supervised UMAP now faster (https://github.com/jlmelville/uwot/issues/34). Contributed Aaron Lun.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-014","dir":"Changelog","previous_headings":"","what":"uwot 0.1.4","title":"uwot 0.1.4","text":"CRAN release: 2019-09-23","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-4","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.4","text":"Fixed incorrect loading Annoy indexes compatible newer versions RcppAnnoy (https://github.com/jlmelville/uwot/issues/31). thanks Dirk Eddelbuettel Erik Bernhardsson aid identifying problem. Fix ERROR: already InterruptableProgressMonitor instance defined. verbose = TRUE, , b curve parameters now logged.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-013","dir":"Changelog","previous_headings":"","what":"uwot 0.1.3","title":"uwot 0.1.3","text":"CRAN release: 2019-04-07","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-3","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.3","text":"Fixed issue session crash Annoy nearest neighbor search unable find k neighbors item.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"known-issue-0-1-3","dir":"Changelog","previous_headings":"","what":"Known issue","title":"uwot 0.1.3","text":"Even fix bug mentioned , nearest neighbor index file larger 2GB size, Annoy may able read data back . occur large high-dimensional datasets. nearest neighbor search fail conditions. work-around set n_threads = 0, index written disk re-loaded circumstances, cost longer search time. Alternatively, set pca parameter reduce dimensionality lower n_trees, reduce size index disk. However, either may lower accuracy nearest neighbor results.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-012","dir":"Changelog","previous_headings":"","what":"uwot 0.1.2","title":"uwot 0.1.2","text":"CRAN release: 2019-04-06 Initial CRAN release.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-1-2","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.1.2","text":"New parameter, tmpdir, allows user specify temporary directory nearest neighbor indexes written Annoy nearest neighbor search. default base::tempdir(). used n_threads > 1 nn_method = \"annoy\".","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-2","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.1.2","text":"Fixed issue lvish --one error calculating input probabilities. Added safe-guard lvish prevent gaussian precision, beta, becoming overly large binary search fails perplexity calibration. lvish perplexity calibration uses log-sum-exp trick avoid numeric underflow beta becomes large.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-0-0-9010","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.0.0.9010 (31 March 2019)","text":"New parameter: pcg_rand. TRUE (default), random number generator PCG family used stochastic optimization phase. old PRNG, direct translation implementation Tausworthe “taus88” PRNG used Python version UMAP, can obtained setting pcg_rand = FALSE. new PRNG slower, likely superior statistical randomness. change behavior break backwards compatibility: now get slightly different results even seed. New parameter: fast_sgd. TRUE, following combination parameters set: n_sgd_threads = \"auto\", pcg_rand = FALSE approx_pow = TRUE. result substantially faster optimization phase, cost slightly less accurate results exactly repeatable. fast_sgd = FALSE default interested visualization, fast_sgd gives perfectly good results. generic dimensionality reduction reproducibility, keep fast_sgd = FALSE. New parameter: init_sdev specifies large standard deviation column initial coordinates . scale input coordinates (including user-provided matrix coordinates). init = \"spca\" can now thought alias init = \"pca\", init_sdev = 1e-4. may aggressive scaling datasets. typical UMAP spectral initializations tend result standard deviations around 2 5, might appropriate cases. spectral initialization detects multiple components affinity graph falls back scaled PCA, uses init_sdev = 1. result adding init_sdev, init options sspectral, slaplacian snormlaplacian removed (weren’t around long anyway). can get behavior e.g. init = \"spectral\", init_sdev = 1e-4. init = \"spca\" sticking around use lot.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-0-9010","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.0.0.9010 (31 March 2019)","text":"Spectral initialization (default) sometimes generating coordinates large range, due erroneous scale factor failed account negative coordinate values. give rise embeddings noticeable outliers distant main clusters. Also spectral initialization, amount noise added standard deviation order magnitude large compared Python implementation (probably didn’t make difference though). requesting spectral initialization, multiple disconnected components present, fall back init = \"spca\". Removed dependency C++ <random> header. breaks backwards compatibility even set pcg_rand = FALSE. metric = \"cosine\" results incorrectly using unmodified Annoy angular distance. Numeric matrix columns can specified target categorical metric (fixes https://github.com/jlmelville/uwot/issues/20).","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"uwot-0009009-1-january-2019","dir":"Changelog","previous_headings":"","what":"uwot 0.0.0.9009 (1 January 2019)","title":"uwot 0.0.0.9009 (1 January 2019)","text":"Data now stored column-wise optimization, result increase performance larger values n_components (e.g. approximately 50% faster optimization time MNIST n_components = 50). New parameter: pca_center, controls whether center data applying PCA. typical set FALSE applying PCA binary data (although note can’t use setting metric = \"hamming\") PCA now used metric \"manhattan\" \"cosine\". ’s still applied using \"hamming\" (data still needs binary format, real-valued). using mixed datatypes, may override pca pca_center parameter values given data block using list value metric, column ids/names unnamed item overriding values named items, e.g. instead manhattan = 1:100, use manhattan = list(1:100, pca_center = FALSE) turn PCA centering just block. functionality exists mainly case mixed binary real-valued data want apply PCA data types. ’s normal apply centering real-valued data binary data.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-0-9009","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.0.0.9009 (1 January 2019)","text":"Fixed bug affected umap_transform, negative sampling size test data (training data). performance improvements (around 10% faster optimization stage MNIST). verbose = TRUE, log Annoy recall accuracy, may help tune values n_trees search_k.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-0-0-9008","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.0.0.9008 (December 23 2018)","text":"New parameter: n_sgd_threads, controls number threads used stochastic gradient descent. default now single-threaded result reproducible results using set.seed. get back old, less consistent, faster settings, set n_sgd_threads = \"auto\". alpha now learning_rate. gamma now repulsion_strength. Default spectral initialization now looks disconnected components initializes separately (also applies laplacian normlaplacian). New init options: sspectral, snormlaplacian slaplacian. like spectral, normlaplacian, laplacian respectively, scaled dimension standard deviation 1e-4. like difference pca spca options.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-0-9008","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.0.0.9008 (December 23 2018)","text":"Hamming distance support (actually using Euclidean distance). Smooth knn/perplexity calibration results small dependency number threads used. Anomalously long spectral initialization times now reduced. Internal changes fixes thanks code review Aaron Lun.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-0-0-9007","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.0.0.9007 (December 9 2018)","text":"New parameter pca: set positive integer reduce matrix data frames number columns using PCA. works metric = \"euclidean\". > 100 columns, can substantially improve speed nearest neighbor search. t-SNE implementations often set value 50.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-0-9007","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.0.0.9007 (December 9 2018)","text":"Laplacian Eigenmap initialization convergence failure now correctly detected. C++ code -writing data passed R function argument.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-0-0-9006","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.0.0.9006 (December 5 2018)","text":"Highly experimental mixed data type support metric: instead specifying single metric name (e.g. metric = \"euclidean\"), can pass list, name item metric use value vector names columns use metric, e.g. metric = list(\"euclidean\" = c(\"A1\", \"A2\"), \"cosine\" = c(\"B1\", \"B2\", \"B3\")) treats columns A1 A2 one block, using Euclidean distance find nearest neighbors, whereas B1, B2 B3 treated second block, using cosine distance. Factor columns can also used metric, using metric name categorical. y may now data frame matrix multiple target data available. New parameter target_metric, specify distance metric use numerical y. capabilities metric. Multiple external nearest neighbor data sources now supported. Instead passing list two matrices, pass list lists, one external metric. details mixed data types can found https://github.com/jlmelville/uwot#mixed-data-types. Compatibility older versions RcppParallel (contributed sirusb). scale = \"Z\" Z-scale column input (synonym scale = TRUE scale = \"scale\"). New scaling option, scale = \"colrange\" scale columns range (0, 1).","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-0-0-9005","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.0.0.9005 (November 4 2018)","text":"Hamming distance now supported, due upgrade RcppAnnoy 0.0.11.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-0-0-9004","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.0.0.9004 (October 21 2018)","text":"supervised UMAP numeric y, may pass nearest neighbor data directly, format supported X-related nearest neighbor data. may useful don’t want use Euclidean distances y data, missing data (way assign nearest neighbors cases, obviously). See Nearest Neighbor Data Format section details.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-0-0-9003","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.0.0.9003 (September 22 2018)","text":"New parameter ret_nn: TRUE returns nearest neighbor matrices nn list: indices item idx distances item dist. Embedded coordinates embedding. ret_nn ret_model can TRUE, cause compatibility issues supervised embeddings. nn_method can now take precomputed nearest neighbor data. Must list two matrices: idx, containing integer indexes, dist containing distances. coincidence, format return ret_nn.","code":""},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-0-9003","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.0.0.9003 (September 22 2018)","text":"Embedding n_components = 1 broken (https://github.com/jlmelville/uwot/issues/6) User-supplied matrices init parameter modified, defiance basic R pass--copy semantics.","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"bug-fixes-and-minor-improvements-0-0-0-9002","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"uwot 0.0.0.9002 (August 14 2018)","text":"metric = \"cosine\" working n_threads greater 0 (https://github.com/jlmelville/uwot/issues/5)","code":""},{"path":[]},{"path":"https://jlmelville.github.io/uwot/news/index.html","id":"new-features-0-0-0-9001","dir":"Changelog","previous_headings":"","what":"New features","title":"uwot 0.0.0.9001","text":"August 5 2018. can now use existing embedding add new points via umap_transform. See example section . August 1 2018. Numerical vectors now supported supervised dimension reduction. July 31 2018. () initial support supervised dimension reduction: categorical data moment. Pass factor vector (use NA unknown labels) y parameter edges bad (unknown) labels -weighted, hopefully leading better separation classes. works remarkably well Fashion MNIST dataset. July 22 2018. can now use cosine Manhattan distances Annoy nearest neighbor search, via metric = \"cosine\" metric = \"manhattan\", respectively. Hamming distance supported RcppAnnoy doesn’t yet support .","code":""}]
