<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="uwot">
<title>UMAP for t-SNE • uwot</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="UMAP for t-SNE">
<meta property="og:description" content="uwot">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light" data-bs-theme="light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">uwot</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.16.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/uwot.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../articles/index.html">Articles</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/jlmelville/uwot/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>UMAP for t-SNE</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/jlmelville/uwot/blob/HEAD/vignettes/articles/umap-for-tsne.Rmd" class="external-link"><code>vignettes/articles/umap-for-tsne.Rmd</code></a></small>
      <div class="d-none name"><code>umap-for-tsne.Rmd</code></div>
    </div>

    
    
<p><em>January 1 2020:</em> I used to say that “the <a href="https://arxiv.org/abs/1802.03426v1" class="external-link">UMAP paper</a> does not go
into much implementation detail.” That was true of the first version of
the paper, but the <a href="https://arxiv.org/abs/1802.03426v2" class="external-link">second
version</a> adds an appendix that covers most of the material here. This
is no coincidence, as I was added as a co-author, and this was my main
contribution.</p>
<p>If you are coming to UMAP from t-SNE and don’t know much about
topology or fuzzy sets (and I certainly don’t), you may find yourself
hankering for some insight into how UMAP achieves its results and what
connection there is with t-SNE and related methods.</p>
<p>Here are some details I have taken from scouring the <a href="https://github.com/lmcinnes/umap" class="external-link">Python source code</a> and from
asking UMAP creator <a href="https://github.com/lmcinnes" class="external-link">Leland
McInnes</a>. In what follows, I assume that you are already familiar
with how t-SNE works.</p>
<p>Broadly, the UMAP implementation uses a similar approach to <a href="https://arxiv.org/abs/1602.00370" class="external-link">LargeVis</a>. LargeVis in turn
uses concepts from <a href="https://lvdmaaten.github.io/tsne/" class="external-link">t-SNE</a>. t-SNE itself is a
modification of the original <a href="https://papers.nips.cc/paper/2276-stochastic-neighbor-embedding" class="external-link">Stochastic
Neighbor Embedding (SNE)</a> method.</p>
<p>Different papers use different symbols and nomenclature, so to make
sure we’re all on the same page, I will restate the relevant definitions
from SNE and t-SNE before we take a look at LargeVis and UMAP.</p>
<div class="section level2">
<h2 id="asymmetric-sne">(Asymmetric) SNE<a class="anchor" aria-label="anchor" href="#asymmetric-sne"></a>
</h2>
<p>Given <span class="math inline">\(N\)</span> observations of some
high dimensional data, for any pair, <span class="math inline">\(\mathbf{x_{i}}\)</span> and <span class="math inline">\(\mathbf{x_{j}}\)</span>, SNE defines the
similarity (aka an affinity or weight) between them, <span class="math inline">\(v_{j|i}\)</span>, using a Gaussian kernel
function:</p>
<p><span class="math display">\[
v_{j|i} = \exp(-\beta_{i} r_{ij}^2)
\]</span></p>
<p>where <span class="math inline">\(r_{ij}\)</span> is the distance
between <span class="math inline">\(\mathbf{x_{i}}\)</span> and <span class="math inline">\(\mathbf{x_{j}}\)</span> and <span class="math inline">\(\beta_{i}\)</span> must be determined by some
method (we’ll get back to that). The notation of <span class="math inline">\(v_{j|i}\)</span> rather than <span class="math inline">\(v_{ij}\)</span>, is to indicate that this quantity
is not symmetric, i.e. <span class="math inline">\(v_{j|i} \neq
v_{i|j}\)</span>. I’ve borrowed this notation from the conditional
versus joint probability definitions used in symmetric SNE (see below)
but we’ll also need it for quantities other than probabilities. The
<span class="math inline">\(r_{ij}\)</span> notation indicates that the
distances are symmetric and this convention will be used for other
symmetric values.</p>
<p>The weights are normalized to form <span class="math inline">\(N\)</span> probability distributions:</p>
<p><span class="math display">\[
p_{j|i} = \frac{v_{j|i}}{\sum_{k}^{N} v_{k|i}}
\]</span> <span class="math inline">\(\beta_{i}\)</span> is chosen by
finding that value that results in the probability distribution having a
specific perplexity. The perplexity has to be chosen by the user, but is
interpreted as being a continuous version of the number of nearest
neighbors, and generally is chosen to take values between 5 and 50.</p>
<p><span class="math inline">\(p_{j|i}\)</span> is a conditional
probability, and is interpreted as meaning “the probability that you
would pick item <span class="math inline">\(j\)</span> as being similar
to item <span class="math inline">\(i\)</span>, given that you’ve
already picked <span class="math inline">\(i\)</span>”.</p>
<p>In the output space of the embedded coordinates, the similarity
between the points <span class="math inline">\(\mathbf{y_i}\)</span> and
<span class="math inline">\(\mathbf{y_j}\)</span> is also defined as a
Gaussian:</p>
<p><span class="math display">\[
w_{ij} = \exp(-d_{ij}^2)
\]</span></p>
<p>where <span class="math inline">\(d_{ij}\)</span> is the Euclidean
distance between <span class="math inline">\(\mathbf{y_i}\)</span> and
<span class="math inline">\(\mathbf{y_j}\)</span>. There is no <span class="math inline">\(\beta\)</span> in this weight definition so these
weights are symmetric. The output probabilities, <span class="math inline">\(q_{j|i}\)</span> are calculated from <span class="math inline">\(w_{ij}\)</span> in the same way that we go from
<span class="math inline">\(v_{j|i}\)</span> to <span class="math inline">\(p_{j|i}\)</span>, again creating <span class="math inline">\(N\)</span> probability distributions. Due to
normalizing by rows, the <span class="math inline">\(q_{j|i}\)</span>
are asymmetric despite the symmetric weights they are generated
from.</p>
<p>The SNE cost function is the sum of the Kullback-Leibler divergences
of the <span class="math inline">\(N\)</span> distributions:</p>
<p><span class="math display">\[
C_{SNE} = \sum_{i}^{N} \sum_{j}^{N} p_{j|i} \log \frac{p_{j|i}}{q_{j|i}}
\]</span> In all of the above (and in what follows), weights and
probabilities when <span class="math inline">\(i =
j\)</span> are not defined. I don’t want to clutter the notation
further, so assume they are excluded from any sums.</p>
</div>
<div class="section level2">
<h2 id="symmetric-sne">Symmetric SNE<a class="anchor" aria-label="anchor" href="#symmetric-sne"></a>
</h2>
<p>In Symmetric SNE, the input probability matrix is symmetrized by
averaging <span class="math inline">\(p_{j|i}\)</span> and <span class="math inline">\(p_{i|j}\)</span> and then re-normalized over all
pairs of points, to create a single (joint) probability distribution,
<span class="math inline">\(p_{ij}\)</span>:</p>
<p><span class="math display">\[
p_{ij} = \frac{p_{j|i} + p_{i|j}}{2N}
\]</span></p>
<p>The output probabilities, <span class="math inline">\(q_{ij}\)</span>
are now defined by normalizing the output weights over all pairs, again
creating a single probability distribution:</p>
<p><span class="math display">\[
q_{ij} = \frac{w_{ij}}{\sum_{k}^N \sum_{l}^N w_{kl}}
\]</span></p>
<p>The cost function for SSNE is then:</p>
<p><span class="math display">\[
C_{SSNE} = \sum_{i}^{N} \sum_{j}^{N} p_{ij} \log \frac{p_{ij}}{q_{ij}}
\]</span></p>
</div>
<div class="section level2">
<h2 id="t-sne">t-SNE<a class="anchor" aria-label="anchor" href="#t-sne"></a>
</h2>
<p>For the purposes of this discussion, t-SNE only differs from
symmetric SNE by its weight function:</p>
<p><span class="math display">\[
w_{ij} = \frac{1}{1 + d_{ij}^2}
\]</span></p>
</div>
<div class="section level2">
<h2 id="sne-optimization">SNE Optimization<a class="anchor" aria-label="anchor" href="#sne-optimization"></a>
</h2>
<p>Optimization in t-SNE proceeds by:</p>
<ul>
<li>Calibrate the input probabilities, <span class="math inline">\(p_{ij}\)</span> according to the desired
perplexity (this only needs to be done once).</li>
<li>Iteratively:</li>
<li>Calculate all pairwise distances, <span class="math inline">\(d_{ij}\)</span> from <span class="math inline">\(\mathbf{y_{i}}\)</span> and <span class="math inline">\(\mathbf{y_{j}}\)</span>
</li>
<li>Calculate the weights, <span class="math inline">\(w_{ij}\)</span>
</li>
<li>Calculate the output probabilities <span class="math inline">\(q_{ij}\)</span>
</li>
<li>Use gradient descent to update the <span class="math inline">\(\mathbf{y_{i}}\)</span> and <span class="math inline">\(\mathbf{y_{j}}\)</span>
</li>
</ul>
<p>This is fundamentally <span class="math inline">\(O(N^2)\)</span>
because of the need to calculate pairwise distances: the t-SNE gradient
requires the <span class="math inline">\(q_{ij}\)</span> to be
calculated and the normalization step that converts <span class="math inline">\(w_{ij}\)</span> to <span class="math inline">\(q_{ij}\)</span> requires all of <span class="math inline">\(w_{ij}\)</span> to be calculated so you also need
all the distances.</p>
<p>Approaches like <a href="https://arxiv.org/abs/1301.3342" class="external-link">Barnes-Hut
t-SNE</a> and others (e.g. <a href="https://arxiv.org/abs/1712.09005" class="external-link">Flt-SNE</a>) attempt to improve
on this by taking advantage of the t-SNE gradient:</p>
<ul>
<li>The attractive part of the gradient depends on <span class="math inline">\(p_{ij}\)</span>, which is constant and only large
for neighbors that are close in the input space. Therefore it’s only
necessary to calculate the attractive gradient for nearest neighbors of
<span class="math inline">\(\mathbf{x_i}\)</span>. In Barnes-Hut t-SNE,
the number of nearest neighbors used is three times whatever the
perplexity is. For larger datasets, a perplexity of 50 is common, so
usually you are looking for the 150-nearest neighbors of each
point.</li>
<li>The repulsive part of the gradient is dependent on <span class="math inline">\(q_{ij}\)</span> which changes with each iteration,
so the improvements here focus on grouping together points which are
distant in the output space and treating them as a single point for the
purposes of the gradient calculation.</li>
</ul>
<p>As you will be able to tell from perusing the publications linked to
above, these approaches are increasing in sophistication and
complexity.</p>
</div>
<div class="section level2">
<h2 id="largevis">LargeVis<a class="anchor" aria-label="anchor" href="#largevis"></a>
</h2>
<p>LargeVis takes a different approach: it re-uses a lot of the same
definitions as t-SNE, but makes sufficient modifications so that it’s
possible to use stochastic gradient descent.</p>
<p>Also, rather than talk about probabilities, LargeVis uses the
language of graph theory. Each observation in our dataset is now
considered to be a vertex or node and the similarity between them is the
weight of the edge between the two vertices. Conceptually we’re still
talking about elements in a matrix, but I will start slipping into the
language of “edges” and “vertices”.</p>
<p>The key change is the cost function, which is now a maximum
likelihood function:</p>
<p><span class="math display">\[
L_{LV} = \sum_{ \left(i, j\right) \in E} p_{ij} \log w_{ij}
+\gamma \sum_{\left(i, j\right) \in \bar{E}} \log \left(1 - w_{ij}
\right)
\]</span> <span class="math inline">\(p_{ij}\)</span> and <span class="math inline">\(w_{ij}\)</span> is the same as in t-SNE (the
authors try some alternative <span class="math inline">\(w_{ij}\)</span>
definitions, but they aren’t as effective).</p>
<p>The new concepts here are <span class="math inline">\(\gamma\)</span>
and <span class="math inline">\(E\)</span>. <span class="math inline">\(\gamma\)</span> is a user-defined positive scalar
to weight repulsive versus attractive forces. Its default in the <a href="https://github.com/lferry007/LargeVis" class="external-link">reference
implementation</a> is 7.</p>
<p><span class="math inline">\(E\)</span> is the set of edges with a
non-zero weight. This is the graph theory way to talk about nearest
neighbors in the input space. Just as with Barnes-Hut t-SNE, we find a
set of nearest neighbors for each point <span class="math inline">\(\mathbf{x_i}\)</span> and only define input
weights and probabilities for pairs of points which are nearest
neighbors. As with the official Barnes-Hut t-SNE implementation, the
LargeVis reference implementation uses a default perplexity of 50, and
the default number of nearest neighbors is 3 times the perplexity.</p>
<p>This cost function therefore consists of two disjoint contributions:
nearest neighbors in the input space contribute to the attractive part
of the cost function (the first part). Everything else contributes to
the second, repulsive part.</p>
<p>The key advantage of this cost function over the KL divergence is
that it doesn’t contain <span class="math inline">\(q_{ij}\)</span>.
With no output normalization, we don’t need to calculate all the output
pairwise distances. So this cost function is amenable to stochastic
gradient descent techniques.</p>
</div>
<div class="section level2">
<h2 id="the-largevis-sampling-strategy">The LargeVis sampling strategy<a class="anchor" aria-label="anchor" href="#the-largevis-sampling-strategy"></a>
</h2>
<p>To calculate a stochastic gradient, LargeVis does the following:</p>
<ul>
<li>Samples an edge in <span class="math inline">\(E\)</span>,
i.e. chooses <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> such that <span class="math inline">\(p_{ij} \neq 0\)</span>. This is called a “positive
edge” in the LargeVis paper. <span class="math inline">\(i\)</span> and
<span class="math inline">\(j\)</span> are used to calculate the
attractive part of the gradient.</li>
<li>Samples one of the <span class="math inline">\(N\)</span> vertices,
let’s call it <span class="math inline">\(k\)</span>. As datasets grow
larger, the probability that <span class="math inline">\(k\)</span> is
in <span class="math inline">\(E\)</span> grows smaller, but that’s not
actually checked for (only that <span class="math inline">\(k \neq
i\)</span>). This is used to calculate the repulsive part of the
gradient. These are called the “negative samples” in the LargeVis
paper.</li>
<li>Repeat the negative sample step a number of times. The default in
LargeVis is to sample 5 negatives for each positive edge.</li>
</ul>
<p>The coordinates of <span class="math inline">\(i\)</span>, <span class="math inline">\(j\)</span> and the various <span class="math inline">\(k\)</span> are then updated according to the
gradients. This concludes one iteration of the SGD.</p>
<p>The attractive and repulsive gradients for LargeVis are
respectively:</p>
<p><span class="math display">\[
\frac{\partial L_{LV}}{\partial \mathbf{y_i}}^+ =
\frac{-2}{1 + d_{ij}^2}p_{ij} \left(\mathbf{y_i - y_j}\right) \\
\frac{\partial L_{LV}}{\partial \mathbf{y_i}}^- =
\frac{2\gamma}{\left(0.1 + d_{ij}^2\right)\left(1 + d_{ij}^2\right)}
\left(\mathbf{y_i - y_j}\right)
\]</span> The value of 0.1 that appears in the repulsive gradient is
there to prevent division by zero.</p>
<p>Sampling of edges and vertices is not uniform. For the attractive
gradient, the authors note that the factor of <span class="math inline">\(p_{ij}\)</span> that appears means that the
magnitude of the gradient can differ hugely between samples to the
extent that choosing an appropriate learning rate can be difficult.
Instead they sample the edges proportionally to <span class="math inline">\(p_{ij}\)</span> and then for the gradient
calculation, treat each edge as if the weights were all equal. The
attractive gradient used as part of LargeVis SGD is therefore:</p>
<p><span class="math display">\[
\frac{\partial L_{LV}}{\partial \mathbf{y_i}}^+ =
\frac{-2}{1 + d_{ij}^2} \left(\mathbf{y_i - y_j}\right)
\]</span></p>
<p>As <span class="math inline">\(p_{ij}\)</span> doesn’t appear in the
repulsive part of the gradient, so it would seem that uniform sampling
would work for the negative sampling. However, vertices are sampled
using a “noisy” distribution proportional to their degree ^ 0.75, where
the degree of the vertex is the sum of the weights of the edges incident
to them. There doesn’t seem to be a theoretical reason to use the degree
^ 0.75. It’s based on results from the field of word embeddings: the
LargeVis authors reference a <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-andphrases" class="external-link">skip-gram</a>
paper, but the same power also shows up in <a href="https://nlp.stanford.edu/projects/glove/" class="external-link">GloVE</a>. In both cases
it is justified purely empirically. The <code>uwot</code> version of
LargeVis (<code>lvish</code>) samples the negative edges uniformly, and
it doesn’t seem to cause any problems.</p>
</div>
<div class="section level2">
<h2 id="umap-at-last">UMAP (at last)<a class="anchor" aria-label="anchor" href="#umap-at-last"></a>
</h2>
<p>The UMAP cost function is the cross-entropy of two fuzzy sets, which
can be represented as symmetric weight matrices:</p>
<p><span class="math display">\[
C_{UMAP} =
\sum_{ij} \left[ v_{ij} \log \left( \frac{v_{ij}}{w_{ij}} \right) +
(1 - v_{ij}) \log \left( \frac{1 - v_{ij}}{1 - w_{ij}} \right) \right]
\]</span> <span class="math inline">\(v_{ij}\)</span> are symmetrized
input affinities, and are not probabilities. The graph interpretation of
them as weights of edges in a graph still applies, though. These are
arrived at differently to t-SNE and LargeVis. The unsymmetrized UMAP
input weights are given by:</p>
<p><span class="math display">\[
v_{j|i} = \exp \left[ -\left( r_{ij} - \rho_{i} \right) / \sigma_{i}
\right]
\]</span></p>
<p>where <span class="math inline">\(r_{ij}\)</span> are the input
distances, <span class="math inline">\(\rho_{i}\)</span> is the distance
to the nearest neighbor (ignoring zero distances where neighbors are
duplicates) and <span class="math inline">\(\sigma_{i}\)</span> is
analogous to <span class="math inline">\(\beta_{i}\)</span> in the
perplexity calibration used in SNE. In this case, <span class="math inline">\(\sigma_{i}\)</span> is determined such that <span class="math inline">\(\sum_{j} v_{j|i} = \log_{2} k\)</span> where <span class="math inline">\(k\)</span> is the number of nearest neighbors.</p>
<p><em>January 1 2020:</em> I assume there is a connection here with the
local scaling advocated for <a href="http://papers.nips.cc/paper/2619-self-tuning-spectral-clustering" class="external-link">self-tuning
spectral clustering</a>, given that spectral decomposition of the
affinity graph is the default initialization method for UMAP.</p>
<p>These weights are symmetrized by a slightly different method to
SNE:</p>
<p><span class="math display">\[
v_{ij} = \left(v_{j|i} + v_{i|j}\right) - v_{j|i}v_{i|j}
\]</span> or as a matrix operation:</p>
<p><span class="math display">\[
V_{symm} = V + V^{T} - V \circ V^{T}
\]</span></p>
<p>where <span class="math inline">\(T\)</span> indicates the transpose
and <span class="math inline">\(\circ\)</span> is the Hadamard
(i.e. entry-wise) product. This effectively carries out a fuzzy set
union.</p>
<p>The output weights are given by:</p>
<p><span class="math display">\[
w_{ij} = 1 / \left(1 + ad_{ij}^{2b}\right)
\]</span></p>
<p>where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are determined by a non-linear least
squares fit based on the <code>min_dist</code> and <code>spread</code>
parameters that control the tightness of the squashing function. By
setting <span class="math inline">\(a = 1\)</span> and <span class="math inline">\(b = 1\)</span> you get t-SNE weighting back.
<del>The current UMAP defaults result in a = 1.929 and b = 0.7915</del>.
<em>April 7 2019</em>: Actually, I got this wrong. The UMAP defaults use
<code>min_dist = 0.1, spread = 1</code>, which results in <span class="math inline">\(a = 1.577\)</span> and <span class="math inline">\(b = 0.8951\)</span>. If you use
<code>min_dist = 0.001, spread = 1</code> then you get the result for
<span class="math inline">\(a = 1.929\)</span> and <span class="math inline">\(b = 0.7915\)</span>.</p>
<p>The attractive and repulsive UMAP gradient expressions are,
respectively:</p>
<p><span class="math display">\[
\frac{\partial C_{UMAP}}{\partial \mathbf{y_i}}^+ =
\frac{-2abd_{ij}^{2\left(b - 1\right)}}{1 + ad_{ij}^{2b}}  v_{ij}
\left(\mathbf{y_i - y_j}\right) \\
\frac{\partial C_{UMAP}}{\partial \mathbf{y_i}}^- =
\frac{2b}{\left(0.001 + d_{ij}^2\right)\left(1 +
ad_{ij}^{2b}\right)}\left(1 - v_{ij}\right)\left(\mathbf{y_i -
y_j}\right)
\]</span></p>
<p>(<em>April 13 2020</em>: In previous versions of this document I had
completely messed up this expression by omitting a factor of 2 in the
repulsive gradient equation <em>and</em> missed out some important <span class="math inline">\(a\)</span>s and <span class="math inline">\(b\)</span>s. This also affected the SGD version
two equations below. Thank you to Dmitry Kobak for spotting this.)</p>
<p>While more complex-looking than the LargeVis gradient, there are
obvious similarities, which become more clearer if you set <span class="math inline">\(a=1\)</span> and <span class="math inline">\(b=1\)</span>, to get back the t-SNE/LargeVis
output weight function. The 0.001 term in the denominator of the
repulsive gradient plays the same role as the 0.1 in the LargeVis
gradient (preventing division by zero).</p>
<p>UMAP uses the same sampling strategy as LargeVis, where sampling of
positive edges is proportional to the weight of the edge (in this case
<span class="math inline">\(v_{ij}\)</span>), and then the value of the
gradient is calculated by assuming that <span class="math inline">\(v_{ij} = 1\)</span> for all edges. So for SGD
purposes, the attractive gradient for UMAP is:</p>
<p><span class="math display">\[
\frac{\partial C_{UMAP}}{\partial \mathbf{y_i}}^+ =
\frac{-2abd_{ij}^{2\left(b - 1\right)}}{1 +
ad_{ij}^{2b}}\left(\mathbf{y_i - y_j}\right) =
\frac{-2abd_{ij}^{2b}}{d_{ij}^2 \left(1 +
ad_{ij}^{2b}\right)}\left(\mathbf{y_i - y_j}\right)
\]</span></p>
<p>The final expression might be more computationally convenient because
it saves on an extra power calculation.</p>
<p>The repulsive part of the gradient contains a <span class="math inline">\(1 - v_{ij}\)</span> term, but because <span class="math inline">\(v_{ij} = 0\)</span> for most pairs of edges, that
term effectively disappears, leaving:</p>
<p><span class="math display">\[
\frac{\partial C_{UMAP}}{\partial \mathbf{y_i}}^- =
\frac{2b}{\left(0.001 + d_{ij}^2\right)\left(1 + ad_{ij}^{2b}\right)}
  \left(\mathbf{y_i - y_j}\right)
\]</span></p>
<p>Unlike LargeVis, negative sampling in UMAP uses a uniform
distribution.</p>
<p>It’s worth considering that, although LargeVis uses <span class="math inline">\(p_{ij}\)</span> and UMAP uses <span class="math inline">\(v_{ij}\)</span> in their cost functions, the
difference isn’t that important, because sampling proportionally to
<span class="math inline">\(p_{ij}\)</span> is exactly the same as
sampling proportionally to <span class="math inline">\(v_{ij}\)</span>.
In fact, if you look at the LargeVis reference implementation (or
<code>lvish</code> in <code>uwot</code>), the input affinities are
symmetrized, but not divided by <span class="math inline">\(N\)</span>.
Nonetheless, because the affinities are only used to construct the
sampling probabilities, the presence of the <span class="math inline">\(\gamma\)</span> parameter in the repulsive part of
the gradient means that you are effectively using <span class="math inline">\(p_{ij}\)</span> in the LargeVis cost function that
is being optimized, not <span class="math inline">\(v_{ij}\)</span>.</p>
</div>
<div class="section level2">
<h2 id="minor-umap-variations">Minor UMAP Variations<a class="anchor" aria-label="anchor" href="#minor-umap-variations"></a>
</h2>
<p>There are some extra parameters in UMAP that make some minor changes
if modified from their default-values:</p>
<ul>
<li><p><code>local_connectivity</code> affects <span class="math inline">\(\rho_{i}\)</span> by using the distance to the
<code>local_connectivity</code>th non-zero near neighbor (or by
interpolating between distances if <code>local_connectivity</code> is
non-integral).</p></li>
<li><p><code>set_op_mix_ratio</code> This changes the form of the
symmetrization from fuzzy set union to fuzzy set intersection which is
just <span class="math inline">\(v_{j|i}v_{i|j}\)</span>, and can also
blend between the two.</p></li>
<li><p><code>gamma</code> This works exactly like the value in LargeVis,
up-weighting the repulsive contribution of the gradient.</p></li>
</ul>
<p><em>2 August 2018</em>: The follow parameter no longer appears in the
reference UMAP implementation:</p>
<ul>
<li>
<code>bandwidth</code> affects <span class="math inline">\(v_{j|i}\)</span> by multiplying the value of <span class="math inline">\(\sigma_{i}\)</span>:</li>
</ul>
<p><span class="math display">\[
v_{j|i} = \exp \left[ -\left( r_{ij} - \rho_{i} \right) / \beta
\sigma_{i} \right]
\]</span></p>
<p>where <code>bandwidth</code> is represented as <span class="math inline">\(\beta\)</span>. The value of <span class="math inline">\(\sigma_{i}\)</span> is determined using
calculations of <span class="math inline">\(v_{j|i}\)</span> without
<span class="math inline">\(\beta\)</span>, before recalculating the
<span class="math inline">\(v_{j|i}\)</span> using
<code>bandwidth</code>.</p>
<p>I’m not sure how useful any of these are when changed from the
defaults.</p>
<p>My thanks to <a href="https://github.com/dkobak" class="external-link">Dmitry Kobak</a> for
some very helpful discussions and typo-spotting.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by James Melville.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.8.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
