<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Python Comparison • uwot</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Python Comparison">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">uwot</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.2.2.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/uwot.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item"><a class="nav-link" href="../articles/index.html">Articles</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/jlmelville/uwot/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Python Comparison</h1>
            <h3 data-toc-skip class="subtitle">and some nearest neighbor
comparisons</h3>
            
      
      <small class="dont-index">Source: <a href="https://github.com/jlmelville/uwot/blob/master/vignettes/articles/pycompare.Rmd" class="external-link"><code>vignettes/articles/pycompare.Rmd</code></a></small>
      <div class="d-none name"><code>pycompare.Rmd</code></div>
    </div>

    
    
<p>This page is a gallery of images comparing the output of
<code>uwot</code> version 0.1.3 to the <a href="https://github.com/lmcinnes/umap" class="external-link">Python <code>UMAP</code>
package</a> version 0.3.8. In an ideal world, this wouldn’t be a very
exciting document, as it would exist to demonstrate that
<code>uwot</code> results resemble those from the Python
implementation.</p>
<p>However, there are a couple of examples that highlight some
differences between the implementations. It’s still not that exciting,
but there may be some information of use to anyone using
<code>uwot</code> and possibly some implications for users of t-SNE
packages. To distinguish between the general technique of UMAP and the
its specific implementation in the Python package of the same name, I’ll
refer to the Python implementation with the formatting
<code>UMAP</code>.</p>
<p>For details on the datasets, see the <a href="https://jlmelville.github.io/uwot/articles/umap-examples.html">UMAP
examples gallery</a>. The <code>uwot</code> results have been re-run for
the images here using different random seeds, but they should resemble
the <code>uwot</code> output on that page.</p>
<p>The datasets originated in R, mostly being loaded via the <a href="https://github.com/jlmelville/snedata" class="external-link">snedata package</a>. I then
exported them to CSV, before reading them into Python. UMAP version
0.3.8 was used to generate the embeddings, using all default settings.
The embeddings were then written to CSV and read back into R for
visualization.</p>
<p>The <code>uwot</code> output also used default settings with the
following exceptions: <code>pca = 100</code> was used to reduce the
dimensionality to a maximum of 100 columns (data was mean-centered as
part of this process, but not rescaled), because the Annoy nearest
neighbor search can be very slow for high-dimensional data.
<code>UMAP</code>’s <a href="https://github.com/lmcinnes/pynndescent" class="external-link">nearest neighbor
search</a> seems less affected by dimensionality. Also,
<code>min_dist = 0.1</code>, which is the default for the Python UMAP.
<code>uwot</code> uses a default of <code>min_dist = 0.01</code>. Why? I
would love to tell you, but I think I just made a mistake when I set the
<code>uwot</code> defaults. Therefore, the defaults will probably change
in some later version of <code>uwot</code>. Fortunately, this doesn’t
make a perceptible practical difference, but I will use the Python
default of <code>min_dist = 0.1</code> in the following results.</p>
<p>There is also a potential difference in initialization. Both packages
use a spectral initialization if possible, but differ if the input graph
contains more than one component: <code>UMAP</code> attempts a
meta-embedding of the two separate components, where <code>uwot</code>
abandons a spectral approach and falls back to using PCA (followed by
rescaling the output to a standard deviation of 1 for each axis). This
affects the following datasets: <code>iris</code>, <code>coil20</code>,
<code>coil100</code>, <code>norb</code> and <code>tasic2018</code>.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">42</span><span class="op">)</span></span>
<span><span class="va">iris_umap</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/umap.html">umap</a></span><span class="op">(</span><span class="va">iris</span>, pca <span class="op">=</span> <span class="fl">100</span>, min_dist <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># iris X data read from CSV</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>iris_umap <span class="op">=</span> umap.UMAP().fit_transform(iris_x)</span></code></pre></div>
<p>Below the results on the left are for the <code>uwot</code> output.
The image on the right is from <code>UMAP</code>. I present most of
these results without comment because they seem to be very similar to
each other. There are two exceptions, for the small NORB dataset
(<code>norb</code>) and the gene expression dataset
<code>macosko2015</code>, which will be discussed in their section.</p>
<div class="section level2">
<h2 id="results">Results<a class="anchor" aria-label="anchor" href="#results"></a>
</h2>
<div class="section level3">
<h3 id="iris">iris<a class="anchor" aria-label="anchor" href="#iris"></a>
</h3>
<table class="table">
<colgroup>
<col width="51%">
<col width="48%">
</colgroup>
<tbody><tr class="odd">
<td align="center"><img src="img/pycompare/iris_r.png" alt="iris r"></td>
<td align="center"><img src="img/pycompare/iris_py.png" alt="iris py"></td>
</tr></tbody>
</table>
</div>
<div class="section level3">
<h3 id="s1k">s1k<a class="anchor" aria-label="anchor" href="#s1k"></a>
</h3>
<table class="table"><tbody><tr class="odd">
<td align="center"><img src="img/pycompare/s1k_r.png" alt="s1k r"></td>
<td align="center"><img src="img/pycompare/s1k_py.png" alt="s1k py"></td>
</tr></tbody></table>
</div>
<div class="section level3">
<h3 id="oli">oli<a class="anchor" aria-label="anchor" href="#oli"></a>
</h3>
<table class="table"><tbody><tr class="odd">
<td align="center"><img src="img/pycompare/oli_r.png" alt="oli r"></td>
<td align="center"><img src="img/pycompare/oli_py.png" alt="oli py"></td>
</tr></tbody></table>
</div>
<div class="section level3">
<h3 id="frey">frey<a class="anchor" aria-label="anchor" href="#frey"></a>
</h3>
<table class="table">
<colgroup>
<col width="51%">
<col width="48%">
</colgroup>
<tbody><tr class="odd">
<td align="center"><img src="img/pycompare/frey_r.png" alt="frey r"></td>
<td align="center"><img src="img/pycompare/frey_py.png" alt="frey py"></td>
</tr></tbody>
</table>
</div>
<div class="section level3">
<h3 id="coil20">coil20<a class="anchor" aria-label="anchor" href="#coil20"></a>
</h3>
<table class="table">
<colgroup>
<col width="51%">
<col width="48%">
</colgroup>
<tbody><tr class="odd">
<td align="center"><img src="img/pycompare/coil20_r.png" alt="coil20 r"></td>
<td align="center"><img src="img/pycompare/coil20_py.png" alt="coil20 py"></td>
</tr></tbody>
</table>
</div>
<div class="section level3">
<h3 id="coil100">coil100<a class="anchor" aria-label="anchor" href="#coil100"></a>
</h3>
<table class="table">
<colgroup>
<col width="51%">
<col width="48%">
</colgroup>
<tbody><tr class="odd">
<td align="center"><img src="img/pycompare/coil100_r.png" alt="coil100 r"></td>
<td align="center"><img src="img/pycompare/coil100_py.png" alt="coil100 py"></td>
</tr></tbody>
</table>
</div>
<div class="section level3">
<h3 id="mnist">mnist<a class="anchor" aria-label="anchor" href="#mnist"></a>
</h3>
<table class="table">
<colgroup>
<col width="51%">
<col width="48%">
</colgroup>
<tbody><tr class="odd">
<td align="center"><img src="img/pycompare/mnist_r.png" alt="mnist r"></td>
<td align="center"><img src="img/pycompare/mnist_py.png" alt="mnist py"></td>
</tr></tbody>
</table>
</div>
<div class="section level3">
<h3 id="fashion">fashion<a class="anchor" aria-label="anchor" href="#fashion"></a>
</h3>
<table class="table">
<colgroup>
<col width="51%">
<col width="48%">
</colgroup>
<tbody><tr class="odd">
<td align="center"><img src="img/pycompare/fashion_r.png" alt="fashion r"></td>
<td align="center"><img src="img/pycompare/fashion_py.png" alt="fashion py"></td>
</tr></tbody>
</table>
</div>
<div class="section level3">
<h3 id="kuzushiji">kuzushiji<a class="anchor" aria-label="anchor" href="#kuzushiji"></a>
</h3>
<table class="table">
<colgroup>
<col width="51%">
<col width="48%">
</colgroup>
<tbody><tr class="odd">
<td align="center"><img src="img/pycompare/kuzushiji_r.png" alt="kuzushiji r"></td>
<td align="center"><img src="img/pycompare/kuzushiji_py.png" alt="kuzushiji py"></td>
</tr></tbody>
</table>
</div>
<div class="section level3">
<h3 id="norb">norb<a class="anchor" aria-label="anchor" href="#norb"></a>
</h3>
<table class="table">
<colgroup>
<col width="51%">
<col width="48%">
</colgroup>
<tbody><tr class="odd">
<td align="center"><img src="img/pycompare/norb_r.png" alt="norb r"></td>
<td align="center"><img src="img/pycompare/norb_py.png" alt="norb py"></td>
</tr></tbody>
</table>
<p>Here’s a result that seems worth commenting on. The <code>uwot</code>
results show slightly less structure, a large blue loop seems to have
been broken up.</p>
<p>I suspect that the PCA preprocessing to 100 dimensions as used by
<code>uwot</code> is too aggressive here and might be throwing away too
much of the variance in the dataset. Compared to the ground truth 15
nearest neighbors using <a href="https://cran.r-project.org/package=FNN" class="external-link">FNN</a>, Annoy results
with <code>pca = 100</code> are 82% accurate. Without PCA, this accuracy
goes up to 99% and the <code>uwot</code> results are much closer to the
<code>UMAP</code> results. This is the image below, on the left.
Meanwhile, if you use the PCA results as input to <code>UMAP</code>, the
output now looks a bit more like the <code>uwot</code> results. That’s
the image on the lower right.</p>
<table class="table">
<colgroup>
<col width="51%">
<col width="48%">
</colgroup>
<tbody><tr class="odd">
<td align="center"><img src="img/pycompare/norb_nopca.png" alt="norb r nopca"></td>
<td align="center"><img src="img/pycompare/norb_pypca.png" alt="norb py pca"></td>
</tr></tbody>
</table>
<p>Unfortunately, the very high dimensionality of <code>norb</code>
makes running Annoy on it directly very slow indeed: with a
single-thread, it took 4 and a half hours, versus around 5 minutes in
the PCA case. We will revisit nearest neighbor accuracies later.</p>
<p>Note also that <code>norb</code> is one of the datasets where the
graph contains two separate components, so the initialization of the
output coordinates is different between <code>uwot</code> and
<code>UMAP</code>, with <code>UMAP</code>’s “meta-embedding” approach
better at retaining local structure.</p>
</div>
<div class="section level3">
<h3 id="tasic2018">tasic2018<a class="anchor" aria-label="anchor" href="#tasic2018"></a>
</h3>
<table class="table">
<colgroup>
<col width="51%">
<col width="48%">
</colgroup>
<tbody><tr class="odd">
<td align="center"><img src="img/pycompare/tasic2018_r.png" alt="tasic2018 r"></td>
<td align="center"><img src="img/pycompare/tasic2018_py.png" alt="tasic2018 py"></td>
</tr></tbody>
</table>
</div>
<div class="section level3">
<h3 id="macosko2015">macosko2015<a class="anchor" aria-label="anchor" href="#macosko2015"></a>
</h3>
<table class="table">
<colgroup>
<col width="51%">
<col width="48%">
</colgroup>
<tbody><tr class="odd">
<td align="center"><img src="img/pycompare/macosko2015_r.png" alt="macosko2015 r"></td>
<td align="center"><img src="img/pycompare/macosko2015_py.png" alt="macosko2015 py"></td>
</tr></tbody>
</table>
<p>This result shows the largest deviation of <code>uwot</code> from
<code>UMAP</code>. That big cyan cluster is of an obviously different
shape in the two plots. The source of the difference here seems to be
nearest neighbor results. It turns out that both <code>uwot</code> and
<code>UMAP</code> have trouble find good approximations to the nearest
neighbors with this dataset, but <code>UMAP</code> does a lot better. We
will get into more detail about the nearest neighbor accuracies across
the datasets in the next section. For now, below are two further plots.
The left-hand plot is the <code>uwot</code> results when it uses the
more accurate nearest neighbor data calculated by <code>UMAP</code>: the
<code>uwot</code> results now resemble the <code>UMAP</code> results,
which is reassuring. The right hand plot uses exact nearest neighbor
results calculated via the <code>FNN</code>.</p>
<table class="table">
<colgroup>
<col width="51%">
<col width="48%">
</colgroup>
<tbody><tr class="odd">
<td align="center"><img src="img/pycompare/macosko2015_rpynn.png" alt="macosko2015 r pynn"></td>
<td align="center"><img src="img/pycompare/macosko2015_uwot_exactnn.png" alt="macosko2015 uwot exact nn"></td>
</tr></tbody>
</table>
<p>Even though the <code>UMAP</code> nearest neighbor data is better
than that which <code>uwot</code> produced, we can still see an obvious
difference in the two plots, so it’s clear that this data set presents a
challenge for the approximate nearest neighbor methods considered
here.</p>
</div>
<div class="section level3">
<h3 id="cifar10">cifar10<a class="anchor" aria-label="anchor" href="#cifar10"></a>
</h3>
<table class="table">
<colgroup>
<col width="51%">
<col width="48%">
</colgroup>
<tbody><tr class="odd">
<td align="center"><img src="img/pycompare/cifar10_r.png" alt="cifar10 r"></td>
<td align="center"><img src="img/pycompare/cifar10_py.png" alt="cifar10 py"></td>
</tr></tbody>
</table>
<p>Based on results with this dataset in the <a href="https://jlmelville.github.io/uwot/articles/umap-examples.html">UMAP
examples gallery</a>, I wasn’t expecting these results to be a feast for
the eyes. After seeing the difference in results with
<code>macosko2015</code>, I am just relieved that these two results look
disappointing in a similar way.</p>
</div>
</div>
<div class="section level2">
<h2 id="nearest-neighbor-accuracy">Nearest Neighbor Accuracy<a class="anchor" aria-label="anchor" href="#nearest-neighbor-accuracy"></a>
</h2>
<p>The <code>norb</code> and <code>macosko2015</code> results suggest
that the use of PCA and the nearest neighbor search methods present the
biggest source of difference between <code>uwot</code> and
<code>UMAP</code>. Here are some more details.</p>
<p>For small datasets, which in both implementations means less than
4,096 observations, both <code>uwot</code> and <code>UMAP</code>
calculate the exact nearest neighbors. For larger datasets,
<code>uwot</code> uses the <a href="https://github.com/spotify/annoy" class="external-link">Annoy</a> method, and
<code>UMAP</code> uses <a href="https://github.com/lmcinnes/pynndescent" class="external-link">pynndescent</a>, which
uses random projection trees, followed by nearest neighbor descent.</p>
<p>For the larger datasets, I used the <a href="https://cran.r-project.org/package=FNN" class="external-link">FNN</a> package to
generate the exact 15 nearest neighbors and then compared results for
<code>uwot</code> and <code>UMAP</code>. For <code>UMAP</code>, I used
default settings. For <code>uwot</code>, I applied
<code>pca = 100</code> as that is what I currently use for high
dimensional datasets. The third column shows the amount of variance
explained by only keeping 100 components.</p>
<table class="table">
<thead><tr class="header">
<th>datasets</th>
<th><code>uwot</code></th>
<th><code>UMAP</code></th>
<th>PCA 100 var</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><code>coil100</code></td>
<td>89.1%</td>
<td>99.7%</td>
<td>88.4%</td>
</tr>
<tr class="even">
<td><code>mnist</code></td>
<td>86.2%</td>
<td>97.5%</td>
<td>91.5%</td>
</tr>
<tr class="odd">
<td><code>fashion</code></td>
<td>74.3%</td>
<td>97.9%</td>
<td>91.2%</td>
</tr>
<tr class="even">
<td><code>kuzushiji</code></td>
<td>78.3%</td>
<td>95.2%</td>
<td>84.6%</td>
</tr>
<tr class="odd">
<td><code>norb</code></td>
<td>81.7%</td>
<td>98.3%</td>
<td>95.6%</td>
</tr>
<tr class="even">
<td><code>tasic2018</code></td>
<td>48.0%</td>
<td>97.7%</td>
<td>67.7%</td>
</tr>
<tr class="odd">
<td><code>macosko2015</code></td>
<td>11.2%</td>
<td>41.8%</td>
<td>37.3%</td>
</tr>
<tr class="even">
<td><code>cifar10</code></td>
<td>67.4%</td>
<td>84.1%</td>
<td>90.1%</td>
</tr>
</tbody>
</table>
<p><code>UMAP</code> does a pretty incredible job with nearly every
dataset, and it’s fast too. <em>And</em> it doesn’t need to use PCA to
reduce the dimensionality. The obvious exception is
<code>macosko2015</code>. I was unable to find a combination of
parameters with <code>pynndescent</code> that produced good results,
e.g. by increasing the number of trees in the RP tree initialization or
the number of iterations in the nearest neighbor descent phase. With
<code>n_trees=250</code>, I was able to get to 54.9% accuracy, but I
started to get out of memory errors much beyond this setting, and
nearest neighbor descent tends to converge after 5-6 iterations anyway.
Doing better will may require more manipulation of the sampling rate and
candidate list size used in nearest neighbor descent, which can quickly
cause a large increase in memory and run time if you aren’t careful.
Also, you can’t fiddle with these parameters in <code>UMAP</code>
version 0.3.8 anyway.</p>
<p><code>uwot</code> results… well, they’re less impressive than
<code>UMAP</code>’s. The <code>tasic2018</code> results are pretty bad,
and the <code>macosko2015</code> results can be fairly described as
horrendous. The accuracies show a rough correlation with the percentage
of variance explained by 100 components. Results for
<code>tasic2018</code> and especially <code>macosko2015</code> show that
100 components may not be enough to extract the meaningful variation
from the datasets, which might explain the inability to find the true
nearest neighbors. The <code>uwot</code> defaults for the Annoy search
are somewhat arbitrary, having been copied from <a href="https://github.com/lferry007/LargeVis" class="external-link">LargeVis</a>. That package
used nearest neighbor descent to improve the accuracy, and increasing
Annoy’s <code>n_trees</code> parameter can give a small improvement to
some of the datasets, but the increase in computation time is probably
not worth the small increase in accuracy I’ve seen when trying this. It
seems that in cases where the current defaults don’t work well, it’s
because too much information is thrown away by the PCA
pre-processing.</p>
<p><code>tasic2018</code> and <code>macosko2015</code> are both
transcriptomics datasets and clearly represent an interesting challenge
to Annoy, while <code>macosko2015</code> troubles even pynndescent. The
other datasets are image datasets. Possibly the way the data has been
scaled and prepared for these biology datasets makes life harder for
approximate nearest neighbors? It’s hard to know where to start when it
comes to unpicking what causes <code>macosko2015</code> to perform so
differently, but here is a histogram of the nearest neighbor distances
for <code>macosko2015</code> and <code>tasic2018</code> (top row), which
misbehave with Annoy, and on the bottom row are the histograms for
<code>mnist</code> and <code>fashion</code>, which are better
behaved:</p>
<table class="table">
<colgroup>
<col width="51%">
<col width="48%">
</colgroup>
<tbody>
<tr class="odd">
<td align="center"><img src="img/pycompare/macosko2015_nnhist.png" alt="macosko2015 NN hist"></td>
<td align="center"><img src="img/pycompare/tasic2018_nnhist.png" alt="tasic2018 NN hist"></td>
</tr>
<tr class="even">
<td align="center"><img src="img/pycompare/mnist_nnhist.png" alt="MNIST NN hist"></td>
<td align="center"><img src="img/pycompare/fashion_nnhist.png" alt="fashion NN hist"></td>
</tr>
</tbody>
</table>
<p>The biomodal distribution of <code>macosko2015</code> and
<code>tasic2018</code> do seem more similar to each other than they do
to <code>mnist</code> or <code>fashion</code>. That said,
<code>tasic2018</code> behaves perfectly well with the pynndescent
method, so I see no obvious answers here.</p>
</div>
<div class="section level2">
<h2 id="conclusions">Conclusions<a class="anchor" aria-label="anchor" href="#conclusions"></a>
</h2>
<p>The good news is that for most datasets, the differences between
<code>uwot</code>’s UMAP implementation and that of the Python
<code>UMAP</code> implementation makes very little difference. The bad
news is that there <em>are</em> a couple of cases where we see
differences. The biggest offender is <code>macosko2015</code> but there
neither method is doing a perfect job. Nonetheless, <code>UMAP</code> is
able to calculate nearest neighbors more accurately and more quickly
than <code>uwot</code>.</p>
<p>One source of the difference is the use of Annoy as the nearest
neighbor calculation method. It does seem to be solidly slower than
pynndescent, but in my experience, it can reach high accuracies. even if
it takes a while. <strong>March 8 2020</strong>: On Windows, there used
to be an issue where an Annoy index that was larger than 2GB on disk
couldn’t be read back in. Make sure you are using a version of <a href="https://cran.r-project.org/package=RcppAnnoy" class="external-link">RcppAnnoy</a> 0.0.15
or later to avoid this.</p>
<p>The time to search an Annoy index seems to start scaling quite badly
with dimensionality &gt; 1000, so if you can get away with picking
500-1000 random features and use that, you could try that. With 1,152
random features (i.e. 1/16th the pixels in<code>norb</code>), visual
results aren’t <em>that</em> much worse with this approach and the
nearest neighbor search runs 100 times faster (50 minutes vs 20
seconds):</p>
<table class="table">
<colgroup>
<col width="51%">
<col width="48%">
</colgroup>
<tbody><tr class="odd">
<td align="center"><img src="img/pycompare/norb_nopca2.png" alt="norb 18432 features"></td>
<td align="center"><img src="img/pycompare/norb1152.png" alt="norb 1152 features"></td>
</tr></tbody>
</table>
<p>Otherwise, if you have a high-dimensional dataset, you should
consider PCA. I don’t have any solid advice for the number of components
to retain if you do use PCA for preprocessing. Less than the usual t-SNE
default of 50 is probably unwise. I use <code>pca = 100</code> in most
examples, but as we’ve seen, that can still sufficiently perturbs the
pairwise distances in the input data to give some differences in the
output. It would be nice if you could use more components to get a
better trade-off of accuracy and speed, but even with fast partial PCA
routines used by <code>uwot</code> (courtesy of <a href="https://cran.r-project.org/package=irlba" class="external-link">irlba</a>), extracting
more than 100 components can be quite time-consuming.</p>
<div class="section level3">
<h3 id="what-about-t-sne">What about t-SNE?<a class="anchor" aria-label="anchor" href="#what-about-t-sne"></a>
</h3>
<p><code>uwot</code> is not the only dimensionality reduction to use PCA
or Annoy for approximate nearest neighbor search. LargeVis uses it, the
t-SNE package <a href="https://github.com/KlugerLab/FIt-SNE" class="external-link">Fit-SNE</a>
has Annoy as an option and <a href="https://github.com/pavlin-policar/openTSNE" class="external-link">openTSNE</a> seems to
have at least considered it (see e.g. <a href="https://github.com/pavlin-policar/openTSNE/issues/28" class="external-link uri">https://github.com/pavlin-policar/openTSNE/issues/28</a>).
It is also standard practice in t-SNE to reduce dimensionality: <a href="https://github.com/jkrijthe/Rtsne" class="external-link">Rtsne</a>, a wrapper around the
de-facto standard Barnes Hut t-SNE routine. will only keep 50 components
via PCA by default.</p>
<p>Does the PCA issue effect t-SNE? Well, a little bit. Below are
results for <code>norb</code> and <code>macosko2015</code> (openTSNE’s
documentation is where I discovered this dataset, as it happens) with
t-SNE results from <code>Rtsne</code>, using
<code>perplexity = 15</code>. The images on the left use PCA to reduce
the initial dimensionality down to 100, while the ones on the right use
the raw input data.</p>
<table class="table">
<colgroup>
<col width="51%">
<col width="48%">
</colgroup>
<tbody>
<tr class="odd">
<td align="center"><img src="img/examples/norb_tsne.png" alt="norb tsne pca"></td>
<td align="center"><img src="img/pycompare/norb_tsne_nopca.png" alt="norb tsne no pca"></td>
</tr>
<tr class="even">
<td align="center"><img src="img/examples/macosko2015_tsne.png" alt="macosko2015 tsne pca"></td>
<td align="center"><img src="img/pycompare/macosko2015_tsne_nopca.png" alt="macosko2015 tsne no pca"></td>
</tr>
</tbody>
</table>
<p><code>norb</code> results seem fairly unaffected, but we also see for
<code>macosko2015</code> that there’s a change to the main large
cluster. Usually, you don’t set perplexity as low as 15, so are there
also differences when the perplexity is set to a more typical value,
like 50? Also, the default PCA dimensionality setting in
<code>Rtsne</code> is 50, not 100, so here are those t-SNE plots
repeated with <code>perplexity = 50</code> and either
<code>pca = TRUE, partial_pca = TRUE, initial_dims = 50</code> on the
left, and <code>pca = FALSE</code> on the right:</p>
<table class="table">
<colgroup>
<col width="51%">
<col width="48%">
</colgroup>
<tbody>
<tr class="odd">
<td align="center"><img src="img/pycompare/norb_tsne_p50_pca50.png" alt="norb tsne perp 50 pca 50"></td>
<td align="center"><img src="img/pycompare/norb_tsne_p50_nopca.png" alt="norb tsne perp 50 no pca"></td>
</tr>
<tr class="even">
<td align="center"><img src="img/pycompare/macosko2015_tsne_p50_pca50.png" alt="macosko2015 tsne perp 50 pca 50"></td>
<td align="center"><img src="img/pycompare/macosko2015_tsne_p50_nopca.png" alt="macosko2015 tsne perp 50 no pca"></td>
</tr>
</tbody>
</table>
<p>Again, the <code>norb</code> dataset is less affected than
<code>macosko2015</code> but for the latter there is a definite effect
of preprocessing with PCA. This is something to bear in mind before
applying PCA to your data even with t-SNE. On balance, I’d probably live
with the effect of applying PCA to your data, because just like with
<code>uwot</code>, <code>Rtsne</code> input processing can take a few
hours with very high dimensional datasets.</p>
<p>Finally, some people consider results obtained with PCA to be
<em>more</em> reliable than those using the “raw” input, because PCA can
be considered a way to denoise data, where the lower-variance components
are assumed to be irrelevant. I might be a bit biased, but of all the
UMAP plots of <code>macosko2015</code>, I think the default
<code>uwot</code> one shows the best separation of clusters, so maybe
there’s something to it. But this is also the reason I tabulated the
amount of variance 100 components explained for each dataset. Where most
of the datasets keep a large majority of the variance with 100
components, in the case of <code>macosko2015</code>, you would have to
be prepared to argue why it was ok to throw away over 60% of variance in
the data. I’m sure it can be done, but you would need a solid, ideally
biology-based justification.</p>
<p>A future version of <code>uwot</code> will hopefully contain an
implementation of nearest neighbor descent optimization to reduce the
gap in nearest neighbor accuracy with <code>UMAP</code>.</p>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by James Melville.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
